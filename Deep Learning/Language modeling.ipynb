{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDL_Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aish793/Sandbox/blob/master/Deep%20Learning/Language%20modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wtSADyaQ3SF",
        "colab_type": "text"
      },
      "source": [
        "# **IDL Assignment 5 - Language Modeling & Recurrent Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfMrlv-fRJYu",
        "colab_type": "text"
      },
      "source": [
        "### Team Members\n",
        "\n",
        "\n",
        "1.   Amar Shivaram - 226015\n",
        "2.   Manish Bhandari - 226011\n",
        "3.   Aishwarya Jauhari - 226084\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3fVxBd5HocV",
        "colab_type": "text"
      },
      "source": [
        "## **Assigning Tensorflow version and importing the libraries required for the tasks**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g1v3YHPFgiV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4cebea7b-0376-4c36-9cea-304a8cd1f771"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIrcPHFKmI_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.getcwd()\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/IDL /IDL Assignments/Assignment helper files\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtZrgAErV832",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess the text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoHuC3T6UXaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python prepare_data.py shakespeare_input.txt skp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuThM3SqW-zX",
        "colab_type": "text"
      },
      "source": [
        "**total serialized seq is 22981**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlitZe4kwCE7",
        "colab_type": "text"
      },
      "source": [
        "**Loading the data from skp.tfrecords and skp_vocab**\n",
        "\n",
        "The files mentioned are the output obtained after running the program *prepare_data.py* for the Shakespeare data. These files are loaded as data using tf.data and create a vocabulary dictionary \n",
        "\n",
        "**Note:** The vocab contains elements as dict with (key,val) as (character, index). Reverse mapping is done and stored as ind_to_ch which has (key,val) as (index,character)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ9hhvAKWLbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8e680f3c-f7cf-447c-ec21-67055f246def"
      },
      "source": [
        "from prepare_data import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x, 200))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "\n",
        "print(\"Indices to char\")\n",
        "print(ind_to_ch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'h': 1, 'L': 2, 'b': 3, 's': 4, 'n': 5, 'q': 6, 'Q': 7, '3': 8, 'H': 9, 'Y': 10, 'w': 11, 'P': 12, '-': 13, 'g': 14, \"'\": 15, 'N': 16, 'f': 17, ';': 18, 'j': 19, 'v': 20, 'E': 21, 'x': 22, 'J': 23, 'S': 24, 'O': 25, 'K': 26, 'G': 27, 'a': 28, 'k': 29, 'o': 30, 'R': 31, 'i': 32, ':': 33, 'u': 34, 'M': 35, 'U': 36, ' ': 37, 'B': 38, 'C': 39, ',': 40, 't': 41, 'X': 42, 'r': 43, '$': 44, 'm': 45, 'z': 46, 'y': 47, '.': 48, 'l': 49, '?': 50, 'D': 51, ']': 52, 'c': 53, '!': 54, 'p': 55, '&': 56, 'Z': 57, 'I': 58, '[': 59, 'd': 60, 'W': 61, 'F': 62, '\\n': 63, 'e': 64, 'T': 65, 'A': 66, 'V': 67, '<S>': 0}\n",
            "68\n",
            "Indices to char\n",
            "{1: 'h', 2: 'L', 3: 'b', 4: 's', 5: 'n', 6: 'q', 7: 'Q', 8: '3', 9: 'H', 10: 'Y', 11: 'w', 12: 'P', 13: '-', 14: 'g', 15: \"'\", 16: 'N', 17: 'f', 18: ';', 19: 'j', 20: 'v', 21: 'E', 22: 'x', 23: 'J', 24: 'S', 25: 'O', 26: 'K', 27: 'G', 28: 'a', 29: 'k', 30: 'o', 31: 'R', 32: 'i', 33: ':', 34: 'u', 35: 'M', 36: 'U', 37: ' ', 38: 'B', 39: 'C', 40: ',', 41: 't', 42: 'X', 43: 'r', 44: '$', 45: 'm', 46: 'z', 47: 'y', 48: '.', 49: 'l', 50: '?', 51: 'D', 52: ']', 53: 'c', 54: '!', 55: 'p', 56: '&', 57: 'Z', 58: 'I', 59: '[', 60: 'd', 61: 'W', 62: 'F', 63: '\\n', 64: 'e', 65: 'T', 66: 'A', 67: 'V', 0: '<S>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQDmDUfTx2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa6dae96-8611-4f28-ff2e-dafd5ca6de91"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: (200,), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFRLy9TYEeKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Declare the sizes of batch, shuffle and repeat\n",
        "\n",
        "SHUFFLE_SIZE = 1000\n",
        "BATCH_SIZE = 128\n",
        "REPEAT_TIMES = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5JAavDtD_cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_shuffle_repeat(data):\n",
        "\n",
        "\n",
        "    data = data.shuffle(SHUFFLE_SIZE)\n",
        "    data = data.padded_batch(BATCH_SIZE, padded_shapes=None,drop_remainder=False)   \n",
        "    data = data.repeat(REPEAT_TIMES)\n",
        "\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyyEwmin_pDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = batch_shuffle_repeat(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpGmPlMCpGiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2a05d935-3524-41cf-f77e-aa7752644497"
      },
      "source": [
        "for x in dataset.take(1):\n",
        "  print(tf.shape(x))\n",
        "  print(type(x))\n",
        "  print(repr(tf.shape(x)))\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([128 200], shape=(2,), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([128, 200], dtype=int32)>\n",
            "tf.Tensor(\n",
            "[[ 0 64 37 ... 29 64  4]\n",
            " [ 0  5 37 ... 30 34 43]\n",
            " [ 0 64 43 ... 37 55 64]\n",
            " ...\n",
            " [ 0 47 40 ... 64 20 30]\n",
            " [ 0 37 32 ...  1 47 40]\n",
            " [ 0 34 53 ... 11  1 32]], shape=(128, 200), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1VmdC2twpp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_h = 512\n",
        "\n",
        "## w_xh is input to hidden weight --> known as U from the literature\n",
        "## w_hh is hidden to hidden weights --> known as W from the literature\n",
        "## w_ho is hidden to output weights --> known as V from the literature\n",
        "## b_h and b_o are the biases at the hidden layer and output layer\n",
        "\n",
        "\n",
        "w_xh = tf.Variable(tf.initializers.glorot_uniform()([vocab_size,n_h]))\n",
        "\n",
        "w_hh = tf.Variable(tf.initializers.glorot_uniform()([n_h,n_h]))\n",
        "b_h = tf.Variable(tf.zeros([n_h]))\n",
        "\n",
        "w_ho = tf.Variable(tf.initializers.glorot_uniform()([n_h,vocab_size]))\n",
        "b_o = tf.Variable(tf.zeros([vocab_size]))\n",
        "\n",
        "variables = [w_xh,w_hh,b_h,w_ho,b_o]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Vqww0cNDBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_sequence(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "        loss = loss.stack()\n",
        "\n",
        "        batch_loss = tf.reduce_mean(loss)\n",
        "        \n",
        "    \n",
        "    grads = tape.gradient(batch_loss, variables)\n",
        "    opt.apply_gradients(zip(grads, variables))\n",
        "\n",
        "    return batch_loss,h_t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3BPxZo22jY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### DONT RUN THIS BLOCK .. THIS IS JUST FOR TRIAL PURPOSES\n",
        "\n",
        "\n",
        "###############################################################################################################################################\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "\"\"\"\n",
        "        tf.print(tf.shape(batch_data))\n",
        "        tf.print(type(batch_data))\n",
        "        tf.print(repr(tf.shape(batch_data)))\n",
        "        tf.print(batch_data)\n",
        "\n",
        "        for i in tf.range(tf.shape(batch_data)[1]-1):\n",
        "          tf.print(i)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def rnn_try(batch_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h_t = tf.zeros([tf.shape(batch_data)[0],n_h])\n",
        "        loss = tf.TensorArray(tf.float32,size=tf.shape(batch_data)[1]-1)\n",
        "        \n",
        "\n",
        "        for timestep in tf.range(tf.shape(batch_data)[1]-1):\n",
        "            tf.print(\"=\"*100)\n",
        "            tf.print(timestep)\n",
        "\n",
        "            x_t = tf.one_hot(batch_data[:,timestep],vocab_size)\n",
        "            tf.print(tf.shape(x_t))\n",
        "            tf.print(type(x_t))\n",
        "            tf.print(repr(tf.shape(x_t)))\n",
        "\n",
        "\n",
        "\n",
        "            h_t = tf.nn.tanh(tf.matmul(x_t,w_xh) + tf.matmul(h_t,w_hh) + b_h)\n",
        "            tf.print(tf.shape(h_t))\n",
        "            tf.print(type(h_t))\n",
        "            tf.print(repr(tf.shape(h_t)))\n",
        "\n",
        "\n",
        "            logits = tf.matmul(h_t,w_ho) + b_o\n",
        "            tf.print(tf.shape(logits))\n",
        "            tf.print(type(logits))\n",
        "            tf.print(repr(tf.shape(logits)))\n",
        "\n",
        "\n",
        "            local_loss = loss_fn(batch_data[:,timestep+1],logits)\n",
        "            tf.print(tf.shape(local_loss))\n",
        "            tf.print(type(local_loss))\n",
        "            tf.print(repr(tf.shape(local_loss)))\n",
        "\n",
        "\n",
        "            loss = loss.write(timestep, local_loss)\n",
        "\n",
        "        loss = loss.stack()\n",
        "        tf.print(tf.shape(loss))\n",
        "        tf.print(type(loss))\n",
        "        tf.print(repr(tf.shape(loss)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKsNkX8luxv8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15350b6e-460a-4ad2-a350-1938223a6445"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "batch_nr = 0\n",
        "for batch_data in dataset:\n",
        "      batch_start = time.time()\n",
        "      batch_nr = batch_nr+1\n",
        "      batch_loss,h_t = rnn_sequence(batch_data)\n",
        "      batch_stop = time.time()\n",
        "#      rnn_try(batch_data)\n",
        "      print(\"Batch Number: {} Loss: {} Time taken: {}\".format(batch_nr,batch_loss,batch_stop-batch_start))\n",
        "#      if not steps % 100:\n",
        "#          train_acc_metric(lbl_batch, logits)\n",
        "#          acc = train_acc_metric.result()\n",
        "#          print(\"Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "#          train_acc_metric.reset_states()\n",
        "\n",
        "stop = time.time()\n",
        "print(\"took {} seconds\\n\".format(stop-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Number: 1 Loss: 4.221291542053223 Time taken: 3.126525640487671\n",
            "Batch Number: 2 Loss: 3.833584785461426 Time taken: 0.38123464584350586\n",
            "Batch Number: 3 Loss: 3.5180790424346924 Time taken: 0.3758275508880615\n",
            "Batch Number: 4 Loss: 3.583409309387207 Time taken: 0.3216116428375244\n",
            "Batch Number: 5 Loss: 3.39400315284729 Time taken: 0.2955586910247803\n",
            "Batch Number: 6 Loss: 3.4939541816711426 Time taken: 0.2938830852508545\n",
            "Batch Number: 7 Loss: 3.3784704208374023 Time taken: 0.3131701946258545\n",
            "Batch Number: 8 Loss: 3.363752603530884 Time taken: 0.2981090545654297\n",
            "Batch Number: 9 Loss: 3.385077953338623 Time taken: 0.3008840084075928\n",
            "Batch Number: 10 Loss: 3.3499796390533447 Time taken: 0.2968623638153076\n",
            "Batch Number: 11 Loss: 3.3489768505096436 Time taken: 0.2958827018737793\n",
            "Batch Number: 12 Loss: 3.3338100910186768 Time taken: 0.3241133689880371\n",
            "Batch Number: 13 Loss: 3.311328887939453 Time taken: 0.3471994400024414\n",
            "Batch Number: 14 Loss: 3.3387887477874756 Time taken: 0.34888124465942383\n",
            "Batch Number: 15 Loss: 3.2918541431427 Time taken: 0.2955467700958252\n",
            "Batch Number: 16 Loss: 3.308284282684326 Time taken: 0.35190820693969727\n",
            "Batch Number: 17 Loss: 3.2967140674591064 Time taken: 0.36300230026245117\n",
            "Batch Number: 18 Loss: 3.26259446144104 Time taken: 0.3815603256225586\n",
            "Batch Number: 19 Loss: 3.2465100288391113 Time taken: 0.3307976722717285\n",
            "Batch Number: 20 Loss: 3.263624429702759 Time taken: 0.3136429786682129\n",
            "Batch Number: 21 Loss: 3.2788078784942627 Time taken: 0.3035268783569336\n",
            "Batch Number: 22 Loss: 3.261151075363159 Time taken: 0.3312079906463623\n",
            "Batch Number: 23 Loss: 3.2316434383392334 Time taken: 0.30765581130981445\n",
            "Batch Number: 24 Loss: 3.2153499126434326 Time taken: 0.30315279960632324\n",
            "Batch Number: 25 Loss: 3.2120096683502197 Time taken: 0.3052847385406494\n",
            "Batch Number: 26 Loss: 3.211106777191162 Time taken: 0.30939388275146484\n",
            "Batch Number: 27 Loss: 3.181825637817383 Time taken: 0.33840489387512207\n",
            "Batch Number: 28 Loss: 3.1744158267974854 Time taken: 0.3716244697570801\n",
            "Batch Number: 29 Loss: 3.1632769107818604 Time taken: 0.3062410354614258\n",
            "Batch Number: 30 Loss: 3.1727092266082764 Time taken: 0.2898092269897461\n",
            "Batch Number: 31 Loss: 3.143272876739502 Time taken: 0.30434417724609375\n",
            "Batch Number: 32 Loss: 3.1559524536132812 Time taken: 0.37133288383483887\n",
            "Batch Number: 33 Loss: 3.142335891723633 Time taken: 0.3866763114929199\n",
            "Batch Number: 34 Loss: 3.114291191101074 Time taken: 0.3427410125732422\n",
            "Batch Number: 35 Loss: 3.1259024143218994 Time taken: 0.30353569984436035\n",
            "Batch Number: 36 Loss: 4.315178394317627 Time taken: 0.29911184310913086\n",
            "Batch Number: 37 Loss: 3.75117826461792 Time taken: 0.32131242752075195\n",
            "Batch Number: 38 Loss: 3.1974518299102783 Time taken: 0.37675046920776367\n",
            "Batch Number: 39 Loss: 3.2551233768463135 Time taken: 0.3668670654296875\n",
            "Batch Number: 40 Loss: 3.2400460243225098 Time taken: 0.3587992191314697\n",
            "Batch Number: 41 Loss: 3.237018585205078 Time taken: 0.3051917552947998\n",
            "Batch Number: 42 Loss: 3.2627978324890137 Time taken: 0.3063380718231201\n",
            "Batch Number: 43 Loss: 3.2485477924346924 Time taken: 0.32333898544311523\n",
            "Batch Number: 44 Loss: 3.245100975036621 Time taken: 0.28576159477233887\n",
            "Batch Number: 45 Loss: 3.2383174896240234 Time taken: 0.3851046562194824\n",
            "Batch Number: 46 Loss: 3.2312397956848145 Time taken: 0.36064672470092773\n",
            "Batch Number: 47 Loss: 3.210880994796753 Time taken: 0.3037834167480469\n",
            "Batch Number: 48 Loss: 3.214014768600464 Time taken: 0.2978246212005615\n",
            "Batch Number: 49 Loss: 3.213958263397217 Time taken: 0.3117661476135254\n",
            "Batch Number: 50 Loss: 3.2192625999450684 Time taken: 0.3012089729309082\n",
            "Batch Number: 51 Loss: 3.2302002906799316 Time taken: 0.3023374080657959\n",
            "Batch Number: 52 Loss: 3.2156519889831543 Time taken: 0.3187730312347412\n",
            "Batch Number: 53 Loss: 3.2330684661865234 Time taken: 0.3019390106201172\n",
            "Batch Number: 54 Loss: 3.232079029083252 Time taken: 0.357745885848999\n",
            "Batch Number: 55 Loss: 3.231034517288208 Time taken: 0.33840513229370117\n",
            "Batch Number: 56 Loss: 3.193636894226074 Time taken: 0.29828953742980957\n",
            "Batch Number: 57 Loss: 3.2170698642730713 Time taken: 0.3021717071533203\n",
            "Batch Number: 58 Loss: 3.1821649074554443 Time taken: 0.32456374168395996\n",
            "Batch Number: 59 Loss: 3.170604944229126 Time taken: 0.3952639102935791\n",
            "Batch Number: 60 Loss: 3.178025245666504 Time taken: 0.3749973773956299\n",
            "Batch Number: 61 Loss: 3.1786727905273438 Time taken: 0.3417208194732666\n",
            "Batch Number: 62 Loss: 3.148435115814209 Time taken: 0.35840344429016113\n",
            "Batch Number: 63 Loss: 3.138148307800293 Time taken: 0.3833608627319336\n",
            "Batch Number: 64 Loss: 3.11800479888916 Time taken: 0.33225178718566895\n",
            "Batch Number: 65 Loss: 3.119670867919922 Time taken: 0.2997894287109375\n",
            "Batch Number: 66 Loss: 3.109919309616089 Time taken: 0.3045668601989746\n",
            "Batch Number: 67 Loss: 3.127122402191162 Time taken: 0.3099958896636963\n",
            "Batch Number: 68 Loss: 3.1200435161590576 Time taken: 0.3104686737060547\n",
            "Batch Number: 69 Loss: 3.0946204662323 Time taken: 0.3122084140777588\n",
            "Batch Number: 70 Loss: 3.0840141773223877 Time taken: 0.32523179054260254\n",
            "Batch Number: 71 Loss: 3.074420213699341 Time taken: 0.30018067359924316\n",
            "Batch Number: 72 Loss: 3.083486795425415 Time taken: 0.31181931495666504\n",
            "Batch Number: 73 Loss: 3.0486276149749756 Time taken: 0.30100131034851074\n",
            "Batch Number: 74 Loss: 3.069012403488159 Time taken: 0.31013965606689453\n",
            "Batch Number: 75 Loss: 3.0826168060302734 Time taken: 0.3083610534667969\n",
            "Batch Number: 76 Loss: 3.0270767211914062 Time taken: 0.3324730396270752\n",
            "Batch Number: 77 Loss: 3.023587226867676 Time taken: 0.34221696853637695\n",
            "Batch Number: 78 Loss: 3.035364866256714 Time taken: 0.29736995697021484\n",
            "Batch Number: 79 Loss: 3.0278704166412354 Time taken: 0.3159170150756836\n",
            "Batch Number: 80 Loss: 3.0203804969787598 Time taken: 0.30931758880615234\n",
            "Batch Number: 81 Loss: 3.016550064086914 Time taken: 0.3027517795562744\n",
            "Batch Number: 82 Loss: 3.0204882621765137 Time taken: 0.33336901664733887\n",
            "Batch Number: 83 Loss: 2.9917750358581543 Time taken: 0.29868054389953613\n",
            "Batch Number: 84 Loss: 2.977339506149292 Time taken: 0.2821061611175537\n",
            "Batch Number: 85 Loss: 2.987994432449341 Time taken: 0.3140416145324707\n",
            "Batch Number: 86 Loss: 2.989161252975464 Time taken: 0.3052060604095459\n",
            "Batch Number: 87 Loss: 2.977196455001831 Time taken: 0.2965373992919922\n",
            "Batch Number: 88 Loss: 2.982862949371338 Time taken: 0.2994730472564697\n",
            "Batch Number: 89 Loss: 2.9549295902252197 Time taken: 0.32113075256347656\n",
            "Batch Number: 90 Loss: 2.9441733360290527 Time taken: 0.29805779457092285\n",
            "Batch Number: 91 Loss: 2.964366912841797 Time taken: 0.2975289821624756\n",
            "Batch Number: 92 Loss: 2.9553728103637695 Time taken: 0.3201444149017334\n",
            "Batch Number: 93 Loss: 2.95920991897583 Time taken: 0.3768346309661865\n",
            "Batch Number: 94 Loss: 2.9644827842712402 Time taken: 0.3750145435333252\n",
            "Batch Number: 95 Loss: 2.945124864578247 Time taken: 0.33477044105529785\n",
            "Batch Number: 96 Loss: 2.938931465148926 Time taken: 0.29208827018737793\n",
            "Batch Number: 97 Loss: 2.936039447784424 Time taken: 0.29119443893432617\n",
            "Batch Number: 98 Loss: 2.935633897781372 Time taken: 0.31809043884277344\n",
            "Batch Number: 99 Loss: 2.913172483444214 Time taken: 0.3800985813140869\n",
            "Batch Number: 100 Loss: 2.8928422927856445 Time taken: 0.38322901725769043\n",
            "Batch Number: 101 Loss: 2.867764949798584 Time taken: 0.3481605052947998\n",
            "Batch Number: 102 Loss: 2.9067423343658447 Time taken: 0.3072786331176758\n",
            "Batch Number: 103 Loss: 2.8805599212646484 Time taken: 0.31253862380981445\n",
            "Batch Number: 104 Loss: 2.870368480682373 Time taken: 0.33095836639404297\n",
            "Batch Number: 105 Loss: 2.8571531772613525 Time taken: 0.3728165626525879\n",
            "Batch Number: 106 Loss: 2.852729320526123 Time taken: 0.385272741317749\n",
            "Batch Number: 107 Loss: 2.8352348804473877 Time taken: 0.3059232234954834\n",
            "Batch Number: 108 Loss: 2.851426839828491 Time taken: 0.30022263526916504\n",
            "Batch Number: 109 Loss: 2.84475040435791 Time taken: 0.334688663482666\n",
            "Batch Number: 110 Loss: 2.8220889568328857 Time taken: 0.3074190616607666\n",
            "Batch Number: 111 Loss: 2.838409185409546 Time taken: 0.2996988296508789\n",
            "Batch Number: 112 Loss: 2.843470335006714 Time taken: 0.3034331798553467\n",
            "Batch Number: 113 Loss: 2.806424140930176 Time taken: 0.3438088893890381\n",
            "Batch Number: 114 Loss: 2.8273260593414307 Time taken: 0.295335054397583\n",
            "Batch Number: 115 Loss: 2.8962228298187256 Time taken: 0.29177045822143555\n",
            "Batch Number: 116 Loss: 2.8108415603637695 Time taken: 0.29916858673095703\n",
            "Batch Number: 117 Loss: 2.9385838508605957 Time taken: 0.3013768196105957\n",
            "Batch Number: 118 Loss: 2.824697494506836 Time taken: 0.29871678352355957\n",
            "Batch Number: 119 Loss: 2.8652961254119873 Time taken: 0.293243408203125\n",
            "Batch Number: 120 Loss: 2.8451337814331055 Time taken: 0.3079235553741455\n",
            "Batch Number: 121 Loss: 2.8138082027435303 Time taken: 0.29227733612060547\n",
            "Batch Number: 122 Loss: 2.7941133975982666 Time taken: 0.30771470069885254\n",
            "Batch Number: 123 Loss: 2.818758726119995 Time taken: 0.33052873611450195\n",
            "Batch Number: 124 Loss: 2.7952659130096436 Time taken: 0.2975473403930664\n",
            "Batch Number: 125 Loss: 2.7840042114257812 Time taken: 0.29850316047668457\n",
            "Batch Number: 126 Loss: 2.7640533447265625 Time taken: 0.31268858909606934\n",
            "Batch Number: 127 Loss: 2.77664852142334 Time taken: 0.2935826778411865\n",
            "Batch Number: 128 Loss: 2.7614946365356445 Time taken: 0.2870211601257324\n",
            "Batch Number: 129 Loss: 2.7604336738586426 Time taken: 0.30515170097351074\n",
            "Batch Number: 130 Loss: 2.759859800338745 Time taken: 0.3205416202545166\n",
            "Batch Number: 131 Loss: 2.7579150199890137 Time taken: 0.28822851181030273\n",
            "Batch Number: 132 Loss: 2.740096092224121 Time taken: 0.332289457321167\n",
            "Batch Number: 133 Loss: 2.7363784313201904 Time taken: 0.3233513832092285\n",
            "Batch Number: 134 Loss: 2.7542014122009277 Time taken: 0.3303847312927246\n",
            "Batch Number: 135 Loss: 2.740473508834839 Time taken: 0.31758785247802734\n",
            "Batch Number: 136 Loss: 2.721022367477417 Time taken: 0.34624576568603516\n",
            "Batch Number: 137 Loss: 2.718489646911621 Time taken: 0.3815774917602539\n",
            "Batch Number: 138 Loss: 2.695298433303833 Time taken: 0.31609535217285156\n",
            "Batch Number: 139 Loss: 2.683253049850464 Time taken: 0.32074880599975586\n",
            "Batch Number: 140 Loss: 2.6902294158935547 Time taken: 0.3106069564819336\n",
            "Batch Number: 141 Loss: 2.6476235389709473 Time taken: 0.2968127727508545\n",
            "Batch Number: 142 Loss: 2.647214651107788 Time taken: 0.3629319667816162\n",
            "Batch Number: 143 Loss: 2.650728225708008 Time taken: 0.3471791744232178\n",
            "Batch Number: 144 Loss: 2.649472713470459 Time taken: 0.32852649688720703\n",
            "Batch Number: 145 Loss: 2.654639720916748 Time taken: 0.3444960117340088\n",
            "Batch Number: 146 Loss: 2.66524600982666 Time taken: 0.3641831874847412\n",
            "Batch Number: 147 Loss: 2.643734931945801 Time taken: 0.31952500343322754\n",
            "Batch Number: 148 Loss: 2.636740207672119 Time taken: 0.3226044178009033\n",
            "Batch Number: 149 Loss: 2.6328418254852295 Time taken: 0.3314800262451172\n",
            "Batch Number: 150 Loss: 2.605236768722534 Time taken: 0.328571081161499\n",
            "Batch Number: 151 Loss: 2.616786241531372 Time taken: 0.3218233585357666\n",
            "Batch Number: 152 Loss: 2.600921869277954 Time taken: 0.2985687255859375\n",
            "Batch Number: 153 Loss: 2.599268913269043 Time taken: 0.3085367679595947\n",
            "Batch Number: 154 Loss: 2.5964486598968506 Time taken: 0.32009005546569824\n",
            "Batch Number: 155 Loss: 2.6103515625 Time taken: 0.2932400703430176\n",
            "Batch Number: 156 Loss: 2.600020170211792 Time taken: 0.34406089782714844\n",
            "Batch Number: 157 Loss: 2.584622859954834 Time taken: 0.3149125576019287\n",
            "Batch Number: 158 Loss: 2.5830583572387695 Time taken: 0.31203699111938477\n",
            "Batch Number: 159 Loss: 2.575843334197998 Time taken: 0.3006174564361572\n",
            "Batch Number: 160 Loss: 2.5781056880950928 Time taken: 0.30930018424987793\n",
            "Batch Number: 161 Loss: 2.5877978801727295 Time taken: 0.3100590705871582\n",
            "Batch Number: 162 Loss: 2.565587282180786 Time taken: 0.2986283302307129\n",
            "Batch Number: 163 Loss: 2.55530047416687 Time taken: 0.302276611328125\n",
            "Batch Number: 164 Loss: 2.557331085205078 Time taken: 0.33632826805114746\n",
            "Batch Number: 165 Loss: 2.5492396354675293 Time taken: 0.3555564880371094\n",
            "Batch Number: 166 Loss: 2.542273998260498 Time taken: 0.2893836498260498\n",
            "Batch Number: 167 Loss: 2.56581711769104 Time taken: 0.30333876609802246\n",
            "Batch Number: 168 Loss: 2.543391466140747 Time taken: 0.29928016662597656\n",
            "Batch Number: 169 Loss: 2.543804168701172 Time taken: 0.3134944438934326\n",
            "Batch Number: 170 Loss: 2.5459885597229004 Time taken: 0.3498423099517822\n",
            "Batch Number: 171 Loss: 2.5313267707824707 Time taken: 0.37117576599121094\n",
            "Batch Number: 172 Loss: 2.5328097343444824 Time taken: 0.30479860305786133\n",
            "Batch Number: 173 Loss: 2.5074784755706787 Time taken: 0.31962013244628906\n",
            "Batch Number: 174 Loss: 2.511653423309326 Time taken: 0.31702756881713867\n",
            "Batch Number: 175 Loss: 2.5194575786590576 Time taken: 0.29782915115356445\n",
            "Batch Number: 176 Loss: 2.4975903034210205 Time taken: 0.31142592430114746\n",
            "Batch Number: 177 Loss: 2.499492883682251 Time taken: 0.2943911552429199\n",
            "Batch Number: 178 Loss: 2.503507137298584 Time taken: 0.2958219051361084\n",
            "Batch Number: 179 Loss: 2.488156318664551 Time taken: 0.3112208843231201\n",
            "Batch Number: 180 Loss: 2.5084497928619385 Time taken: 0.8261206150054932\n",
            "Batch Number: 181 Loss: 2.477332830429077 Time taken: 0.3255622386932373\n",
            "Batch Number: 182 Loss: 2.4692368507385254 Time taken: 0.2932133674621582\n",
            "Batch Number: 183 Loss: 2.4743611812591553 Time taken: 0.31369829177856445\n",
            "Batch Number: 184 Loss: 2.4673919677734375 Time taken: 0.3003828525543213\n",
            "Batch Number: 185 Loss: 2.45743989944458 Time taken: 0.300339937210083\n",
            "Batch Number: 186 Loss: 2.466020107269287 Time taken: 0.3023662567138672\n",
            "Batch Number: 187 Loss: 2.470492362976074 Time taken: 0.3018641471862793\n",
            "Batch Number: 188 Loss: 2.4589037895202637 Time taken: 0.3096656799316406\n",
            "Batch Number: 189 Loss: 2.486771821975708 Time taken: 0.3081221580505371\n",
            "Batch Number: 190 Loss: 2.457686185836792 Time taken: 0.2956044673919678\n",
            "Batch Number: 191 Loss: 2.473898410797119 Time taken: 0.3191566467285156\n",
            "Batch Number: 192 Loss: 2.4892899990081787 Time taken: 0.2983663082122803\n",
            "Batch Number: 193 Loss: 2.4885666370391846 Time taken: 0.309844970703125\n",
            "Batch Number: 194 Loss: 2.512035846710205 Time taken: 0.3158128261566162\n",
            "Batch Number: 195 Loss: 2.483377695083618 Time taken: 0.30326199531555176\n",
            "Batch Number: 196 Loss: 2.463517427444458 Time taken: 0.30019426345825195\n",
            "Batch Number: 197 Loss: 2.4549360275268555 Time taken: 0.2985715866088867\n",
            "Batch Number: 198 Loss: 2.4492790699005127 Time taken: 0.3013439178466797\n",
            "Batch Number: 199 Loss: 2.4602138996124268 Time taken: 0.3142244815826416\n",
            "Batch Number: 200 Loss: 2.44181227684021 Time taken: 0.2922077178955078\n",
            "Batch Number: 201 Loss: 2.4444448947906494 Time taken: 0.3120729923248291\n",
            "Batch Number: 202 Loss: 2.427546739578247 Time taken: 0.3278324604034424\n",
            "Batch Number: 203 Loss: 2.4366655349731445 Time taken: 0.3046584129333496\n",
            "Batch Number: 204 Loss: 2.434373140335083 Time taken: 0.32271623611450195\n",
            "Batch Number: 205 Loss: 2.434020519256592 Time taken: 0.30266427993774414\n",
            "Batch Number: 206 Loss: 2.42305850982666 Time taken: 0.3847527503967285\n",
            "Batch Number: 207 Loss: 2.4259512424468994 Time taken: 0.3224771022796631\n",
            "Batch Number: 208 Loss: 2.4307734966278076 Time taken: 0.33860206604003906\n",
            "Batch Number: 209 Loss: 2.4333462715148926 Time taken: 0.2952256202697754\n",
            "Batch Number: 210 Loss: 2.4330179691314697 Time taken: 0.3180277347564697\n",
            "Batch Number: 211 Loss: 2.4296905994415283 Time taken: 0.3061063289642334\n",
            "Batch Number: 212 Loss: 2.417768716812134 Time taken: 0.2963888645172119\n",
            "Batch Number: 213 Loss: 2.4282922744750977 Time taken: 0.3104522228240967\n",
            "Batch Number: 214 Loss: 2.4034783840179443 Time taken: 0.2995028495788574\n",
            "Batch Number: 215 Loss: 2.398634195327759 Time taken: 0.29183363914489746\n",
            "Batch Number: 216 Loss: 2.4023118019104004 Time taken: 0.295123815536499\n",
            "Batch Number: 217 Loss: 2.3894009590148926 Time taken: 0.30900073051452637\n",
            "Batch Number: 218 Loss: 2.3998372554779053 Time taken: 0.3020198345184326\n",
            "Batch Number: 219 Loss: 2.4054605960845947 Time taken: 0.28840184211730957\n",
            "Batch Number: 220 Loss: 2.3931005001068115 Time taken: 0.3183932304382324\n",
            "Batch Number: 221 Loss: 2.380218744277954 Time taken: 0.3052222728729248\n",
            "Batch Number: 222 Loss: 2.3965976238250732 Time taken: 0.30231285095214844\n",
            "Batch Number: 223 Loss: 2.3850932121276855 Time taken: 0.30836915969848633\n",
            "Batch Number: 224 Loss: 2.3697593212127686 Time taken: 0.29573988914489746\n",
            "Batch Number: 225 Loss: 2.374994993209839 Time taken: 0.28859758377075195\n",
            "Batch Number: 226 Loss: 2.3798747062683105 Time taken: 0.2856423854827881\n",
            "Batch Number: 227 Loss: 2.393359661102295 Time taken: 0.318281888961792\n",
            "Batch Number: 228 Loss: 2.3665971755981445 Time taken: 0.28973889350891113\n",
            "Batch Number: 229 Loss: 2.3709933757781982 Time taken: 0.28766560554504395\n",
            "Batch Number: 230 Loss: 2.3751816749572754 Time taken: 0.31452012062072754\n",
            "Batch Number: 231 Loss: 2.3773086071014404 Time taken: 0.29715704917907715\n",
            "Batch Number: 232 Loss: 2.3851733207702637 Time taken: 0.290294885635376\n",
            "Batch Number: 233 Loss: 2.3860068321228027 Time taken: 0.30548620223999023\n",
            "Batch Number: 234 Loss: 2.37994647026062 Time taken: 0.29753947257995605\n",
            "Batch Number: 235 Loss: 2.408025026321411 Time taken: 0.2792205810546875\n",
            "Batch Number: 236 Loss: 2.39005446434021 Time taken: 0.29166173934936523\n",
            "Batch Number: 237 Loss: 2.379051446914673 Time taken: 0.3540632724761963\n",
            "Batch Number: 238 Loss: 2.381289005279541 Time taken: 0.3014030456542969\n",
            "Batch Number: 239 Loss: 2.3549485206604004 Time taken: 0.29918694496154785\n",
            "Batch Number: 240 Loss: 2.347196578979492 Time taken: 0.34112000465393066\n",
            "Batch Number: 241 Loss: 2.3543238639831543 Time taken: 0.2947707176208496\n",
            "Batch Number: 242 Loss: 2.352294445037842 Time taken: 0.29784655570983887\n",
            "Batch Number: 243 Loss: 2.345970630645752 Time taken: 0.3149871826171875\n",
            "Batch Number: 244 Loss: 2.3202216625213623 Time taken: 0.30037760734558105\n",
            "Batch Number: 245 Loss: 2.3184456825256348 Time taken: 0.28645801544189453\n",
            "Batch Number: 246 Loss: 2.347548007965088 Time taken: 0.28711795806884766\n",
            "Batch Number: 247 Loss: 2.3577187061309814 Time taken: 0.3251473903656006\n",
            "Batch Number: 248 Loss: 2.3261823654174805 Time taken: 0.285564661026001\n",
            "Batch Number: 249 Loss: 2.3252456188201904 Time taken: 0.29431867599487305\n",
            "Batch Number: 250 Loss: 2.3216989040374756 Time taken: 0.33556699752807617\n",
            "Batch Number: 251 Loss: 2.3361432552337646 Time taken: 0.3017446994781494\n",
            "Batch Number: 252 Loss: 2.349198818206787 Time taken: 0.29361581802368164\n",
            "Batch Number: 253 Loss: 2.333010673522949 Time taken: 0.34766650199890137\n",
            "Batch Number: 254 Loss: 2.3374531269073486 Time taken: 0.30073118209838867\n",
            "Batch Number: 255 Loss: 2.346143960952759 Time taken: 0.30052709579467773\n",
            "Batch Number: 256 Loss: 2.336181640625 Time taken: 0.35010266304016113\n",
            "Batch Number: 257 Loss: 2.340902090072632 Time taken: 0.3351564407348633\n",
            "Batch Number: 258 Loss: 2.350294828414917 Time taken: 0.36862802505493164\n",
            "Batch Number: 259 Loss: 2.3336920738220215 Time taken: 0.34364819526672363\n",
            "Batch Number: 260 Loss: 2.3155031204223633 Time taken: 0.29158806800842285\n",
            "Batch Number: 261 Loss: 2.3411483764648438 Time taken: 0.3091425895690918\n",
            "Batch Number: 262 Loss: 2.3299286365509033 Time taken: 0.3180420398712158\n",
            "Batch Number: 263 Loss: 2.318183422088623 Time taken: 0.34361982345581055\n",
            "Batch Number: 264 Loss: 2.321702480316162 Time taken: 0.3685884475708008\n",
            "Batch Number: 265 Loss: 2.3100106716156006 Time taken: 0.38413262367248535\n",
            "Batch Number: 266 Loss: 2.3137283325195312 Time taken: 0.3713490962982178\n",
            "Batch Number: 267 Loss: 2.337268114089966 Time taken: 0.3703477382659912\n",
            "Batch Number: 268 Loss: 2.3242855072021484 Time taken: 0.3884153366088867\n",
            "Batch Number: 269 Loss: 2.3103346824645996 Time taken: 0.365969181060791\n",
            "Batch Number: 270 Loss: 2.321842670440674 Time taken: 0.3455324172973633\n",
            "Batch Number: 271 Loss: 2.31449556350708 Time taken: 0.3202652931213379\n",
            "Batch Number: 272 Loss: 2.3123786449432373 Time taken: 0.3150322437286377\n",
            "Batch Number: 273 Loss: 2.333629846572876 Time taken: 0.3041543960571289\n",
            "Batch Number: 274 Loss: 2.3491337299346924 Time taken: 0.3186469078063965\n",
            "Batch Number: 275 Loss: 2.3567087650299072 Time taken: 0.30152201652526855\n",
            "Batch Number: 276 Loss: 2.3355634212493896 Time taken: 0.29927849769592285\n",
            "Batch Number: 277 Loss: 2.329986333847046 Time taken: 0.30561113357543945\n",
            "Batch Number: 278 Loss: 2.3475048542022705 Time taken: 0.30037808418273926\n",
            "Batch Number: 279 Loss: 2.326220989227295 Time taken: 0.2983732223510742\n",
            "Batch Number: 280 Loss: 2.3214080333709717 Time taken: 0.375103235244751\n",
            "Batch Number: 281 Loss: 2.3163609504699707 Time taken: 0.3491377830505371\n",
            "Batch Number: 282 Loss: 2.3193652629852295 Time taken: 0.2866501808166504\n",
            "Batch Number: 283 Loss: 2.341237783432007 Time taken: 0.32074475288391113\n",
            "Batch Number: 284 Loss: 2.3043999671936035 Time taken: 0.30562686920166016\n",
            "Batch Number: 285 Loss: 2.3351075649261475 Time taken: 0.30427122116088867\n",
            "Batch Number: 286 Loss: 2.310427188873291 Time taken: 0.3586437702178955\n",
            "Batch Number: 287 Loss: 2.312861204147339 Time taken: 0.36475205421447754\n",
            "Batch Number: 288 Loss: 2.3135597705841064 Time taken: 0.3482823371887207\n",
            "Batch Number: 289 Loss: 2.3058366775512695 Time taken: 0.3230748176574707\n",
            "Batch Number: 290 Loss: 2.3299076557159424 Time taken: 0.3560786247253418\n",
            "Batch Number: 291 Loss: 2.307818651199341 Time taken: 0.36365652084350586\n",
            "Batch Number: 292 Loss: 2.301069498062134 Time taken: 0.30347418785095215\n",
            "Batch Number: 293 Loss: 2.322126865386963 Time taken: 0.2898831367492676\n",
            "Batch Number: 294 Loss: 2.315556287765503 Time taken: 0.2891116142272949\n",
            "Batch Number: 295 Loss: 2.2972629070281982 Time taken: 0.36990785598754883\n",
            "Batch Number: 296 Loss: 2.3042678833007812 Time taken: 0.38307976722717285\n",
            "Batch Number: 297 Loss: 2.300126075744629 Time taken: 0.35976338386535645\n",
            "Batch Number: 298 Loss: 2.302748441696167 Time taken: 0.3196990489959717\n",
            "Batch Number: 299 Loss: 2.296114683151245 Time taken: 0.29927563667297363\n",
            "Batch Number: 300 Loss: 2.311453104019165 Time taken: 0.30066895484924316\n",
            "Batch Number: 301 Loss: 2.307509422302246 Time taken: 0.31337714195251465\n",
            "Batch Number: 302 Loss: 2.308894395828247 Time taken: 0.31206536293029785\n",
            "Batch Number: 303 Loss: 2.304825782775879 Time taken: 0.33736562728881836\n",
            "Batch Number: 304 Loss: 2.286550760269165 Time taken: 0.34928417205810547\n",
            "Batch Number: 305 Loss: 2.2988483905792236 Time taken: 0.34276700019836426\n",
            "Batch Number: 306 Loss: 2.2864890098571777 Time taken: 0.3135569095611572\n",
            "Batch Number: 307 Loss: 2.2744460105895996 Time taken: 0.310619592666626\n",
            "Batch Number: 308 Loss: 2.282564401626587 Time taken: 0.31641173362731934\n",
            "Batch Number: 309 Loss: 2.2741827964782715 Time taken: 0.311384916305542\n",
            "Batch Number: 310 Loss: 2.289423942565918 Time taken: 0.30825018882751465\n",
            "Batch Number: 311 Loss: 2.28983998298645 Time taken: 0.32038331031799316\n",
            "Batch Number: 312 Loss: 2.320309638977051 Time taken: 0.3005971908569336\n",
            "Batch Number: 313 Loss: 2.329638957977295 Time taken: 0.30175113677978516\n",
            "Batch Number: 314 Loss: 2.2881882190704346 Time taken: 0.3238997459411621\n",
            "Batch Number: 315 Loss: 2.3001656532287598 Time taken: 0.2930634021759033\n",
            "Batch Number: 316 Loss: 2.305166244506836 Time taken: 0.32405900955200195\n",
            "Batch Number: 317 Loss: 2.280012845993042 Time taken: 0.3170008659362793\n",
            "Batch Number: 318 Loss: 2.2881524562835693 Time taken: 0.2991602420806885\n",
            "Batch Number: 319 Loss: 2.2771055698394775 Time taken: 0.30337047576904297\n",
            "Batch Number: 320 Loss: 2.297987222671509 Time taken: 0.3167257308959961\n",
            "Batch Number: 321 Loss: 2.2891147136688232 Time taken: 0.30774879455566406\n",
            "Batch Number: 322 Loss: 2.2997825145721436 Time taken: 0.3223998546600342\n",
            "Batch Number: 323 Loss: 2.2963929176330566 Time taken: 0.36278438568115234\n",
            "Batch Number: 324 Loss: 2.280550003051758 Time taken: 0.3829166889190674\n",
            "Batch Number: 325 Loss: 2.282409429550171 Time taken: 0.30394554138183594\n",
            "Batch Number: 326 Loss: 2.2948379516601562 Time taken: 0.29068636894226074\n",
            "Batch Number: 327 Loss: 2.265425682067871 Time taken: 0.366652250289917\n",
            "Batch Number: 328 Loss: 2.284580945968628 Time taken: 0.33420228958129883\n",
            "Batch Number: 329 Loss: 2.2787184715270996 Time taken: 0.3210783004760742\n",
            "Batch Number: 330 Loss: 2.265730142593384 Time taken: 0.3029155731201172\n",
            "Batch Number: 331 Loss: 2.2827975749969482 Time taken: 0.3007521629333496\n",
            "Batch Number: 332 Loss: 2.2856392860412598 Time taken: 0.30670785903930664\n",
            "Batch Number: 333 Loss: 2.2622153759002686 Time taken: 0.3148040771484375\n",
            "Batch Number: 334 Loss: 2.28279709815979 Time taken: 0.3286712169647217\n",
            "Batch Number: 335 Loss: 2.265573740005493 Time taken: 0.31212449073791504\n",
            "Batch Number: 336 Loss: 2.2842984199523926 Time taken: 0.3429548740386963\n",
            "Batch Number: 337 Loss: 2.301102638244629 Time taken: 0.2961461544036865\n",
            "Batch Number: 338 Loss: 2.2851529121398926 Time taken: 0.28641247749328613\n",
            "Batch Number: 339 Loss: 2.2889840602874756 Time taken: 0.30782079696655273\n",
            "Batch Number: 340 Loss: 2.317185640335083 Time taken: 0.31464624404907227\n",
            "Batch Number: 341 Loss: 2.3380064964294434 Time taken: 0.3487539291381836\n",
            "Batch Number: 342 Loss: 2.2866125106811523 Time taken: 0.39184999465942383\n",
            "Batch Number: 343 Loss: 2.2755465507507324 Time taken: 0.34750986099243164\n",
            "Batch Number: 344 Loss: 2.2926223278045654 Time taken: 0.3167083263397217\n",
            "Batch Number: 345 Loss: 2.283766508102417 Time taken: 0.3436133861541748\n",
            "Batch Number: 346 Loss: 2.279978036880493 Time taken: 0.3640735149383545\n",
            "Batch Number: 347 Loss: 2.2801294326782227 Time taken: 0.346250057220459\n",
            "Batch Number: 348 Loss: 2.2772274017333984 Time taken: 0.382645845413208\n",
            "Batch Number: 349 Loss: 2.2832767963409424 Time taken: 0.2960076332092285\n",
            "Batch Number: 350 Loss: 2.258737325668335 Time taken: 0.30353856086730957\n",
            "Batch Number: 351 Loss: 2.272993564605713 Time taken: 0.3013722896575928\n",
            "Batch Number: 352 Loss: 2.2487785816192627 Time taken: 0.34256696701049805\n",
            "Batch Number: 353 Loss: 2.2523701190948486 Time taken: 0.299731969833374\n",
            "Batch Number: 354 Loss: 2.247148275375366 Time taken: 0.3161129951477051\n",
            "Batch Number: 355 Loss: 2.2384960651397705 Time taken: 0.2977025508880615\n",
            "Batch Number: 356 Loss: 2.2588279247283936 Time taken: 0.3060445785522461\n",
            "Batch Number: 357 Loss: 2.2501344680786133 Time taken: 0.3080439567565918\n",
            "Batch Number: 358 Loss: 2.2439770698547363 Time taken: 0.31123971939086914\n",
            "Batch Number: 359 Loss: 2.234144449234009 Time taken: 0.3072817325592041\n",
            "Batch Number: 360 Loss: 2.267383575439453 Time taken: 0.3226594924926758\n",
            "Batch Number: 361 Loss: 2.2473323345184326 Time taken: 0.32660961151123047\n",
            "Batch Number: 362 Loss: 2.2345192432403564 Time taken: 0.3055548667907715\n",
            "Batch Number: 363 Loss: 2.229301929473877 Time taken: 0.3799262046813965\n",
            "Batch Number: 364 Loss: 2.2302587032318115 Time taken: 0.3696022033691406\n",
            "Batch Number: 365 Loss: 2.2249224185943604 Time taken: 0.29887938499450684\n",
            "Batch Number: 366 Loss: 2.241727352142334 Time taken: 0.3137505054473877\n",
            "Batch Number: 367 Loss: 2.2080767154693604 Time taken: 0.34313488006591797\n",
            "Batch Number: 368 Loss: 2.2291347980499268 Time taken: 0.3058280944824219\n",
            "Batch Number: 369 Loss: 2.2247676849365234 Time taken: 0.30299854278564453\n",
            "Batch Number: 370 Loss: 2.224672317504883 Time taken: 0.33280301094055176\n",
            "Batch Number: 371 Loss: 2.2394652366638184 Time taken: 0.31882476806640625\n",
            "Batch Number: 372 Loss: 2.2321929931640625 Time taken: 0.2921121120452881\n",
            "Batch Number: 373 Loss: 2.239548921585083 Time taken: 0.3079824447631836\n",
            "Batch Number: 374 Loss: 2.2446393966674805 Time taken: 0.30437707901000977\n",
            "Batch Number: 375 Loss: 2.2505545616149902 Time taken: 0.3117098808288574\n",
            "Batch Number: 376 Loss: 2.286895513534546 Time taken: 0.32426905632019043\n",
            "Batch Number: 377 Loss: 2.2185730934143066 Time taken: 0.3160221576690674\n",
            "Batch Number: 378 Loss: 2.271209478378296 Time taken: 0.3051176071166992\n",
            "Batch Number: 379 Loss: 2.234199285507202 Time taken: 0.3134276866912842\n",
            "Batch Number: 380 Loss: 2.2262463569641113 Time taken: 0.3048553466796875\n",
            "Batch Number: 381 Loss: 2.217587947845459 Time taken: 0.29824113845825195\n",
            "Batch Number: 382 Loss: 2.2496986389160156 Time taken: 0.30565357208251953\n",
            "Batch Number: 383 Loss: 2.2720611095428467 Time taken: 0.3366279602050781\n",
            "Batch Number: 384 Loss: 2.234895706176758 Time taken: 0.2973058223724365\n",
            "Batch Number: 385 Loss: 2.2214043140411377 Time taken: 0.3004136085510254\n",
            "Batch Number: 386 Loss: 2.2298781871795654 Time taken: 0.2989940643310547\n",
            "Batch Number: 387 Loss: 2.210484266281128 Time taken: 0.30870532989501953\n",
            "Batch Number: 388 Loss: 2.2222812175750732 Time taken: 0.3061258792877197\n",
            "Batch Number: 389 Loss: 2.231323719024658 Time taken: 0.3118412494659424\n",
            "Batch Number: 390 Loss: 2.234057903289795 Time taken: 0.36566805839538574\n",
            "Batch Number: 391 Loss: 2.229651689529419 Time taken: 0.3786005973815918\n",
            "Batch Number: 392 Loss: 2.2092411518096924 Time taken: 0.33594536781311035\n",
            "Batch Number: 393 Loss: 2.2186667919158936 Time taken: 0.29906392097473145\n",
            "Batch Number: 394 Loss: 2.22102427482605 Time taken: 0.3026895523071289\n",
            "Batch Number: 395 Loss: 2.226590394973755 Time taken: 0.2934257984161377\n",
            "Batch Number: 396 Loss: 2.2359728813171387 Time taken: 0.3268897533416748\n",
            "Batch Number: 397 Loss: 2.226047992706299 Time taken: 0.38603663444519043\n",
            "Batch Number: 398 Loss: 2.223018169403076 Time taken: 0.3311316967010498\n",
            "Batch Number: 399 Loss: 2.2193825244903564 Time taken: 0.29976654052734375\n",
            "Batch Number: 400 Loss: 2.2234511375427246 Time taken: 0.32304906845092773\n",
            "Batch Number: 401 Loss: 2.2321932315826416 Time taken: 0.3354973793029785\n",
            "Batch Number: 402 Loss: 2.2311134338378906 Time taken: 0.3064553737640381\n",
            "Batch Number: 403 Loss: 2.1968016624450684 Time taken: 0.29709887504577637\n",
            "Batch Number: 404 Loss: 2.230346441268921 Time taken: 0.30457282066345215\n",
            "Batch Number: 405 Loss: 2.20544171333313 Time taken: 0.30736827850341797\n",
            "Batch Number: 406 Loss: 2.2173235416412354 Time taken: 0.30594754219055176\n",
            "Batch Number: 407 Loss: 2.2028603553771973 Time taken: 0.3620259761810303\n",
            "Batch Number: 408 Loss: 2.194512367248535 Time taken: 0.31435513496398926\n",
            "Batch Number: 409 Loss: 2.203709363937378 Time taken: 0.35781359672546387\n",
            "Batch Number: 410 Loss: 2.2159202098846436 Time taken: 0.36142921447753906\n",
            "Batch Number: 411 Loss: 2.21414852142334 Time taken: 0.34659600257873535\n",
            "Batch Number: 412 Loss: 2.2358202934265137 Time taken: 0.37043213844299316\n",
            "Batch Number: 413 Loss: 2.2413711547851562 Time taken: 0.3530998229980469\n",
            "Batch Number: 414 Loss: 2.2142138481140137 Time taken: 0.31850433349609375\n",
            "Batch Number: 415 Loss: 2.2278356552124023 Time taken: 0.2997598648071289\n",
            "Batch Number: 416 Loss: 2.201829671859741 Time taken: 0.30908894538879395\n",
            "Batch Number: 417 Loss: 2.2047717571258545 Time taken: 0.31339573860168457\n",
            "Batch Number: 418 Loss: 2.203547954559326 Time taken: 0.2997770309448242\n",
            "Batch Number: 419 Loss: 2.1994800567626953 Time taken: 0.34253716468811035\n",
            "Batch Number: 420 Loss: 2.200995683670044 Time taken: 0.3963165283203125\n",
            "Batch Number: 421 Loss: 2.1771187782287598 Time taken: 0.3658933639526367\n",
            "Batch Number: 422 Loss: 2.194690465927124 Time taken: 0.32837390899658203\n",
            "Batch Number: 423 Loss: 2.1914474964141846 Time taken: 0.38116955757141113\n",
            "Batch Number: 424 Loss: 2.1889286041259766 Time taken: 0.31189894676208496\n",
            "Batch Number: 425 Loss: 2.190589427947998 Time taken: 0.3272664546966553\n",
            "Batch Number: 426 Loss: 2.2010891437530518 Time taken: 0.3265976905822754\n",
            "Batch Number: 427 Loss: 2.200770854949951 Time taken: 0.36861157417297363\n",
            "Batch Number: 428 Loss: 2.207066297531128 Time taken: 0.31014204025268555\n",
            "Batch Number: 429 Loss: 2.197222948074341 Time taken: 0.3091747760772705\n",
            "Batch Number: 430 Loss: 2.2101426124572754 Time taken: 0.3066117763519287\n",
            "Batch Number: 431 Loss: 2.2019340991973877 Time taken: 0.3308908939361572\n",
            "Batch Number: 432 Loss: 2.1998844146728516 Time taken: 0.37871479988098145\n",
            "Batch Number: 433 Loss: 2.1988067626953125 Time taken: 0.3609490394592285\n",
            "Batch Number: 434 Loss: 2.204244613647461 Time taken: 0.3111753463745117\n",
            "Batch Number: 435 Loss: 2.206840753555298 Time taken: 0.3166191577911377\n",
            "Batch Number: 436 Loss: 2.213982582092285 Time taken: 0.3203907012939453\n",
            "Batch Number: 437 Loss: 2.20182466506958 Time taken: 0.3343641757965088\n",
            "Batch Number: 438 Loss: 2.1858952045440674 Time taken: 0.3116946220397949\n",
            "Batch Number: 439 Loss: 2.1901655197143555 Time taken: 0.29529428482055664\n",
            "Batch Number: 440 Loss: 2.1750223636627197 Time taken: 0.3072319030761719\n",
            "Batch Number: 441 Loss: 2.195704221725464 Time taken: 0.3167397975921631\n",
            "Batch Number: 442 Loss: 2.1824114322662354 Time taken: 0.3014674186706543\n",
            "Batch Number: 443 Loss: 2.1818466186523438 Time taken: 0.33468079566955566\n",
            "Batch Number: 444 Loss: 2.184325933456421 Time taken: 0.3835330009460449\n",
            "Batch Number: 445 Loss: 2.1920244693756104 Time taken: 0.3737955093383789\n",
            "Batch Number: 446 Loss: 2.194187641143799 Time taken: 0.31448984146118164\n",
            "Batch Number: 447 Loss: 2.1823792457580566 Time taken: 0.3122868537902832\n",
            "Batch Number: 448 Loss: 2.191636800765991 Time taken: 0.3066878318786621\n",
            "Batch Number: 449 Loss: 2.189624309539795 Time taken: 0.36055898666381836\n",
            "Batch Number: 450 Loss: 2.1857550144195557 Time taken: 0.3717503547668457\n",
            "Batch Number: 451 Loss: 2.209050416946411 Time taken: 0.35282421112060547\n",
            "Batch Number: 452 Loss: 2.204329013824463 Time taken: 0.29807424545288086\n",
            "Batch Number: 453 Loss: 2.2102150917053223 Time taken: 0.3196985721588135\n",
            "Batch Number: 454 Loss: 2.2065632343292236 Time taken: 0.3034381866455078\n",
            "Batch Number: 455 Loss: 2.195707321166992 Time taken: 0.3209652900695801\n",
            "Batch Number: 456 Loss: 2.207218885421753 Time taken: 0.3047330379486084\n",
            "Batch Number: 457 Loss: 2.1983447074890137 Time taken: 0.36669492721557617\n",
            "Batch Number: 458 Loss: 2.2059895992279053 Time taken: 0.29871296882629395\n",
            "Batch Number: 459 Loss: 2.20479154586792 Time taken: 0.302365779876709\n",
            "Batch Number: 460 Loss: 2.2186543941497803 Time taken: 0.33135533332824707\n",
            "Batch Number: 461 Loss: 2.2094950675964355 Time taken: 0.34562182426452637\n",
            "Batch Number: 462 Loss: 2.1974194049835205 Time taken: 0.3852217197418213\n",
            "Batch Number: 463 Loss: 2.214524269104004 Time taken: 0.3708946704864502\n",
            "Batch Number: 464 Loss: 2.2210936546325684 Time taken: 0.3178598880767822\n",
            "Batch Number: 465 Loss: 2.216224193572998 Time taken: 0.31008005142211914\n",
            "Batch Number: 466 Loss: 2.1931748390197754 Time taken: 0.31617236137390137\n",
            "Batch Number: 467 Loss: 2.2122642993927 Time taken: 0.31894397735595703\n",
            "Batch Number: 468 Loss: 2.2039852142333984 Time taken: 0.3020296096801758\n",
            "Batch Number: 469 Loss: 2.1812634468078613 Time taken: 0.3481571674346924\n",
            "Batch Number: 470 Loss: 2.208655834197998 Time taken: 0.3193483352661133\n",
            "Batch Number: 471 Loss: 2.207143783569336 Time taken: 0.3101074695587158\n",
            "Batch Number: 472 Loss: 2.212418794631958 Time taken: 0.34911012649536133\n",
            "Batch Number: 473 Loss: 2.187865972518921 Time taken: 0.3505878448486328\n",
            "Batch Number: 474 Loss: 2.2030155658721924 Time taken: 0.38457298278808594\n",
            "Batch Number: 475 Loss: 2.177091598510742 Time taken: 0.33217692375183105\n",
            "Batch Number: 476 Loss: 2.1946465969085693 Time taken: 0.2977566719055176\n",
            "Batch Number: 477 Loss: 2.1807496547698975 Time taken: 0.3128335475921631\n",
            "Batch Number: 478 Loss: 2.1933999061584473 Time taken: 0.32090330123901367\n",
            "Batch Number: 479 Loss: 2.194889545440674 Time taken: 0.3792445659637451\n",
            "Batch Number: 480 Loss: 2.1835646629333496 Time taken: 0.3864929676055908\n",
            "Batch Number: 481 Loss: 2.212000846862793 Time taken: 0.32471489906311035\n",
            "Batch Number: 482 Loss: 2.1927037239074707 Time taken: 0.36350584030151367\n",
            "Batch Number: 483 Loss: 2.196113109588623 Time taken: 0.3859081268310547\n",
            "Batch Number: 484 Loss: 2.1670620441436768 Time taken: 0.3433821201324463\n",
            "Batch Number: 485 Loss: 2.1818792819976807 Time taken: 0.30550146102905273\n",
            "Batch Number: 486 Loss: 2.160830497741699 Time taken: 0.3138740062713623\n",
            "Batch Number: 487 Loss: 2.1844308376312256 Time taken: 0.3168201446533203\n",
            "Batch Number: 488 Loss: 2.194336175918579 Time taken: 0.2978384494781494\n",
            "Batch Number: 489 Loss: 2.1648380756378174 Time taken: 0.30078649520874023\n",
            "Batch Number: 490 Loss: 2.2154886722564697 Time taken: 0.3184494972229004\n",
            "Batch Number: 491 Loss: 2.1801917552948 Time taken: 0.3023664951324463\n",
            "Batch Number: 492 Loss: 2.1921639442443848 Time taken: 0.35068798065185547\n",
            "Batch Number: 493 Loss: 2.2037341594696045 Time taken: 0.32584238052368164\n",
            "Batch Number: 494 Loss: 2.197277545928955 Time taken: 0.3196594715118408\n",
            "Batch Number: 495 Loss: 2.1678500175476074 Time taken: 0.33899807929992676\n",
            "Batch Number: 496 Loss: 2.189887046813965 Time taken: 0.32759857177734375\n",
            "Batch Number: 497 Loss: 2.1916396617889404 Time taken: 0.30866289138793945\n",
            "Batch Number: 498 Loss: 2.1740875244140625 Time taken: 0.30104875564575195\n",
            "Batch Number: 499 Loss: 2.1912059783935547 Time taken: 0.31002235412597656\n",
            "Batch Number: 500 Loss: 2.1704611778259277 Time taken: 0.29222798347473145\n",
            "Batch Number: 501 Loss: 2.199927806854248 Time taken: 0.28295016288757324\n",
            "Batch Number: 502 Loss: 2.1894705295562744 Time taken: 0.33550286293029785\n",
            "Batch Number: 503 Loss: 2.1900863647460938 Time taken: 0.37291741371154785\n",
            "Batch Number: 504 Loss: 2.215848445892334 Time taken: 0.3702542781829834\n",
            "Batch Number: 505 Loss: 2.174950122833252 Time taken: 0.33142685890197754\n",
            "Batch Number: 506 Loss: 2.1868863105773926 Time taken: 0.31903696060180664\n",
            "Batch Number: 507 Loss: 2.1973278522491455 Time taken: 0.30178046226501465\n",
            "Batch Number: 508 Loss: 2.2010788917541504 Time taken: 0.32173800468444824\n",
            "Batch Number: 509 Loss: 2.1632463932037354 Time taken: 0.3073117733001709\n",
            "Batch Number: 510 Loss: 2.1827778816223145 Time taken: 0.3135089874267578\n",
            "Batch Number: 511 Loss: 2.1697068214416504 Time taken: 0.303936243057251\n",
            "Batch Number: 512 Loss: 2.181691884994507 Time taken: 0.2986466884613037\n",
            "Batch Number: 513 Loss: 2.1741437911987305 Time taken: 0.30178117752075195\n",
            "Batch Number: 514 Loss: 2.185459852218628 Time taken: 0.309431791305542\n",
            "Batch Number: 515 Loss: 2.1779391765594482 Time taken: 0.31220054626464844\n",
            "Batch Number: 516 Loss: 2.169867515563965 Time taken: 0.3153269290924072\n",
            "Batch Number: 517 Loss: 2.173394203186035 Time taken: 0.32576608657836914\n",
            "Batch Number: 518 Loss: 2.1834018230438232 Time taken: 0.3219001293182373\n",
            "Batch Number: 519 Loss: 2.2115073204040527 Time taken: 0.3158559799194336\n",
            "Batch Number: 520 Loss: 2.210327386856079 Time taken: 0.3840677738189697\n",
            "Batch Number: 521 Loss: 2.1871893405914307 Time taken: 0.3189239501953125\n",
            "Batch Number: 522 Loss: 2.1795620918273926 Time taken: 0.3179042339324951\n",
            "Batch Number: 523 Loss: 2.166163206100464 Time taken: 0.30012989044189453\n",
            "Batch Number: 524 Loss: 2.181396245956421 Time taken: 0.3215181827545166\n",
            "Batch Number: 525 Loss: 2.1862072944641113 Time taken: 0.32092785835266113\n",
            "Batch Number: 526 Loss: 2.159595012664795 Time taken: 0.3076913356781006\n",
            "Batch Number: 527 Loss: 2.1691930294036865 Time taken: 0.3760824203491211\n",
            "Batch Number: 528 Loss: 2.1741442680358887 Time taken: 0.32389044761657715\n",
            "Batch Number: 529 Loss: 2.165796995162964 Time taken: 0.2974390983581543\n",
            "Batch Number: 530 Loss: 2.172532796859741 Time taken: 0.3129885196685791\n",
            "Batch Number: 531 Loss: 2.158327102661133 Time taken: 0.31901073455810547\n",
            "Batch Number: 532 Loss: 2.164139747619629 Time taken: 0.3030104637145996\n",
            "Batch Number: 533 Loss: 2.1739556789398193 Time taken: 0.34339308738708496\n",
            "Batch Number: 534 Loss: 2.159390687942505 Time taken: 0.30618834495544434\n",
            "Batch Number: 535 Loss: 2.1624672412872314 Time taken: 0.3073701858520508\n",
            "Batch Number: 536 Loss: 2.129194974899292 Time taken: 0.3048715591430664\n",
            "Batch Number: 537 Loss: 2.149319648742676 Time taken: 0.289135217666626\n",
            "Batch Number: 538 Loss: 2.159396171569824 Time taken: 0.3044004440307617\n",
            "Batch Number: 539 Loss: 2.136641263961792 Time taken: 0.3492155075073242\n",
            "Batch Number: 540 Loss: 2.14705753326416 Time taken: 0.293748140335083\n",
            "Batch Number: 541 Loss: 2.130997896194458 Time taken: 0.32181382179260254\n",
            "Batch Number: 542 Loss: 2.1432080268859863 Time taken: 0.3310697078704834\n",
            "Batch Number: 543 Loss: 2.129295825958252 Time taken: 0.31290602684020996\n",
            "Batch Number: 544 Loss: 2.136960983276367 Time taken: 0.3032867908477783\n",
            "Batch Number: 545 Loss: 2.1501355171203613 Time taken: 0.3201413154602051\n",
            "Batch Number: 546 Loss: 2.1499361991882324 Time taken: 0.37625646591186523\n",
            "Batch Number: 547 Loss: 2.1335813999176025 Time taken: 0.3730649948120117\n",
            "Batch Number: 548 Loss: 2.125025987625122 Time taken: 0.3426046371459961\n",
            "Batch Number: 549 Loss: 2.1224558353424072 Time taken: 0.3056302070617676\n",
            "Batch Number: 550 Loss: 2.114982843399048 Time taken: 0.3037388324737549\n",
            "Batch Number: 551 Loss: 2.139228582382202 Time taken: 0.30382633209228516\n",
            "Batch Number: 552 Loss: 2.1531600952148438 Time taken: 0.29234862327575684\n",
            "Batch Number: 553 Loss: 2.144057273864746 Time taken: 0.30295538902282715\n",
            "Batch Number: 554 Loss: 2.1816482543945312 Time taken: 0.2898731231689453\n",
            "Batch Number: 555 Loss: 2.208836793899536 Time taken: 0.30555224418640137\n",
            "Batch Number: 556 Loss: 2.190175771713257 Time taken: 0.301724910736084\n",
            "Batch Number: 557 Loss: 2.1401588916778564 Time taken: 0.29750967025756836\n",
            "Batch Number: 558 Loss: 2.1858103275299072 Time taken: 0.3063831329345703\n",
            "Batch Number: 559 Loss: 2.1640939712524414 Time taken: 0.30829787254333496\n",
            "Batch Number: 560 Loss: 2.1980440616607666 Time taken: 0.36335110664367676\n",
            "Batch Number: 561 Loss: 2.1277923583984375 Time taken: 0.29820728302001953\n",
            "Batch Number: 562 Loss: 2.149653673171997 Time taken: 0.3585073947906494\n",
            "Batch Number: 563 Loss: 2.1741509437561035 Time taken: 0.3674020767211914\n",
            "Batch Number: 564 Loss: 2.158444404602051 Time taken: 0.3099360466003418\n",
            "Batch Number: 565 Loss: 2.1398138999938965 Time taken: 0.3123347759246826\n",
            "Batch Number: 566 Loss: 2.1527721881866455 Time taken: 0.30861520767211914\n",
            "Batch Number: 567 Loss: 2.136594295501709 Time taken: 0.2979617118835449\n",
            "Batch Number: 568 Loss: 2.144909143447876 Time taken: 0.31021833419799805\n",
            "Batch Number: 569 Loss: 2.130645275115967 Time taken: 0.3079361915588379\n",
            "Batch Number: 570 Loss: 2.1430983543395996 Time taken: 0.31666016578674316\n",
            "Batch Number: 571 Loss: 2.142740249633789 Time taken: 0.3079037666320801\n",
            "Batch Number: 572 Loss: 2.1420745849609375 Time taken: 0.3100612163543701\n",
            "Batch Number: 573 Loss: 2.1393275260925293 Time taken: 0.3175067901611328\n",
            "Batch Number: 574 Loss: 2.1394007205963135 Time taken: 0.2924220561981201\n",
            "Batch Number: 575 Loss: 2.1463186740875244 Time taken: 0.30071520805358887\n",
            "Batch Number: 576 Loss: 2.117731809616089 Time taken: 0.3311643600463867\n",
            "Batch Number: 577 Loss: 2.1410555839538574 Time taken: 0.3031620979309082\n",
            "Batch Number: 578 Loss: 2.1378111839294434 Time taken: 0.31840062141418457\n",
            "Batch Number: 579 Loss: 2.142646074295044 Time taken: 0.29392337799072266\n",
            "Batch Number: 580 Loss: 2.1560723781585693 Time taken: 0.2991600036621094\n",
            "Batch Number: 581 Loss: 2.1364846229553223 Time taken: 0.3156096935272217\n",
            "Batch Number: 582 Loss: 2.1345741748809814 Time taken: 0.3485696315765381\n",
            "Batch Number: 583 Loss: 2.1322643756866455 Time taken: 0.32091426849365234\n",
            "Batch Number: 584 Loss: 2.1351852416992188 Time taken: 0.2937934398651123\n",
            "Batch Number: 585 Loss: 2.1260838508605957 Time taken: 0.29432058334350586\n",
            "Batch Number: 586 Loss: 2.1220877170562744 Time taken: 0.32836413383483887\n",
            "Batch Number: 587 Loss: 2.125915050506592 Time taken: 0.3059229850769043\n",
            "Batch Number: 588 Loss: 2.1454432010650635 Time taken: 0.30542659759521484\n",
            "Batch Number: 589 Loss: 2.130488157272339 Time taken: 0.3238043785095215\n",
            "Batch Number: 590 Loss: 2.1347739696502686 Time taken: 0.3009014129638672\n",
            "Batch Number: 591 Loss: 2.138657808303833 Time taken: 0.34786319732666016\n",
            "Batch Number: 592 Loss: 2.155819892883301 Time taken: 0.33588171005249023\n",
            "Batch Number: 593 Loss: 2.134730815887451 Time taken: 0.3146703243255615\n",
            "Batch Number: 594 Loss: 2.156846046447754 Time taken: 0.30771899223327637\n",
            "Batch Number: 595 Loss: 2.1350533962249756 Time taken: 0.3143758773803711\n",
            "Batch Number: 596 Loss: 2.148061513900757 Time taken: 0.30246734619140625\n",
            "Batch Number: 597 Loss: 2.1352779865264893 Time taken: 0.3207240104675293\n",
            "Batch Number: 598 Loss: 2.110567569732666 Time taken: 0.29333949089050293\n",
            "Batch Number: 599 Loss: 2.1469919681549072 Time taken: 0.304973840713501\n",
            "Batch Number: 600 Loss: 2.128734588623047 Time taken: 0.3127727508544922\n",
            "Batch Number: 601 Loss: 2.124354839324951 Time taken: 0.31711626052856445\n",
            "Batch Number: 602 Loss: 2.1227166652679443 Time taken: 0.31051063537597656\n",
            "Batch Number: 603 Loss: 2.0986814498901367 Time taken: 0.30705881118774414\n",
            "Batch Number: 604 Loss: 2.114269733428955 Time taken: 0.30368757247924805\n",
            "Batch Number: 605 Loss: 2.108149290084839 Time taken: 0.2996499538421631\n",
            "Batch Number: 606 Loss: 2.1163456439971924 Time taken: 0.3384392261505127\n",
            "Batch Number: 607 Loss: 2.137143611907959 Time taken: 0.37919116020202637\n",
            "Batch Number: 608 Loss: 2.133580207824707 Time taken: 0.31339526176452637\n",
            "Batch Number: 609 Loss: 2.1210885047912598 Time taken: 0.3041064739227295\n",
            "Batch Number: 610 Loss: 2.124988555908203 Time taken: 0.3132517337799072\n",
            "Batch Number: 611 Loss: 2.1118454933166504 Time taken: 0.3093912601470947\n",
            "Batch Number: 612 Loss: 2.1308345794677734 Time taken: 0.300551176071167\n",
            "Batch Number: 613 Loss: 2.135139226913452 Time taken: 0.31920957565307617\n",
            "Batch Number: 614 Loss: 2.1360385417938232 Time taken: 0.3110077381134033\n",
            "Batch Number: 615 Loss: 2.1497650146484375 Time taken: 0.3050262928009033\n",
            "Batch Number: 616 Loss: 2.1312055587768555 Time taken: 0.34463953971862793\n",
            "Batch Number: 617 Loss: 2.133786916732788 Time taken: 0.3114135265350342\n",
            "Batch Number: 618 Loss: 2.1096503734588623 Time taken: 0.36545825004577637\n",
            "Batch Number: 619 Loss: 2.1312520503997803 Time taken: 0.3967592716217041\n",
            "Batch Number: 620 Loss: 2.1220176219940186 Time taken: 0.29465246200561523\n",
            "Batch Number: 621 Loss: 2.113823652267456 Time taken: 0.2824881076812744\n",
            "Batch Number: 622 Loss: 2.122157096862793 Time taken: 0.30353355407714844\n",
            "Batch Number: 623 Loss: 2.1214964389801025 Time taken: 0.30754709243774414\n",
            "Batch Number: 624 Loss: 2.0983874797821045 Time taken: 0.29599714279174805\n",
            "Batch Number: 625 Loss: 2.108154773712158 Time taken: 0.3113241195678711\n",
            "Batch Number: 626 Loss: 2.1137120723724365 Time taken: 0.34813809394836426\n",
            "Batch Number: 627 Loss: 2.111827850341797 Time taken: 0.29552555084228516\n",
            "Batch Number: 628 Loss: 2.0986080169677734 Time taken: 0.30809950828552246\n",
            "Batch Number: 629 Loss: 2.110628128051758 Time taken: 0.31859850883483887\n",
            "Batch Number: 630 Loss: 2.1041877269744873 Time taken: 0.301084041595459\n",
            "Batch Number: 631 Loss: 2.132668972015381 Time taken: 0.2907702922821045\n",
            "Batch Number: 632 Loss: 2.1398088932037354 Time taken: 0.3489053249359131\n",
            "Batch Number: 633 Loss: 2.134392738342285 Time taken: 0.2963097095489502\n",
            "Batch Number: 634 Loss: 2.103757381439209 Time taken: 0.2979435920715332\n",
            "Batch Number: 635 Loss: 2.1393582820892334 Time taken: 0.3293614387512207\n",
            "Batch Number: 636 Loss: 2.146517515182495 Time taken: 0.3076515197753906\n",
            "Batch Number: 637 Loss: 2.1140711307525635 Time taken: 0.3087887763977051\n",
            "Batch Number: 638 Loss: 2.1351983547210693 Time taken: 0.3165006637573242\n",
            "Batch Number: 639 Loss: 2.128558397293091 Time taken: 0.3102583885192871\n",
            "Batch Number: 640 Loss: 2.151325225830078 Time taken: 0.2979304790496826\n",
            "Batch Number: 641 Loss: 2.1260576248168945 Time taken: 0.30838680267333984\n",
            "Batch Number: 642 Loss: 2.1401255130767822 Time taken: 0.30379819869995117\n",
            "Batch Number: 643 Loss: 2.129977226257324 Time taken: 0.30191612243652344\n",
            "Batch Number: 644 Loss: 2.1422924995422363 Time taken: 0.31832218170166016\n",
            "Batch Number: 645 Loss: 2.1248779296875 Time taken: 0.34532856941223145\n",
            "Batch Number: 646 Loss: 2.1335222721099854 Time taken: 0.36016154289245605\n",
            "Batch Number: 647 Loss: 2.125657558441162 Time taken: 0.389315128326416\n",
            "Batch Number: 648 Loss: 2.11875581741333 Time taken: 0.30069422721862793\n",
            "Batch Number: 649 Loss: 2.1356923580169678 Time taken: 0.2960333824157715\n",
            "Batch Number: 650 Loss: 2.109869956970215 Time taken: 0.28972792625427246\n",
            "Batch Number: 651 Loss: 2.119513988494873 Time taken: 0.3333165645599365\n",
            "Batch Number: 652 Loss: 2.117974042892456 Time taken: 0.30306005477905273\n",
            "Batch Number: 653 Loss: 2.1218056678771973 Time taken: 0.29797935485839844\n",
            "Batch Number: 654 Loss: 2.144320011138916 Time taken: 0.30868101119995117\n",
            "Batch Number: 655 Loss: 2.122699737548828 Time taken: 0.30205202102661133\n",
            "Batch Number: 656 Loss: 2.111144781112671 Time taken: 0.3019411563873291\n",
            "Batch Number: 657 Loss: 2.0999398231506348 Time taken: 0.3408372402191162\n",
            "Batch Number: 658 Loss: 2.1194941997528076 Time taken: 0.32532668113708496\n",
            "Batch Number: 659 Loss: 2.1101131439208984 Time taken: 0.29134130477905273\n",
            "Batch Number: 660 Loss: 2.1178574562072754 Time taken: 0.3066112995147705\n",
            "Batch Number: 661 Loss: 2.1193037033081055 Time taken: 0.37700724601745605\n",
            "Batch Number: 662 Loss: 2.1215217113494873 Time taken: 0.36501240730285645\n",
            "Batch Number: 663 Loss: 2.1246399879455566 Time taken: 0.34003376960754395\n",
            "Batch Number: 664 Loss: 2.109466075897217 Time taken: 0.3530566692352295\n",
            "Batch Number: 665 Loss: 2.103480100631714 Time taken: 0.3695218563079834\n",
            "Batch Number: 666 Loss: 2.091515302658081 Time taken: 0.3359205722808838\n",
            "Batch Number: 667 Loss: 2.0867342948913574 Time taken: 0.29753661155700684\n",
            "Batch Number: 668 Loss: 2.109783172607422 Time taken: 0.30927038192749023\n",
            "Batch Number: 669 Loss: 2.1085219383239746 Time taken: 0.33150529861450195\n",
            "Batch Number: 670 Loss: 2.107866048812866 Time taken: 0.34268975257873535\n",
            "Batch Number: 671 Loss: 2.1008458137512207 Time taken: 0.2951781749725342\n",
            "Batch Number: 672 Loss: 2.1228268146514893 Time taken: 0.3172736167907715\n",
            "Batch Number: 673 Loss: 2.1120569705963135 Time taken: 0.3082430362701416\n",
            "Batch Number: 674 Loss: 2.131457805633545 Time taken: 0.30510687828063965\n",
            "Batch Number: 675 Loss: 2.1184611320495605 Time taken: 0.35349130630493164\n",
            "Batch Number: 676 Loss: 2.1044821739196777 Time taken: 0.3039836883544922\n",
            "Batch Number: 677 Loss: 2.111025094985962 Time taken: 0.31029224395751953\n",
            "Batch Number: 678 Loss: 2.106665849685669 Time taken: 0.325397253036499\n",
            "Batch Number: 679 Loss: 2.0970563888549805 Time taken: 0.3086369037628174\n",
            "Batch Number: 680 Loss: 2.126269817352295 Time taken: 0.29785943031311035\n",
            "Batch Number: 681 Loss: 2.1140406131744385 Time taken: 0.30322718620300293\n",
            "Batch Number: 682 Loss: 2.1019601821899414 Time taken: 0.3772001266479492\n",
            "Batch Number: 683 Loss: 2.105403184890747 Time taken: 0.36519598960876465\n",
            "Batch Number: 684 Loss: 2.0948238372802734 Time taken: 0.3282959461212158\n",
            "Batch Number: 685 Loss: 2.1115734577178955 Time taken: 0.3399178981781006\n",
            "Batch Number: 686 Loss: 2.0807394981384277 Time taken: 0.36671996116638184\n",
            "Batch Number: 687 Loss: 2.122394561767578 Time taken: 0.34880685806274414\n",
            "Batch Number: 688 Loss: 2.099573850631714 Time taken: 0.33303213119506836\n",
            "Batch Number: 689 Loss: 2.10380482673645 Time taken: 0.38707828521728516\n",
            "Batch Number: 690 Loss: 2.085801839828491 Time taken: 0.31810569763183594\n",
            "Batch Number: 691 Loss: 2.114047050476074 Time taken: 0.2998955249786377\n",
            "Batch Number: 692 Loss: 2.092625617980957 Time taken: 0.2910912036895752\n",
            "Batch Number: 693 Loss: 2.116507053375244 Time taken: 0.2921435832977295\n",
            "Batch Number: 694 Loss: 2.0951695442199707 Time taken: 0.3591628074645996\n",
            "Batch Number: 695 Loss: 2.1178369522094727 Time taken: 0.34258174896240234\n",
            "Batch Number: 696 Loss: 2.0902786254882812 Time taken: 0.3153676986694336\n",
            "Batch Number: 697 Loss: 2.10739803314209 Time taken: 0.3166313171386719\n",
            "Batch Number: 698 Loss: 2.103732109069824 Time taken: 0.3292081356048584\n",
            "Batch Number: 699 Loss: 2.1039044857025146 Time taken: 0.366253137588501\n",
            "Batch Number: 700 Loss: 2.1105856895446777 Time taken: 0.3896050453186035\n",
            "Batch Number: 701 Loss: 2.1044318675994873 Time taken: 0.2944064140319824\n",
            "Batch Number: 702 Loss: 2.111135244369507 Time taken: 0.3286900520324707\n",
            "Batch Number: 703 Loss: 2.112973690032959 Time taken: 0.3131258487701416\n",
            "Batch Number: 704 Loss: 2.104670286178589 Time taken: 0.30167222023010254\n",
            "Batch Number: 705 Loss: 2.0968215465545654 Time taken: 0.3532404899597168\n",
            "Batch Number: 706 Loss: 2.0949056148529053 Time taken: 0.33342695236206055\n",
            "Batch Number: 707 Loss: 2.0976669788360596 Time taken: 0.3187215328216553\n",
            "Batch Number: 708 Loss: 2.0894668102264404 Time taken: 0.2986772060394287\n",
            "Batch Number: 709 Loss: 2.0889697074890137 Time taken: 0.31549763679504395\n",
            "Batch Number: 710 Loss: 2.093428611755371 Time taken: 0.2988002300262451\n",
            "Batch Number: 711 Loss: 2.10367751121521 Time taken: 0.33115720748901367\n",
            "Batch Number: 712 Loss: 2.0764987468719482 Time taken: 0.31611013412475586\n",
            "Batch Number: 713 Loss: 2.0595109462738037 Time taken: 0.2968008518218994\n",
            "Batch Number: 714 Loss: 2.0827724933624268 Time taken: 0.32055234909057617\n",
            "Batch Number: 715 Loss: 2.0945520401000977 Time taken: 0.30716466903686523\n",
            "Batch Number: 716 Loss: 2.0754616260528564 Time taken: 0.2899293899536133\n",
            "Batch Number: 717 Loss: 2.078193187713623 Time taken: 0.3617570400238037\n",
            "Batch Number: 718 Loss: 2.0821964740753174 Time taken: 0.3852238655090332\n",
            "Batch Number: 719 Loss: 2.071824073791504 Time taken: 0.3566558361053467\n",
            "Batch Number: 720 Loss: 2.0715765953063965 Time taken: 0.3227579593658447\n",
            "Batch Number: 721 Loss: 2.091550827026367 Time taken: 0.40180373191833496\n",
            "Batch Number: 722 Loss: 2.076582908630371 Time taken: 0.38733458518981934\n",
            "Batch Number: 723 Loss: 2.0477921962738037 Time taken: 0.3213162422180176\n",
            "Batch Number: 724 Loss: 2.0545825958251953 Time taken: 0.33065271377563477\n",
            "Batch Number: 725 Loss: 2.067697286605835 Time taken: 0.2960193157196045\n",
            "Batch Number: 726 Loss: 2.0596375465393066 Time taken: 0.2999715805053711\n",
            "Batch Number: 727 Loss: 2.055084228515625 Time taken: 0.30930519104003906\n",
            "Batch Number: 728 Loss: 2.0556931495666504 Time taken: 0.30299973487854004\n",
            "Batch Number: 729 Loss: 2.0601563453674316 Time taken: 0.30892491340637207\n",
            "Batch Number: 730 Loss: 2.067662477493286 Time taken: 0.4006786346435547\n",
            "Batch Number: 731 Loss: 2.071565628051758 Time taken: 0.36060094833374023\n",
            "Batch Number: 732 Loss: 2.0828943252563477 Time taken: 0.3219935894012451\n",
            "Batch Number: 733 Loss: 2.072563409805298 Time taken: 0.3158273696899414\n",
            "Batch Number: 734 Loss: 2.104100465774536 Time taken: 0.29613733291625977\n",
            "Batch Number: 735 Loss: 2.1531646251678467 Time taken: 0.33351826667785645\n",
            "Batch Number: 736 Loss: 2.1722822189331055 Time taken: 0.30171704292297363\n",
            "Batch Number: 737 Loss: 2.199786424636841 Time taken: 0.31394433975219727\n",
            "Batch Number: 738 Loss: 2.1280441284179688 Time taken: 0.2996490001678467\n",
            "Batch Number: 739 Loss: 2.121471643447876 Time taken: 0.3072531223297119\n",
            "Batch Number: 740 Loss: 2.1111507415771484 Time taken: 0.32118988037109375\n",
            "Batch Number: 741 Loss: 2.1445071697235107 Time taken: 0.3240809440612793\n",
            "Batch Number: 742 Loss: 2.116248846054077 Time taken: 0.3078348636627197\n",
            "Batch Number: 743 Loss: 2.1055331230163574 Time taken: 0.31391167640686035\n",
            "Batch Number: 744 Loss: 2.0895938873291016 Time taken: 0.29755282402038574\n",
            "Batch Number: 745 Loss: 2.102483034133911 Time taken: 0.30858540534973145\n",
            "Batch Number: 746 Loss: 2.103889226913452 Time taken: 0.375286340713501\n",
            "Batch Number: 747 Loss: 2.0871329307556152 Time taken: 0.34996938705444336\n",
            "Batch Number: 748 Loss: 2.0923640727996826 Time taken: 0.3740103244781494\n",
            "Batch Number: 749 Loss: 2.100339412689209 Time taken: 0.3872804641723633\n",
            "Batch Number: 750 Loss: 2.1106832027435303 Time taken: 0.3003504276275635\n",
            "Batch Number: 751 Loss: 2.0757620334625244 Time taken: 0.2943241596221924\n",
            "Batch Number: 752 Loss: 2.1039884090423584 Time taken: 0.30617785453796387\n",
            "Batch Number: 753 Loss: 2.0800681114196777 Time taken: 0.35521960258483887\n",
            "Batch Number: 754 Loss: 2.0820558071136475 Time taken: 0.2968735694885254\n",
            "Batch Number: 755 Loss: 2.069472312927246 Time taken: 0.31691908836364746\n",
            "Batch Number: 756 Loss: 2.0760998725891113 Time taken: 0.29694318771362305\n",
            "Batch Number: 757 Loss: 2.0683486461639404 Time taken: 0.29982495307922363\n",
            "Batch Number: 758 Loss: 2.062896966934204 Time taken: 0.3191101551055908\n",
            "Batch Number: 759 Loss: 2.088803768157959 Time taken: 0.3177053928375244\n",
            "Batch Number: 760 Loss: 2.0620193481445312 Time taken: 0.3096787929534912\n",
            "Batch Number: 761 Loss: 2.075955867767334 Time taken: 0.3188745975494385\n",
            "Batch Number: 762 Loss: 2.0573625564575195 Time taken: 0.3259153366088867\n",
            "Batch Number: 763 Loss: 2.0744664669036865 Time taken: 0.2937953472137451\n",
            "Batch Number: 764 Loss: 2.070711612701416 Time taken: 0.30153822898864746\n",
            "Batch Number: 765 Loss: 2.052096366882324 Time taken: 0.3772411346435547\n",
            "Batch Number: 766 Loss: 2.0642549991607666 Time taken: 0.3618342876434326\n",
            "Batch Number: 767 Loss: 2.0648889541625977 Time taken: 0.38785862922668457\n",
            "Batch Number: 768 Loss: 2.0702831745147705 Time taken: 0.3306584358215332\n",
            "Batch Number: 769 Loss: 2.053466320037842 Time taken: 0.30020880699157715\n",
            "Batch Number: 770 Loss: 2.0586583614349365 Time taken: 0.2980046272277832\n",
            "Batch Number: 771 Loss: 2.091959238052368 Time taken: 0.31804656982421875\n",
            "Batch Number: 772 Loss: 2.0848734378814697 Time taken: 0.2950928211212158\n",
            "Batch Number: 773 Loss: 2.079904079437256 Time taken: 0.2945523262023926\n",
            "Batch Number: 774 Loss: 2.082658290863037 Time taken: 0.3033432960510254\n",
            "Batch Number: 775 Loss: 2.071945905685425 Time taken: 0.35115742683410645\n",
            "Batch Number: 776 Loss: 2.0717968940734863 Time taken: 0.3817250728607178\n",
            "Batch Number: 777 Loss: 2.0488479137420654 Time taken: 0.365220308303833\n",
            "Batch Number: 778 Loss: 2.0612683296203613 Time taken: 0.29921889305114746\n",
            "Batch Number: 779 Loss: 2.04719877243042 Time taken: 0.30908966064453125\n",
            "Batch Number: 780 Loss: 2.034715414047241 Time taken: 0.3081188201904297\n",
            "Batch Number: 781 Loss: 2.048982620239258 Time taken: 0.3007636070251465\n",
            "Batch Number: 782 Loss: 2.056757926940918 Time taken: 0.30235838890075684\n",
            "Batch Number: 783 Loss: 2.05561900138855 Time taken: 0.324840784072876\n",
            "Batch Number: 784 Loss: 2.043085813522339 Time taken: 0.31085920333862305\n",
            "Batch Number: 785 Loss: 2.0627338886260986 Time taken: 0.326235294342041\n",
            "Batch Number: 786 Loss: 2.042088270187378 Time taken: 0.3186161518096924\n",
            "Batch Number: 787 Loss: 2.056687116622925 Time taken: 0.30147624015808105\n",
            "Batch Number: 788 Loss: 2.0504956245422363 Time taken: 0.2999553680419922\n",
            "Batch Number: 789 Loss: 2.039774179458618 Time taken: 0.3132176399230957\n",
            "Batch Number: 790 Loss: 2.049361228942871 Time taken: 0.325437068939209\n",
            "Batch Number: 791 Loss: 2.062175750732422 Time taken: 0.29538559913635254\n",
            "Batch Number: 792 Loss: 2.0639145374298096 Time taken: 0.32143664360046387\n",
            "Batch Number: 793 Loss: 2.070497989654541 Time taken: 0.337510347366333\n",
            "Batch Number: 794 Loss: 2.0488765239715576 Time taken: 0.2997307777404785\n",
            "Batch Number: 795 Loss: 2.0622153282165527 Time taken: 0.30394959449768066\n",
            "Batch Number: 796 Loss: 2.038287401199341 Time taken: 0.30901360511779785\n",
            "Batch Number: 797 Loss: 2.0657598972320557 Time taken: 0.30040764808654785\n",
            "Batch Number: 798 Loss: 2.04461407661438 Time taken: 0.29749298095703125\n",
            "Batch Number: 799 Loss: 2.0502140522003174 Time taken: 0.33255767822265625\n",
            "Batch Number: 800 Loss: 2.0725646018981934 Time taken: 0.35196948051452637\n",
            "Batch Number: 801 Loss: 2.0445399284362793 Time taken: 0.3394780158996582\n",
            "Batch Number: 802 Loss: 2.0522356033325195 Time taken: 0.30385661125183105\n",
            "Batch Number: 803 Loss: 2.047419309616089 Time taken: 0.30170226097106934\n",
            "Batch Number: 804 Loss: 2.045323371887207 Time taken: 0.2908446788787842\n",
            "Batch Number: 805 Loss: 2.044396162033081 Time taken: 0.3049774169921875\n",
            "Batch Number: 806 Loss: 2.0342650413513184 Time taken: 0.3685739040374756\n",
            "Batch Number: 807 Loss: 2.0463178157806396 Time taken: 0.364501953125\n",
            "Batch Number: 808 Loss: 2.032649278640747 Time taken: 0.37676334381103516\n",
            "Batch Number: 809 Loss: 2.046048164367676 Time taken: 0.30525994300842285\n",
            "Batch Number: 810 Loss: 2.0571722984313965 Time taken: 0.2932300567626953\n",
            "Batch Number: 811 Loss: 2.035545825958252 Time taken: 0.3156108856201172\n",
            "Batch Number: 812 Loss: 2.070040702819824 Time taken: 0.3081226348876953\n",
            "Batch Number: 813 Loss: 2.0429797172546387 Time taken: 0.30198192596435547\n",
            "Batch Number: 814 Loss: 2.0601279735565186 Time taken: 0.31317996978759766\n",
            "Batch Number: 815 Loss: 2.075627326965332 Time taken: 0.32156801223754883\n",
            "Batch Number: 816 Loss: 2.0705597400665283 Time taken: 0.3249537944793701\n",
            "Batch Number: 817 Loss: 2.058960199356079 Time taken: 0.3249208927154541\n",
            "Batch Number: 818 Loss: 2.0652384757995605 Time taken: 0.34247350692749023\n",
            "Batch Number: 819 Loss: 2.051640033721924 Time taken: 0.3017709255218506\n",
            "Batch Number: 820 Loss: 2.077474355697632 Time taken: 0.32354187965393066\n",
            "Batch Number: 821 Loss: 2.1109566688537598 Time taken: 0.35784244537353516\n",
            "Batch Number: 822 Loss: 2.103022575378418 Time taken: 0.3015177249908447\n",
            "Batch Number: 823 Loss: 2.078835964202881 Time taken: 0.35397791862487793\n",
            "Batch Number: 824 Loss: 2.076845645904541 Time taken: 0.32822680473327637\n",
            "Batch Number: 825 Loss: 2.069927930831909 Time taken: 0.3067638874053955\n",
            "Batch Number: 826 Loss: 2.067932367324829 Time taken: 0.34459519386291504\n",
            "Batch Number: 827 Loss: 2.0734851360321045 Time taken: 0.3594696521759033\n",
            "Batch Number: 828 Loss: 2.0520212650299072 Time taken: 0.38097476959228516\n",
            "Batch Number: 829 Loss: 2.071415424346924 Time taken: 0.36356663703918457\n",
            "Batch Number: 830 Loss: 2.0812594890594482 Time taken: 0.33408188819885254\n",
            "Batch Number: 831 Loss: 2.0678212642669678 Time taken: 0.3347623348236084\n",
            "Batch Number: 832 Loss: 2.0646140575408936 Time taken: 0.41294217109680176\n",
            "Batch Number: 833 Loss: 2.0775697231292725 Time taken: 0.3735687732696533\n",
            "Batch Number: 834 Loss: 2.0904171466827393 Time taken: 0.3584775924682617\n",
            "Batch Number: 835 Loss: 2.049376964569092 Time taken: 0.31503891944885254\n",
            "Batch Number: 836 Loss: 2.05712890625 Time taken: 0.31001806259155273\n",
            "Batch Number: 837 Loss: 2.0356767177581787 Time taken: 0.3256392478942871\n",
            "Batch Number: 838 Loss: 2.0368924140930176 Time taken: 0.33411121368408203\n",
            "Batch Number: 839 Loss: 2.07497501373291 Time taken: 0.31075596809387207\n",
            "Batch Number: 840 Loss: 2.043397903442383 Time taken: 0.31260037422180176\n",
            "Batch Number: 841 Loss: 2.0486040115356445 Time taken: 0.3046739101409912\n",
            "Batch Number: 842 Loss: 2.0617458820343018 Time taken: 0.3093373775482178\n",
            "Batch Number: 843 Loss: 2.0403409004211426 Time taken: 0.31044483184814453\n",
            "Batch Number: 844 Loss: 2.061478853225708 Time taken: 0.3374507427215576\n",
            "Batch Number: 845 Loss: 2.0272042751312256 Time taken: 0.3276247978210449\n",
            "Batch Number: 846 Loss: 2.0446410179138184 Time taken: 0.3056662082672119\n",
            "Batch Number: 847 Loss: 2.0264792442321777 Time taken: 0.30348896980285645\n",
            "Batch Number: 848 Loss: 2.0316007137298584 Time taken: 0.3136870861053467\n",
            "Batch Number: 849 Loss: 2.035968065261841 Time taken: 0.3032679557800293\n",
            "Batch Number: 850 Loss: 2.029099702835083 Time taken: 0.325822114944458\n",
            "Batch Number: 851 Loss: 2.059467315673828 Time taken: 0.2980523109436035\n",
            "Batch Number: 852 Loss: 2.048283338546753 Time taken: 0.29608869552612305\n",
            "Batch Number: 853 Loss: 2.0520803928375244 Time taken: 0.2990608215332031\n",
            "Batch Number: 854 Loss: 2.057365894317627 Time taken: 0.31047558784484863\n",
            "Batch Number: 855 Loss: 2.044837474822998 Time taken: 0.31925177574157715\n",
            "Batch Number: 856 Loss: 2.033721446990967 Time taken: 0.3505516052246094\n",
            "Batch Number: 857 Loss: 2.014941692352295 Time taken: 0.29142284393310547\n",
            "Batch Number: 858 Loss: 2.0149967670440674 Time taken: 0.29961538314819336\n",
            "Batch Number: 859 Loss: 2.0114634037017822 Time taken: 0.2985868453979492\n",
            "Batch Number: 860 Loss: 2.0161960124969482 Time taken: 0.30365562438964844\n",
            "Batch Number: 861 Loss: 2.064500570297241 Time taken: 0.3128175735473633\n",
            "Batch Number: 862 Loss: 2.0623483657836914 Time taken: 0.31255292892456055\n",
            "Batch Number: 863 Loss: 2.042891502380371 Time taken: 0.3342258930206299\n",
            "Batch Number: 864 Loss: 2.049975633621216 Time taken: 0.30486512184143066\n",
            "Batch Number: 865 Loss: 2.0343053340911865 Time taken: 0.35479044914245605\n",
            "Batch Number: 866 Loss: 2.033221483230591 Time taken: 0.3100011348724365\n",
            "Batch Number: 867 Loss: 2.0293362140655518 Time taken: 0.30530619621276855\n",
            "Batch Number: 868 Loss: 2.0307250022888184 Time taken: 0.30204296112060547\n",
            "Batch Number: 869 Loss: 2.010021924972534 Time taken: 0.3346896171569824\n",
            "Batch Number: 870 Loss: 2.015230178833008 Time taken: 0.383969783782959\n",
            "Batch Number: 871 Loss: 2.0456488132476807 Time taken: 0.36221933364868164\n",
            "Batch Number: 872 Loss: 2.0233516693115234 Time taken: 0.33599424362182617\n",
            "Batch Number: 873 Loss: 2.0528924465179443 Time taken: 0.3795166015625\n",
            "Batch Number: 874 Loss: 2.0549726486206055 Time taken: 0.3624439239501953\n",
            "Batch Number: 875 Loss: 2.01203989982605 Time taken: 0.31483888626098633\n",
            "Batch Number: 876 Loss: 2.038090944290161 Time taken: 0.30504822731018066\n",
            "Batch Number: 877 Loss: 2.0469655990600586 Time taken: 0.2975127696990967\n",
            "Batch Number: 878 Loss: 2.0460689067840576 Time taken: 0.3052048683166504\n",
            "Batch Number: 879 Loss: 2.038020610809326 Time taken: 0.31644320487976074\n",
            "Batch Number: 880 Loss: 2.0311174392700195 Time taken: 0.308469295501709\n",
            "Batch Number: 881 Loss: 2.0356619358062744 Time taken: 0.3349425792694092\n",
            "Batch Number: 882 Loss: 2.033315420150757 Time taken: 0.3420870304107666\n",
            "Batch Number: 883 Loss: 2.0274412631988525 Time taken: 0.35372185707092285\n",
            "Batch Number: 884 Loss: 2.0214602947235107 Time taken: 0.3177363872528076\n",
            "Batch Number: 885 Loss: 2.0217883586883545 Time taken: 0.35588598251342773\n",
            "Batch Number: 886 Loss: 2.0264623165130615 Time taken: 0.2840137481689453\n",
            "Batch Number: 887 Loss: 2.0215725898742676 Time taken: 0.3130190372467041\n",
            "Batch Number: 888 Loss: 2.035771131515503 Time taken: 0.3023867607116699\n",
            "Batch Number: 889 Loss: 2.027862310409546 Time taken: 0.3126859664916992\n",
            "Batch Number: 890 Loss: 2.0050015449523926 Time taken: 0.30194735527038574\n",
            "Batch Number: 891 Loss: 2.0176773071289062 Time taken: 0.3120596408843994\n",
            "Batch Number: 892 Loss: 2.0021438598632812 Time taken: 0.3434574604034424\n",
            "Batch Number: 893 Loss: 2.0079588890075684 Time taken: 0.3268570899963379\n",
            "Batch Number: 894 Loss: 2.024312734603882 Time taken: 0.3149240016937256\n",
            "Batch Number: 895 Loss: 2.007503032684326 Time taken: 0.3035757541656494\n",
            "Batch Number: 896 Loss: 2.0111706256866455 Time taken: 0.31018877029418945\n",
            "Batch Number: 897 Loss: 2.0139219760894775 Time taken: 0.3545956611633301\n",
            "Batch Number: 898 Loss: 1.9999535083770752 Time taken: 0.37213993072509766\n",
            "Batch Number: 899 Loss: 1.9948785305023193 Time taken: 0.3329737186431885\n",
            "Batch Number: 900 Loss: 2.017543077468872 Time taken: 0.30439281463623047\n",
            "Batch Number: 901 Loss: 2.0012009143829346 Time taken: 0.3097991943359375\n",
            "Batch Number: 902 Loss: 1.9968491792678833 Time taken: 0.29532647132873535\n",
            "Batch Number: 903 Loss: 1.9955706596374512 Time taken: 0.30895066261291504\n",
            "Batch Number: 904 Loss: 1.9984831809997559 Time taken: 0.3089935779571533\n",
            "Batch Number: 905 Loss: 1.9820222854614258 Time taken: 0.3398268222808838\n",
            "Batch Number: 906 Loss: 1.9818801879882812 Time taken: 0.3748300075531006\n",
            "Batch Number: 907 Loss: 2.0128636360168457 Time taken: 0.38578104972839355\n",
            "Batch Number: 908 Loss: 1.9825209379196167 Time taken: 0.30445146560668945\n",
            "Batch Number: 909 Loss: 2.0007355213165283 Time taken: 0.3067343235015869\n",
            "Batch Number: 910 Loss: 1.9668605327606201 Time taken: 0.3048865795135498\n",
            "Batch Number: 911 Loss: 1.9921215772628784 Time taken: 0.3596196174621582\n",
            "Batch Number: 912 Loss: 2.005603313446045 Time taken: 0.3779327869415283\n",
            "Batch Number: 913 Loss: 2.0047950744628906 Time taken: 0.3799471855163574\n",
            "Batch Number: 914 Loss: 2.0191118717193604 Time taken: 0.3240854740142822\n",
            "Batch Number: 915 Loss: 2.0295701026916504 Time taken: 0.3219470977783203\n",
            "Batch Number: 916 Loss: 2.015456199645996 Time taken: 0.3735661506652832\n",
            "Batch Number: 917 Loss: 2.0135486125946045 Time taken: 0.31791162490844727\n",
            "Batch Number: 918 Loss: 1.9950722455978394 Time taken: 0.30266833305358887\n",
            "Batch Number: 919 Loss: 1.976142406463623 Time taken: 0.30799102783203125\n",
            "Batch Number: 920 Loss: 1.9928480386734009 Time taken: 0.32282137870788574\n",
            "Batch Number: 921 Loss: 1.9666815996170044 Time taken: 0.308743953704834\n",
            "Batch Number: 922 Loss: 1.982633352279663 Time taken: 0.31427526473999023\n",
            "Batch Number: 923 Loss: 2.0008034706115723 Time taken: 0.2999110221862793\n",
            "Batch Number: 924 Loss: 1.9990527629852295 Time taken: 0.30284667015075684\n",
            "Batch Number: 925 Loss: 2.006751537322998 Time taken: 0.28929734230041504\n",
            "Batch Number: 926 Loss: 2.0039594173431396 Time taken: 0.30088043212890625\n",
            "Batch Number: 927 Loss: 1.989559292793274 Time taken: 0.2977871894836426\n",
            "Batch Number: 928 Loss: 1.9998806715011597 Time taken: 0.28798627853393555\n",
            "Batch Number: 929 Loss: 2.0044162273406982 Time taken: 0.3245992660522461\n",
            "Batch Number: 930 Loss: 1.997176170349121 Time taken: 0.37398433685302734\n",
            "Batch Number: 931 Loss: 1.9961544275283813 Time taken: 0.36362719535827637\n",
            "Batch Number: 932 Loss: 2.010246753692627 Time taken: 0.316694974899292\n",
            "Batch Number: 933 Loss: 2.0099828243255615 Time taken: 0.30493903160095215\n",
            "Batch Number: 934 Loss: 2.0007708072662354 Time taken: 0.292008638381958\n",
            "Batch Number: 935 Loss: 1.9948084354400635 Time taken: 0.3334383964538574\n",
            "Batch Number: 936 Loss: 2.0041959285736084 Time taken: 0.2970414161682129\n",
            "Batch Number: 937 Loss: 2.0086233615875244 Time taken: 0.30223655700683594\n",
            "Batch Number: 938 Loss: 2.0197644233703613 Time taken: 0.3224525451660156\n",
            "Batch Number: 939 Loss: 2.0259475708007812 Time taken: 0.2951819896697998\n",
            "Batch Number: 940 Loss: 1.9947535991668701 Time taken: 0.29822778701782227\n",
            "Batch Number: 941 Loss: 1.9863462448120117 Time taken: 0.2858879566192627\n",
            "Batch Number: 942 Loss: 2.001303195953369 Time taken: 0.3087022304534912\n",
            "Batch Number: 943 Loss: 1.9720900058746338 Time taken: 0.31870174407958984\n",
            "Batch Number: 944 Loss: 1.9886382818222046 Time taken: 0.2919142246246338\n",
            "Batch Number: 945 Loss: 1.9775866270065308 Time taken: 0.30967140197753906\n",
            "Batch Number: 946 Loss: 1.9906809329986572 Time taken: 0.32001304626464844\n",
            "Batch Number: 947 Loss: 2.015582799911499 Time taken: 0.29230332374572754\n",
            "Batch Number: 948 Loss: 1.998423457145691 Time taken: 0.29546213150024414\n",
            "Batch Number: 949 Loss: 1.9992687702178955 Time taken: 0.2824399471282959\n",
            "Batch Number: 950 Loss: 1.9966306686401367 Time taken: 0.3066105842590332\n",
            "Batch Number: 951 Loss: 2.003542900085449 Time taken: 0.29917383193969727\n",
            "Batch Number: 952 Loss: 1.9913063049316406 Time taken: 0.3110365867614746\n",
            "Batch Number: 953 Loss: 2.0103020668029785 Time taken: 0.2987782955169678\n",
            "Batch Number: 954 Loss: 2.0222744941711426 Time taken: 0.30017638206481934\n",
            "Batch Number: 955 Loss: 1.984790325164795 Time taken: 0.3258371353149414\n",
            "Batch Number: 956 Loss: 1.9962743520736694 Time taken: 0.35097622871398926\n",
            "Batch Number: 957 Loss: 1.984325885772705 Time taken: 0.30011653900146484\n",
            "Batch Number: 958 Loss: 2.006479024887085 Time taken: 0.3223392963409424\n",
            "Batch Number: 959 Loss: 1.9999347925186157 Time taken: 0.2989633083343506\n",
            "Batch Number: 960 Loss: 1.9912233352661133 Time taken: 0.29014015197753906\n",
            "Batch Number: 961 Loss: 1.9789597988128662 Time taken: 0.33290982246398926\n",
            "Batch Number: 962 Loss: 1.9770244359970093 Time taken: 0.30228710174560547\n",
            "Batch Number: 963 Loss: 1.9763790369033813 Time taken: 0.36553025245666504\n",
            "Batch Number: 964 Loss: 1.9709938764572144 Time taken: 0.32485175132751465\n",
            "Batch Number: 965 Loss: 1.9830163717269897 Time taken: 0.29543113708496094\n",
            "Batch Number: 966 Loss: 1.9761744737625122 Time taken: 0.31501150131225586\n",
            "Batch Number: 967 Loss: 1.984525442123413 Time taken: 0.3355278968811035\n",
            "Batch Number: 968 Loss: 1.978686809539795 Time taken: 0.30919528007507324\n",
            "Batch Number: 969 Loss: 1.9821621179580688 Time taken: 0.3006618022918701\n",
            "Batch Number: 970 Loss: 1.9829726219177246 Time taken: 0.29567861557006836\n",
            "Batch Number: 971 Loss: 1.9834034442901611 Time taken: 0.3117105960845947\n",
            "Batch Number: 972 Loss: 1.9888997077941895 Time taken: 0.30759620666503906\n",
            "Batch Number: 973 Loss: 2.0082387924194336 Time taken: 0.31014275550842285\n",
            "Batch Number: 974 Loss: 1.9821910858154297 Time taken: 0.29871034622192383\n",
            "Batch Number: 975 Loss: 1.9898970127105713 Time taken: 0.2955622673034668\n",
            "Batch Number: 976 Loss: 1.9907859563827515 Time taken: 0.32196855545043945\n",
            "Batch Number: 977 Loss: 2.0050837993621826 Time taken: 0.3144383430480957\n",
            "Batch Number: 978 Loss: 1.9759407043457031 Time taken: 0.30468249320983887\n",
            "Batch Number: 979 Loss: 1.9898054599761963 Time taken: 0.337188720703125\n",
            "Batch Number: 980 Loss: 2.022550344467163 Time taken: 0.3076953887939453\n",
            "Batch Number: 981 Loss: 2.028667449951172 Time taken: 0.29631543159484863\n",
            "Batch Number: 982 Loss: 2.0032565593719482 Time taken: 0.3080780506134033\n",
            "Batch Number: 983 Loss: 1.9939935207366943 Time taken: 0.29839372634887695\n",
            "Batch Number: 984 Loss: 2.0063257217407227 Time taken: 0.3180413246154785\n",
            "Batch Number: 985 Loss: 1.966497540473938 Time taken: 0.3633003234863281\n",
            "Batch Number: 986 Loss: 1.9956334829330444 Time taken: 0.2891695499420166\n",
            "Batch Number: 987 Loss: 1.979447364807129 Time taken: 0.32187557220458984\n",
            "Batch Number: 988 Loss: 1.9948863983154297 Time taken: 0.325253963470459\n",
            "Batch Number: 989 Loss: 1.969430685043335 Time taken: 0.34925270080566406\n",
            "Batch Number: 990 Loss: 1.975266695022583 Time taken: 0.3746347427368164\n",
            "Batch Number: 991 Loss: 1.995042324066162 Time taken: 0.3760378360748291\n",
            "Batch Number: 992 Loss: 1.9897739887237549 Time taken: 0.31252026557922363\n",
            "Batch Number: 993 Loss: 2.0086145401000977 Time taken: 0.30285143852233887\n",
            "Batch Number: 994 Loss: 2.0126869678497314 Time taken: 0.3048515319824219\n",
            "Batch Number: 995 Loss: 2.023186683654785 Time taken: 0.29654622077941895\n",
            "Batch Number: 996 Loss: 1.9913426637649536 Time taken: 0.3000941276550293\n",
            "Batch Number: 997 Loss: 2.01613187789917 Time taken: 0.30022215843200684\n",
            "Batch Number: 998 Loss: 1.9947797060012817 Time taken: 0.318432092666626\n",
            "Batch Number: 999 Loss: 2.031134605407715 Time taken: 0.306255578994751\n",
            "Batch Number: 1000 Loss: 1.9982556104660034 Time taken: 0.2990446090698242\n",
            "Batch Number: 1001 Loss: 2.0148937702178955 Time taken: 0.30278825759887695\n",
            "Batch Number: 1002 Loss: 2.014875650405884 Time taken: 0.29622411727905273\n",
            "Batch Number: 1003 Loss: 1.99140202999115 Time taken: 0.3096044063568115\n",
            "Batch Number: 1004 Loss: 2.018287420272827 Time taken: 0.30356597900390625\n",
            "Batch Number: 1005 Loss: 1.9953056573867798 Time taken: 0.35102128982543945\n",
            "Batch Number: 1006 Loss: 2.0179026126861572 Time taken: 0.3326225280761719\n",
            "Batch Number: 1007 Loss: 2.005025863647461 Time taken: 0.3320004940032959\n",
            "Batch Number: 1008 Loss: 2.0287046432495117 Time taken: 0.3054194450378418\n",
            "Batch Number: 1009 Loss: 1.9945669174194336 Time taken: 0.3087034225463867\n",
            "Batch Number: 1010 Loss: 2.0221219062805176 Time taken: 0.30489444732666016\n",
            "Batch Number: 1011 Loss: 2.001319646835327 Time taken: 0.29856419563293457\n",
            "Batch Number: 1012 Loss: 2.018043041229248 Time taken: 0.33550214767456055\n",
            "Batch Number: 1013 Loss: 1.9907660484313965 Time taken: 0.3118245601654053\n",
            "Batch Number: 1014 Loss: 1.9896080493927002 Time taken: 0.3591468334197998\n",
            "Batch Number: 1015 Loss: 1.9939665794372559 Time taken: 0.39221954345703125\n",
            "Batch Number: 1016 Loss: 1.9939966201782227 Time taken: 0.3977656364440918\n",
            "Batch Number: 1017 Loss: 1.9767574071884155 Time taken: 0.3732779026031494\n",
            "Batch Number: 1018 Loss: 2.0010313987731934 Time taken: 0.3861503601074219\n",
            "Batch Number: 1019 Loss: 1.9736238718032837 Time taken: 0.30878663063049316\n",
            "Batch Number: 1020 Loss: 2.000810384750366 Time taken: 0.2966923713684082\n",
            "Batch Number: 1021 Loss: 1.9989522695541382 Time taken: 0.3189413547515869\n",
            "Batch Number: 1022 Loss: 1.970667839050293 Time taken: 0.30091309547424316\n",
            "Batch Number: 1023 Loss: 1.989938497543335 Time taken: 0.29862141609191895\n",
            "Batch Number: 1024 Loss: 2.0113143920898438 Time taken: 0.3164811134338379\n",
            "Batch Number: 1025 Loss: 1.974164605140686 Time taken: 0.30615806579589844\n",
            "Batch Number: 1026 Loss: 1.979280710220337 Time taken: 0.2966339588165283\n",
            "Batch Number: 1027 Loss: 2.00076961517334 Time taken: 0.3030107021331787\n",
            "Batch Number: 1028 Loss: 2.008410930633545 Time taken: 0.3132915496826172\n",
            "Batch Number: 1029 Loss: 1.967637300491333 Time taken: 0.37153172492980957\n",
            "Batch Number: 1030 Loss: 1.9837684631347656 Time taken: 0.37269067764282227\n",
            "Batch Number: 1031 Loss: 1.950913429260254 Time taken: 0.3066701889038086\n",
            "Batch Number: 1032 Loss: 1.967301845550537 Time taken: 0.3438866138458252\n",
            "Batch Number: 1033 Loss: 1.9839723110198975 Time taken: 0.37440061569213867\n",
            "Batch Number: 1034 Loss: 1.9761711359024048 Time taken: 0.3351879119873047\n",
            "Batch Number: 1035 Loss: 1.9659942388534546 Time taken: 0.29949235916137695\n",
            "Batch Number: 1036 Loss: 1.952883005142212 Time taken: 0.3005540370941162\n",
            "Batch Number: 1037 Loss: 1.9560743570327759 Time taken: 0.32531070709228516\n",
            "Batch Number: 1038 Loss: 1.9524449110031128 Time taken: 0.29259490966796875\n",
            "Batch Number: 1039 Loss: 1.9626753330230713 Time taken: 0.31117844581604004\n",
            "Batch Number: 1040 Loss: 1.9609614610671997 Time taken: 0.32630419731140137\n",
            "Batch Number: 1041 Loss: 1.988187551498413 Time taken: 0.30271220207214355\n",
            "Batch Number: 1042 Loss: 1.9909394979476929 Time taken: 0.3037998676300049\n",
            "Batch Number: 1043 Loss: 1.9616979360580444 Time taken: 0.30948758125305176\n",
            "Batch Number: 1044 Loss: 1.99726402759552 Time taken: 0.3321564197540283\n",
            "Batch Number: 1045 Loss: 1.9670346975326538 Time taken: 0.2987325191497803\n",
            "Batch Number: 1046 Loss: 2.004204273223877 Time taken: 0.3496971130371094\n",
            "Batch Number: 1047 Loss: 1.9912029504776 Time taken: 0.2888674736022949\n",
            "Batch Number: 1048 Loss: 1.971373438835144 Time taken: 0.33963966369628906\n",
            "Batch Number: 1049 Loss: 1.9789386987686157 Time taken: 0.35148143768310547\n",
            "Batch Number: 1050 Loss: 1.9492250680923462 Time taken: 0.2933485507965088\n",
            "Batch Number: 1051 Loss: 1.9745538234710693 Time taken: 0.30942869186401367\n",
            "Batch Number: 1052 Loss: 1.953513264656067 Time taken: 0.3560149669647217\n",
            "Batch Number: 1053 Loss: 1.968554973602295 Time taken: 0.36237406730651855\n",
            "Batch Number: 1054 Loss: 1.958235502243042 Time taken: 0.37050962448120117\n",
            "Batch Number: 1055 Loss: 1.9741569757461548 Time taken: 0.3442211151123047\n",
            "Batch Number: 1056 Loss: 1.970898985862732 Time taken: 0.37259578704833984\n",
            "Batch Number: 1057 Loss: 1.9791998863220215 Time taken: 0.3642995357513428\n",
            "Batch Number: 1058 Loss: 1.9866186380386353 Time taken: 0.3245396614074707\n",
            "Batch Number: 1059 Loss: 1.9710966348648071 Time taken: 0.29668521881103516\n",
            "Batch Number: 1060 Loss: 1.9569708108901978 Time taken: 0.2900569438934326\n",
            "Batch Number: 1061 Loss: 1.953995943069458 Time taken: 0.3188185691833496\n",
            "Batch Number: 1062 Loss: 1.9662684202194214 Time taken: 0.30058932304382324\n",
            "Batch Number: 1063 Loss: 1.966812252998352 Time taken: 0.31701016426086426\n",
            "Batch Number: 1064 Loss: 1.9857256412506104 Time taken: 0.3151984214782715\n",
            "Batch Number: 1065 Loss: 1.9603476524353027 Time taken: 0.28868532180786133\n",
            "Batch Number: 1066 Loss: 1.9803047180175781 Time taken: 0.29009222984313965\n",
            "Batch Number: 1067 Loss: 1.952027440071106 Time taken: 0.31614184379577637\n",
            "Batch Number: 1068 Loss: 1.96283757686615 Time taken: 0.3463406562805176\n",
            "Batch Number: 1069 Loss: 1.9664536714553833 Time taken: 0.36443233489990234\n",
            "Batch Number: 1070 Loss: 1.9772565364837646 Time taken: 0.38089704513549805\n",
            "Batch Number: 1071 Loss: 1.9574319124221802 Time taken: 0.3026854991912842\n",
            "Batch Number: 1072 Loss: 1.9523155689239502 Time taken: 0.27902650833129883\n",
            "Batch Number: 1073 Loss: 1.9323500394821167 Time taken: 0.3029201030731201\n",
            "Batch Number: 1074 Loss: 1.9584354162216187 Time taken: 0.30541300773620605\n",
            "Batch Number: 1075 Loss: 1.944433569908142 Time taken: 0.3156931400299072\n",
            "Batch Number: 1076 Loss: 1.957257866859436 Time taken: 0.28922176361083984\n",
            "Batch Number: 1077 Loss: 1.9508662223815918 Time taken: 0.3102688789367676\n",
            "Batch Number: 1078 Loss: 1.938133955001831 Time taken: 0.3000814914703369\n",
            "Batch Number: 1079 Loss: 1.9413665533065796 Time taken: 0.3081095218658447\n",
            "Batch Number: 1080 Loss: 1.9722821712493896 Time taken: 0.33104467391967773\n",
            "Batch Number: 1081 Loss: 1.9644725322723389 Time taken: 0.31310081481933594\n",
            "Batch Number: 1082 Loss: 1.9239733219146729 Time taken: 0.30274248123168945\n",
            "Batch Number: 1083 Loss: 1.917824625968933 Time taken: 0.34700632095336914\n",
            "Batch Number: 1084 Loss: 1.924172043800354 Time taken: 0.3563535213470459\n",
            "Batch Number: 1085 Loss: 1.9343427419662476 Time taken: 0.3729262351989746\n",
            "Batch Number: 1086 Loss: 1.9418216943740845 Time taken: 0.3520667552947998\n",
            "Batch Number: 1087 Loss: 1.9335837364196777 Time taken: 0.3718600273132324\n",
            "Batch Number: 1088 Loss: 1.9207512140274048 Time taken: 0.3867990970611572\n",
            "Batch Number: 1089 Loss: 1.9429008960723877 Time taken: 0.31868958473205566\n",
            "Batch Number: 1090 Loss: 1.9102801084518433 Time taken: 0.30088376998901367\n",
            "Batch Number: 1091 Loss: 1.9311853647232056 Time taken: 0.346940279006958\n",
            "Batch Number: 1092 Loss: 1.946094274520874 Time taken: 0.32353901863098145\n",
            "Batch Number: 1093 Loss: 1.9338222742080688 Time taken: 0.3018984794616699\n",
            "Batch Number: 1094 Loss: 1.9615933895111084 Time taken: 0.29798054695129395\n",
            "Batch Number: 1095 Loss: 1.9402735233306885 Time taken: 0.32648491859436035\n",
            "Batch Number: 1096 Loss: 1.9490435123443604 Time taken: 0.3065474033355713\n",
            "Batch Number: 1097 Loss: 1.951015591621399 Time taken: 0.29294919967651367\n",
            "Batch Number: 1098 Loss: 1.944332242012024 Time taken: 0.3286106586456299\n",
            "Batch Number: 1099 Loss: 1.9418001174926758 Time taken: 0.2908306121826172\n",
            "Batch Number: 1100 Loss: 1.9274495840072632 Time taken: 0.28723740577697754\n",
            "Batch Number: 1101 Loss: 1.9153597354888916 Time taken: 0.33599328994750977\n",
            "Batch Number: 1102 Loss: 1.9316588640213013 Time taken: 0.38910961151123047\n",
            "Batch Number: 1103 Loss: 1.923607587814331 Time taken: 0.36379265785217285\n",
            "Batch Number: 1104 Loss: 1.9380853176116943 Time taken: 0.3626596927642822\n",
            "Batch Number: 1105 Loss: 1.949377417564392 Time taken: 0.37032580375671387\n",
            "Batch Number: 1106 Loss: 1.9497615098953247 Time taken: 0.3769717216491699\n",
            "Batch Number: 1107 Loss: 1.9287549257278442 Time taken: 0.41342759132385254\n",
            "Batch Number: 1108 Loss: 1.9311699867248535 Time taken: 0.32314205169677734\n",
            "Batch Number: 1109 Loss: 1.9341908693313599 Time taken: 0.30454373359680176\n",
            "Batch Number: 1110 Loss: 1.9310671091079712 Time taken: 0.3038187026977539\n",
            "Batch Number: 1111 Loss: 1.951980710029602 Time taken: 0.29178428649902344\n",
            "Batch Number: 1112 Loss: 1.9765304327011108 Time taken: 0.29573822021484375\n",
            "Batch Number: 1113 Loss: 1.9545817375183105 Time taken: 0.3104584217071533\n",
            "Batch Number: 1114 Loss: 1.9408899545669556 Time taken: 0.2980623245239258\n",
            "Batch Number: 1115 Loss: 1.9416545629501343 Time taken: 0.3036942481994629\n",
            "Batch Number: 1116 Loss: 1.9165637493133545 Time taken: 0.30930089950561523\n",
            "Batch Number: 1117 Loss: 1.928475022315979 Time taken: 0.32076501846313477\n",
            "Batch Number: 1118 Loss: 1.9487013816833496 Time taken: 0.3076772689819336\n",
            "Batch Number: 1119 Loss: 1.9484564065933228 Time taken: 0.36136579513549805\n",
            "Batch Number: 1120 Loss: 1.954088807106018 Time taken: 0.38633084297180176\n",
            "Batch Number: 1121 Loss: 1.9474143981933594 Time taken: 0.35021138191223145\n",
            "Batch Number: 1122 Loss: 1.9460346698760986 Time taken: 0.30169224739074707\n",
            "Batch Number: 1123 Loss: 1.917323112487793 Time taken: 0.3130664825439453\n",
            "Batch Number: 1124 Loss: 1.929608702659607 Time taken: 0.31574010848999023\n",
            "Batch Number: 1125 Loss: 1.9342254400253296 Time taken: 0.2874913215637207\n",
            "Batch Number: 1126 Loss: 1.9297062158584595 Time taken: 0.30570316314697266\n",
            "Batch Number: 1127 Loss: 1.9167232513427734 Time taken: 0.2953338623046875\n",
            "Batch Number: 1128 Loss: 1.9434075355529785 Time taken: 0.3074502944946289\n",
            "Batch Number: 1129 Loss: 1.9251681566238403 Time taken: 0.32140064239501953\n",
            "Batch Number: 1130 Loss: 1.9328892230987549 Time taken: 0.29589200019836426\n",
            "Batch Number: 1131 Loss: 1.9415326118469238 Time taken: 0.3139007091522217\n",
            "Batch Number: 1132 Loss: 1.9421969652175903 Time taken: 0.3080317974090576\n",
            "Batch Number: 1133 Loss: 1.9464244842529297 Time taken: 0.29745960235595703\n",
            "Batch Number: 1134 Loss: 1.96022367477417 Time taken: 0.29264283180236816\n",
            "Batch Number: 1135 Loss: 1.9377474784851074 Time taken: 0.3004796504974365\n",
            "Batch Number: 1136 Loss: 1.925132155418396 Time taken: 0.3039073944091797\n",
            "Batch Number: 1137 Loss: 1.9493739604949951 Time taken: 0.3041956424713135\n",
            "Batch Number: 1138 Loss: 1.9403433799743652 Time taken: 0.29599952697753906\n",
            "Batch Number: 1139 Loss: 1.9276310205459595 Time taken: 0.31632089614868164\n",
            "Batch Number: 1140 Loss: 1.9078397750854492 Time taken: 0.32977771759033203\n",
            "Batch Number: 1141 Loss: 1.920866847038269 Time taken: 0.2958364486694336\n",
            "Batch Number: 1142 Loss: 1.913685917854309 Time taken: 0.3151524066925049\n",
            "Batch Number: 1143 Loss: 1.9321770668029785 Time taken: 0.29857635498046875\n",
            "Batch Number: 1144 Loss: 1.9319392442703247 Time taken: 0.34823131561279297\n",
            "Batch Number: 1145 Loss: 1.924892544746399 Time taken: 0.38686323165893555\n",
            "Batch Number: 1146 Loss: 1.9510161876678467 Time taken: 0.3412959575653076\n",
            "Batch Number: 1147 Loss: 1.9380189180374146 Time taken: 0.29644060134887695\n",
            "Batch Number: 1148 Loss: 1.9357680082321167 Time taken: 0.31533193588256836\n",
            "Batch Number: 1149 Loss: 1.939131736755371 Time taken: 0.31821560859680176\n",
            "Batch Number: 1150 Loss: 1.92588472366333 Time taken: 0.3275458812713623\n",
            "Batch Number: 1151 Loss: 1.9286751747131348 Time taken: 0.3884439468383789\n",
            "Batch Number: 1152 Loss: 1.9424662590026855 Time taken: 0.3350942134857178\n",
            "Batch Number: 1153 Loss: 1.931331753730774 Time taken: 0.29853081703186035\n",
            "Batch Number: 1154 Loss: 1.9320690631866455 Time taken: 0.3122217655181885\n",
            "Batch Number: 1155 Loss: 1.9494959115982056 Time taken: 0.31149721145629883\n",
            "Batch Number: 1156 Loss: 1.9270529747009277 Time taken: 0.3398869037628174\n",
            "Batch Number: 1157 Loss: 1.9415225982666016 Time taken: 0.3376502990722656\n",
            "Batch Number: 1158 Loss: 1.9507269859313965 Time taken: 0.3521413803100586\n",
            "Batch Number: 1159 Loss: 1.9510160684585571 Time taken: 0.3044297695159912\n",
            "Batch Number: 1160 Loss: 1.9232518672943115 Time taken: 0.3160223960876465\n",
            "Batch Number: 1161 Loss: 1.9223706722259521 Time taken: 0.32947492599487305\n",
            "Batch Number: 1162 Loss: 1.952615737915039 Time taken: 0.3036329746246338\n",
            "Batch Number: 1163 Loss: 1.9196927547454834 Time taken: 0.3054234981536865\n",
            "Batch Number: 1164 Loss: 1.9299250841140747 Time taken: 0.3118550777435303\n",
            "Batch Number: 1165 Loss: 1.9372986555099487 Time taken: 0.3212580680847168\n",
            "Batch Number: 1166 Loss: 1.9336706399917603 Time taken: 0.37600040435791016\n",
            "Batch Number: 1167 Loss: 1.9023956060409546 Time taken: 0.37537288665771484\n",
            "Batch Number: 1168 Loss: 1.9143927097320557 Time taken: 0.3426830768585205\n",
            "Batch Number: 1169 Loss: 1.9247536659240723 Time taken: 0.39424729347229004\n",
            "Batch Number: 1170 Loss: 1.9232347011566162 Time taken: 0.34070587158203125\n",
            "Batch Number: 1171 Loss: 1.9331916570663452 Time taken: 0.3100290298461914\n",
            "Batch Number: 1172 Loss: 1.9316647052764893 Time taken: 0.322770357131958\n",
            "Batch Number: 1173 Loss: 1.9529955387115479 Time taken: 0.3119063377380371\n",
            "Batch Number: 1174 Loss: 1.9324012994766235 Time taken: 0.3023054599761963\n",
            "Batch Number: 1175 Loss: 1.9407422542572021 Time taken: 0.3016011714935303\n",
            "Batch Number: 1176 Loss: 1.947352409362793 Time taken: 0.31369876861572266\n",
            "Batch Number: 1177 Loss: 1.9540029764175415 Time taken: 0.2903625965118408\n",
            "Batch Number: 1178 Loss: 1.9526913166046143 Time taken: 0.30822014808654785\n",
            "Batch Number: 1179 Loss: 1.930017113685608 Time taken: 0.3187704086303711\n",
            "Batch Number: 1180 Loss: 1.9527201652526855 Time taken: 0.36249852180480957\n",
            "Batch Number: 1181 Loss: 1.9453214406967163 Time taken: 0.37796854972839355\n",
            "Batch Number: 1182 Loss: 1.9565216302871704 Time taken: 0.3718223571777344\n",
            "Batch Number: 1183 Loss: 1.942307710647583 Time taken: 0.2960376739501953\n",
            "Batch Number: 1184 Loss: 1.9474185705184937 Time taken: 0.30985164642333984\n",
            "Batch Number: 1185 Loss: 1.9591562747955322 Time taken: 0.3377513885498047\n",
            "Batch Number: 1186 Loss: 1.9567415714263916 Time taken: 0.3023993968963623\n",
            "Batch Number: 1187 Loss: 1.9488815069198608 Time taken: 0.29908108711242676\n",
            "Batch Number: 1188 Loss: 1.9714436531066895 Time taken: 0.3435795307159424\n",
            "Batch Number: 1189 Loss: 1.9408178329467773 Time taken: 0.2897522449493408\n",
            "Batch Number: 1190 Loss: 1.9617950916290283 Time taken: 0.29209470748901367\n",
            "Batch Number: 1191 Loss: 1.972965121269226 Time taken: 0.3265054225921631\n",
            "Batch Number: 1192 Loss: 1.9379850625991821 Time taken: 0.34485888481140137\n",
            "Batch Number: 1193 Loss: 1.9643137454986572 Time taken: 0.37081360816955566\n",
            "Batch Number: 1194 Loss: 1.971604347229004 Time taken: 0.35953617095947266\n",
            "Batch Number: 1195 Loss: 1.9493060111999512 Time taken: 0.29572319984436035\n",
            "Batch Number: 1196 Loss: 1.9308100938796997 Time taken: 0.2889375686645508\n",
            "Batch Number: 1197 Loss: 1.9416534900665283 Time taken: 0.38561487197875977\n",
            "Batch Number: 1198 Loss: 1.915664553642273 Time taken: 0.30121755599975586\n",
            "Batch Number: 1199 Loss: 1.9419503211975098 Time taken: 0.30288267135620117\n",
            "Batch Number: 1200 Loss: 1.92353093624115 Time taken: 0.34853339195251465\n",
            "Batch Number: 1201 Loss: 1.9036118984222412 Time taken: 0.30602169036865234\n",
            "Batch Number: 1202 Loss: 1.9513111114501953 Time taken: 0.30504822731018066\n",
            "Batch Number: 1203 Loss: 1.912087082862854 Time taken: 0.3553035259246826\n",
            "Batch Number: 1204 Loss: 1.9266999959945679 Time taken: 0.3159825801849365\n",
            "Batch Number: 1205 Loss: 1.922845721244812 Time taken: 0.2955434322357178\n",
            "Batch Number: 1206 Loss: 1.93085515499115 Time taken: 0.29947829246520996\n",
            "Batch Number: 1207 Loss: 1.9212405681610107 Time taken: 0.31256985664367676\n",
            "Batch Number: 1208 Loss: 1.923604130744934 Time taken: 0.30233049392700195\n",
            "Batch Number: 1209 Loss: 1.89805006980896 Time taken: 0.30533432960510254\n",
            "Batch Number: 1210 Loss: 1.9019556045532227 Time taken: 0.3302445411682129\n",
            "Batch Number: 1211 Loss: 1.9202884435653687 Time taken: 0.2963368892669678\n",
            "Batch Number: 1212 Loss: 1.936110496520996 Time taken: 0.3159613609313965\n",
            "Batch Number: 1213 Loss: 1.950217366218567 Time taken: 0.3082430362701416\n",
            "Batch Number: 1214 Loss: 1.9258108139038086 Time taken: 0.3210875988006592\n",
            "Batch Number: 1215 Loss: 1.8915400505065918 Time taken: 0.3772749900817871\n",
            "Batch Number: 1216 Loss: 1.894698143005371 Time taken: 0.3499464988708496\n",
            "Batch Number: 1217 Loss: 1.8885905742645264 Time taken: 0.29920339584350586\n",
            "Batch Number: 1218 Loss: 1.9096754789352417 Time taken: 0.29592227935791016\n",
            "Batch Number: 1219 Loss: 1.9006268978118896 Time taken: 0.31430768966674805\n",
            "Batch Number: 1220 Loss: 1.9376331567764282 Time taken: 0.28459930419921875\n",
            "Batch Number: 1221 Loss: 1.9200612306594849 Time taken: 0.2857697010040283\n",
            "Batch Number: 1222 Loss: 1.9363285303115845 Time taken: 0.3520796298980713\n",
            "Batch Number: 1223 Loss: 1.9120222330093384 Time taken: 0.35215306282043457\n",
            "Batch Number: 1224 Loss: 1.9271870851516724 Time taken: 0.3022153377532959\n",
            "Batch Number: 1225 Loss: 1.8988381624221802 Time taken: 0.303821325302124\n",
            "Batch Number: 1226 Loss: 1.9397426843643188 Time taken: 0.3044006824493408\n",
            "Batch Number: 1227 Loss: 1.9066179990768433 Time taken: 0.31064867973327637\n",
            "Batch Number: 1228 Loss: 1.8973191976547241 Time taken: 0.3048131465911865\n",
            "Batch Number: 1229 Loss: 1.9234720468521118 Time taken: 0.3106553554534912\n",
            "Batch Number: 1230 Loss: 1.9052464962005615 Time taken: 0.3004024028778076\n",
            "Batch Number: 1231 Loss: 1.9126752614974976 Time taken: 0.31151390075683594\n",
            "Batch Number: 1232 Loss: 1.9437620639801025 Time taken: 0.32688474655151367\n",
            "Batch Number: 1233 Loss: 1.9205981492996216 Time taken: 0.2976083755493164\n",
            "Batch Number: 1234 Loss: 1.9171806573867798 Time taken: 0.3085505962371826\n",
            "Batch Number: 1235 Loss: 1.9287370443344116 Time taken: 0.317974328994751\n",
            "Batch Number: 1236 Loss: 1.9117542505264282 Time taken: 0.29747509956359863\n",
            "Batch Number: 1237 Loss: 1.9637454748153687 Time taken: 0.32065296173095703\n",
            "Batch Number: 1238 Loss: 1.9306374788284302 Time taken: 0.31142354011535645\n",
            "Batch Number: 1239 Loss: 1.9215998649597168 Time taken: 0.31548190116882324\n",
            "Batch Number: 1240 Loss: 1.9213498830795288 Time taken: 0.29792261123657227\n",
            "Batch Number: 1241 Loss: 1.9229105710983276 Time taken: 0.33245325088500977\n",
            "Batch Number: 1242 Loss: 1.9539917707443237 Time taken: 0.3140275478363037\n",
            "Batch Number: 1243 Loss: 1.9239778518676758 Time taken: 0.29564356803894043\n",
            "Batch Number: 1244 Loss: 1.916181206703186 Time taken: 0.3097374439239502\n",
            "Batch Number: 1245 Loss: 1.896673321723938 Time taken: 0.3124103546142578\n",
            "Batch Number: 1246 Loss: 1.909623384475708 Time taken: 0.30533790588378906\n",
            "Batch Number: 1247 Loss: 1.9088830947875977 Time taken: 0.3003084659576416\n",
            "Batch Number: 1248 Loss: 1.907415509223938 Time taken: 0.3713037967681885\n",
            "Batch Number: 1249 Loss: 1.8947784900665283 Time taken: 0.37127232551574707\n",
            "Batch Number: 1250 Loss: 1.9171136617660522 Time taken: 0.33065080642700195\n",
            "Batch Number: 1251 Loss: 1.8942265510559082 Time taken: 0.3046867847442627\n",
            "Batch Number: 1252 Loss: 1.929479956626892 Time taken: 0.31494736671447754\n",
            "Batch Number: 1253 Loss: 1.9111589193344116 Time taken: 0.3204038143157959\n",
            "Batch Number: 1254 Loss: 1.901305079460144 Time taken: 0.3198709487915039\n",
            "Batch Number: 1255 Loss: 1.895350456237793 Time taken: 0.315213680267334\n",
            "Batch Number: 1256 Loss: 1.9012138843536377 Time taken: 0.29687952995300293\n",
            "Batch Number: 1257 Loss: 1.8726218938827515 Time taken: 0.30637574195861816\n",
            "Batch Number: 1258 Loss: 1.8789345026016235 Time taken: 0.32463741302490234\n",
            "Batch Number: 1259 Loss: 1.897456169128418 Time taken: 0.30552053451538086\n",
            "Batch Number: 1260 Loss: 1.8862465620040894 Time taken: 0.3366811275482178\n",
            "Batch Number: 1261 Loss: 1.8837146759033203 Time taken: 0.31375646591186523\n",
            "Batch Number: 1262 Loss: 1.8671510219573975 Time taken: 0.29628849029541016\n",
            "Batch Number: 1263 Loss: 1.881032943725586 Time taken: 0.32830023765563965\n",
            "Batch Number: 1264 Loss: 1.8779758214950562 Time taken: 0.3115818500518799\n",
            "Batch Number: 1265 Loss: 1.86976158618927 Time taken: 0.3260970115661621\n",
            "Batch Number: 1266 Loss: 1.8630441427230835 Time taken: 0.3336353302001953\n",
            "Batch Number: 1267 Loss: 1.8918023109436035 Time taken: 0.3706347942352295\n",
            "Batch Number: 1268 Loss: 1.8720567226409912 Time taken: 0.3664677143096924\n",
            "Batch Number: 1269 Loss: 1.8642714023590088 Time taken: 0.33504605293273926\n",
            "Batch Number: 1270 Loss: 1.873216986656189 Time taken: 0.34363341331481934\n",
            "Batch Number: 1271 Loss: 1.8682606220245361 Time taken: 0.36664652824401855\n",
            "Batch Number: 1272 Loss: 1.8648521900177002 Time taken: 0.32518458366394043\n",
            "Batch Number: 1273 Loss: 1.892898678779602 Time taken: 0.31655073165893555\n",
            "Batch Number: 1274 Loss: 1.8864707946777344 Time taken: 0.3182353973388672\n",
            "Batch Number: 1275 Loss: 1.8997328281402588 Time taken: 0.3104267120361328\n",
            "Batch Number: 1276 Loss: 1.8869574069976807 Time taken: 0.31319522857666016\n",
            "Batch Number: 1277 Loss: 1.908821940422058 Time taken: 0.36652374267578125\n",
            "Batch Number: 1278 Loss: 1.8791545629501343 Time taken: 0.31542062759399414\n",
            "Batch Number: 1279 Loss: 1.8668243885040283 Time taken: 0.32004880905151367\n",
            "Batch Number: 1280 Loss: 1.9071879386901855 Time taken: 0.31400537490844727\n",
            "Batch Number: 1281 Loss: 1.9249471426010132 Time taken: 0.3330106735229492\n",
            "Batch Number: 1282 Loss: 1.9123064279556274 Time taken: 0.30666065216064453\n",
            "Batch Number: 1283 Loss: 1.8909810781478882 Time taken: 0.31504130363464355\n",
            "Batch Number: 1284 Loss: 1.9254419803619385 Time taken: 0.32743406295776367\n",
            "Batch Number: 1285 Loss: 1.9071592092514038 Time taken: 0.3155074119567871\n",
            "Batch Number: 1286 Loss: 1.9241052865982056 Time taken: 0.3103780746459961\n",
            "Batch Number: 1287 Loss: 1.9054536819458008 Time taken: 0.31435489654541016\n",
            "Batch Number: 1288 Loss: 1.8753806352615356 Time taken: 0.3103644847869873\n",
            "Batch Number: 1289 Loss: 1.903721809387207 Time taken: 0.3352346420288086\n",
            "Batch Number: 1290 Loss: 1.894222378730774 Time taken: 0.32680392265319824\n",
            "Batch Number: 1291 Loss: 1.8721988201141357 Time taken: 0.3015749454498291\n",
            "Batch Number: 1292 Loss: 1.895227313041687 Time taken: 0.3106510639190674\n",
            "Batch Number: 1293 Loss: 1.8980400562286377 Time taken: 0.2929718494415283\n",
            "Batch Number: 1294 Loss: 1.8989815711975098 Time taken: 0.33899569511413574\n",
            "Batch Number: 1295 Loss: 1.8822112083435059 Time taken: 0.3870241641998291\n",
            "Batch Number: 1296 Loss: 1.8896384239196777 Time taken: 0.3284599781036377\n",
            "Batch Number: 1297 Loss: 1.8881938457489014 Time taken: 0.29758119583129883\n",
            "Batch Number: 1298 Loss: 1.897518277168274 Time taken: 0.3111274242401123\n",
            "Batch Number: 1299 Loss: 1.8910613059997559 Time taken: 0.3036530017852783\n",
            "Batch Number: 1300 Loss: 1.88233482837677 Time taken: 0.3067138195037842\n",
            "Batch Number: 1301 Loss: 1.8849539756774902 Time taken: 0.323183536529541\n",
            "Batch Number: 1302 Loss: 1.8981715440750122 Time taken: 0.32413816452026367\n",
            "Batch Number: 1303 Loss: 1.90500807762146 Time taken: 0.30747437477111816\n",
            "Batch Number: 1304 Loss: 1.8923321962356567 Time taken: 0.3249480724334717\n",
            "Batch Number: 1305 Loss: 1.8634134531021118 Time taken: 0.3584294319152832\n",
            "Batch Number: 1306 Loss: 1.8760462999343872 Time taken: 0.3858952522277832\n",
            "Batch Number: 1307 Loss: 1.900468349456787 Time taken: 0.38277482986450195\n",
            "Batch Number: 1308 Loss: 1.8870298862457275 Time taken: 0.32177186012268066\n",
            "Batch Number: 1309 Loss: 1.8799995183944702 Time taken: 0.301746129989624\n",
            "Batch Number: 1310 Loss: 1.9018446207046509 Time taken: 0.3023340702056885\n",
            "Batch Number: 1311 Loss: 1.88239324092865 Time taken: 0.3058352470397949\n",
            "Batch Number: 1312 Loss: 1.9009873867034912 Time taken: 0.3114750385284424\n",
            "Batch Number: 1313 Loss: 1.9087533950805664 Time taken: 0.3341543674468994\n",
            "Batch Number: 1314 Loss: 1.8774125576019287 Time taken: 0.3584418296813965\n",
            "Batch Number: 1315 Loss: 1.8756842613220215 Time taken: 0.37480759620666504\n",
            "Batch Number: 1316 Loss: 1.906688928604126 Time taken: 0.3549807071685791\n",
            "Batch Number: 1317 Loss: 1.8789048194885254 Time taken: 0.2901275157928467\n",
            "Batch Number: 1318 Loss: 1.8688280582427979 Time taken: 0.2983977794647217\n",
            "Batch Number: 1319 Loss: 1.8512547016143799 Time taken: 0.32149362564086914\n",
            "Batch Number: 1320 Loss: 1.8780988454818726 Time taken: 0.3418757915496826\n",
            "Batch Number: 1321 Loss: 1.8674770593643188 Time taken: 0.36762237548828125\n",
            "Batch Number: 1322 Loss: 1.8673957586288452 Time taken: 0.35680699348449707\n",
            "Batch Number: 1323 Loss: 1.882702350616455 Time taken: 0.3052239418029785\n",
            "Batch Number: 1324 Loss: 1.8799453973770142 Time taken: 0.3611738681793213\n",
            "Batch Number: 1325 Loss: 1.8472435474395752 Time taken: 0.37253403663635254\n",
            "Batch Number: 1326 Loss: 1.892115592956543 Time taken: 0.3097965717315674\n",
            "Batch Number: 1327 Loss: 1.8949730396270752 Time taken: 0.29966211318969727\n",
            "Batch Number: 1328 Loss: 1.8859916925430298 Time taken: 0.3111691474914551\n",
            "Batch Number: 1329 Loss: 1.8929474353790283 Time taken: 0.29687976837158203\n",
            "Batch Number: 1330 Loss: 1.8746095895767212 Time taken: 0.36623263359069824\n",
            "Batch Number: 1331 Loss: 1.8816570043563843 Time taken: 0.3837289810180664\n",
            "Batch Number: 1332 Loss: 1.8930212259292603 Time taken: 0.3125896453857422\n",
            "Batch Number: 1333 Loss: 1.8654110431671143 Time taken: 0.30783677101135254\n",
            "Batch Number: 1334 Loss: 1.8983896970748901 Time taken: 0.3021085262298584\n",
            "Batch Number: 1335 Loss: 1.90126633644104 Time taken: 0.2979564666748047\n",
            "Batch Number: 1336 Loss: 1.9079610109329224 Time taken: 0.3049948215484619\n",
            "Batch Number: 1337 Loss: 1.889730453491211 Time taken: 0.30123209953308105\n",
            "Batch Number: 1338 Loss: 1.892744541168213 Time taken: 0.31125855445861816\n",
            "Batch Number: 1339 Loss: 1.882434368133545 Time taken: 0.3001539707183838\n",
            "Batch Number: 1340 Loss: 1.8740507364273071 Time taken: 0.28586649894714355\n",
            "Batch Number: 1341 Loss: 1.8774610757827759 Time taken: 0.37326931953430176\n",
            "Batch Number: 1342 Loss: 1.8691904544830322 Time taken: 0.3714139461517334\n",
            "Batch Number: 1343 Loss: 1.8713383674621582 Time taken: 0.37492847442626953\n",
            "Batch Number: 1344 Loss: 1.852908968925476 Time taken: 0.3660311698913574\n",
            "Batch Number: 1345 Loss: 1.8622639179229736 Time taken: 0.3585686683654785\n",
            "Batch Number: 1346 Loss: 1.860408902168274 Time taken: 0.30485105514526367\n",
            "Batch Number: 1347 Loss: 1.878458857536316 Time taken: 0.29411911964416504\n",
            "Batch Number: 1348 Loss: 1.8507721424102783 Time taken: 0.30272912979125977\n",
            "Batch Number: 1349 Loss: 1.865494966506958 Time taken: 0.2890963554382324\n",
            "Batch Number: 1350 Loss: 1.847177267074585 Time taken: 0.3295152187347412\n",
            "Batch Number: 1351 Loss: 1.875224232673645 Time taken: 0.31506872177124023\n",
            "Batch Number: 1352 Loss: 1.8806681632995605 Time taken: 0.3562905788421631\n",
            "Batch Number: 1353 Loss: 1.901369333267212 Time taken: 0.37778186798095703\n",
            "Batch Number: 1354 Loss: 1.8806554079055786 Time taken: 0.38440680503845215\n",
            "Batch Number: 1355 Loss: 1.8936249017715454 Time taken: 0.3645634651184082\n",
            "Batch Number: 1356 Loss: 1.9012086391448975 Time taken: 0.3266565799713135\n",
            "Batch Number: 1357 Loss: 1.886572241783142 Time taken: 0.3892180919647217\n",
            "Batch Number: 1358 Loss: 1.9066270589828491 Time taken: 0.3163013458251953\n",
            "Batch Number: 1359 Loss: 1.8859429359436035 Time taken: 0.29694032669067383\n",
            "Batch Number: 1360 Loss: 1.8698554039001465 Time taken: 0.3117039203643799\n",
            "Batch Number: 1361 Loss: 1.9213860034942627 Time taken: 0.33151769638061523\n",
            "Batch Number: 1362 Loss: 1.9055277109146118 Time taken: 0.3078768253326416\n",
            "Batch Number: 1363 Loss: 1.890839695930481 Time taken: 0.31587815284729004\n",
            "Batch Number: 1364 Loss: 1.8979684114456177 Time taken: 0.3468282222747803\n",
            "Batch Number: 1365 Loss: 1.8894215822219849 Time taken: 0.30791687965393066\n",
            "Batch Number: 1366 Loss: 1.9143462181091309 Time taken: 0.3140583038330078\n",
            "Batch Number: 1367 Loss: 1.9150192737579346 Time taken: 0.31998324394226074\n",
            "Batch Number: 1368 Loss: 1.904183030128479 Time taken: 0.307542085647583\n",
            "Batch Number: 1369 Loss: 1.895119071006775 Time taken: 0.31488585472106934\n",
            "Batch Number: 1370 Loss: 1.926259994506836 Time taken: 0.33579397201538086\n",
            "Batch Number: 1371 Loss: 1.9035874605178833 Time taken: 0.2965850830078125\n",
            "Batch Number: 1372 Loss: 1.9010077714920044 Time taken: 0.3065659999847412\n",
            "Batch Number: 1373 Loss: 1.9257186651229858 Time taken: 0.3038194179534912\n",
            "Batch Number: 1374 Loss: 1.88661527633667 Time taken: 0.3842027187347412\n",
            "Batch Number: 1375 Loss: 1.880181908607483 Time taken: 0.37654662132263184\n",
            "Batch Number: 1376 Loss: 1.8853777647018433 Time taken: 0.3518383502960205\n",
            "Batch Number: 1377 Loss: 1.8888860940933228 Time taken: 0.3037686347961426\n",
            "Batch Number: 1378 Loss: 1.8865512609481812 Time taken: 0.3087751865386963\n",
            "Batch Number: 1379 Loss: 1.8714572191238403 Time taken: 0.32263994216918945\n",
            "Batch Number: 1380 Loss: 1.8975872993469238 Time taken: 0.29961299896240234\n",
            "Batch Number: 1381 Loss: 1.8836443424224854 Time taken: 0.30074191093444824\n",
            "Batch Number: 1382 Loss: 1.8799463510513306 Time taken: 0.3023838996887207\n",
            "Batch Number: 1383 Loss: 1.8721901178359985 Time taken: 0.28986406326293945\n",
            "Batch Number: 1384 Loss: 1.8498320579528809 Time taken: 0.29474806785583496\n",
            "Batch Number: 1385 Loss: 1.8705121278762817 Time taken: 0.29827451705932617\n",
            "Batch Number: 1386 Loss: 1.8726441860198975 Time taken: 0.3025796413421631\n",
            "Batch Number: 1387 Loss: 1.8829635381698608 Time taken: 0.31498003005981445\n",
            "Batch Number: 1388 Loss: 1.8607163429260254 Time taken: 0.30557918548583984\n",
            "Batch Number: 1389 Loss: 1.8414056301116943 Time taken: 0.3174724578857422\n",
            "Batch Number: 1390 Loss: 1.8685344457626343 Time taken: 0.31005334854125977\n",
            "Batch Number: 1391 Loss: 1.8719936609268188 Time taken: 0.33690929412841797\n",
            "Batch Number: 1392 Loss: 1.8754346370697021 Time taken: 0.3477027416229248\n",
            "Batch Number: 1393 Loss: 1.875630497932434 Time taken: 0.35180115699768066\n",
            "Batch Number: 1394 Loss: 1.874662160873413 Time taken: 0.3749573230743408\n",
            "Batch Number: 1395 Loss: 1.8415837287902832 Time taken: 0.3106870651245117\n",
            "Batch Number: 1396 Loss: 1.8388057947158813 Time taken: 0.32755446434020996\n",
            "Batch Number: 1397 Loss: 1.8480662107467651 Time taken: 0.3035852909088135\n",
            "Batch Number: 1398 Loss: 1.8572221994400024 Time taken: 0.3289172649383545\n",
            "Batch Number: 1399 Loss: 1.852878451347351 Time taken: 0.3155076503753662\n",
            "Batch Number: 1400 Loss: 1.888993263244629 Time taken: 0.3029041290283203\n",
            "Batch Number: 1401 Loss: 1.8599436283111572 Time taken: 0.3160388469696045\n",
            "Batch Number: 1402 Loss: 1.869880199432373 Time taken: 0.3079664707183838\n",
            "Batch Number: 1403 Loss: 1.9152547121047974 Time taken: 0.31622815132141113\n",
            "Batch Number: 1404 Loss: 1.8652766942977905 Time taken: 0.306490421295166\n",
            "Batch Number: 1405 Loss: 1.885169506072998 Time taken: 0.30141329765319824\n",
            "Batch Number: 1406 Loss: 1.866602897644043 Time taken: 0.30788660049438477\n",
            "Batch Number: 1407 Loss: 1.8229702711105347 Time taken: 0.29537248611450195\n",
            "Batch Number: 1408 Loss: 1.87898850440979 Time taken: 0.31984901428222656\n",
            "Batch Number: 1409 Loss: 1.8696836233139038 Time taken: 0.30829453468322754\n",
            "Batch Number: 1410 Loss: 1.848338007926941 Time taken: 0.3252100944519043\n",
            "Batch Number: 1411 Loss: 1.8730453252792358 Time taken: 0.3041849136352539\n",
            "Batch Number: 1412 Loss: 1.8790720701217651 Time taken: 0.3038778305053711\n",
            "Batch Number: 1413 Loss: 1.8994441032409668 Time taken: 0.35172080993652344\n",
            "Batch Number: 1414 Loss: 1.8685122728347778 Time taken: 0.30470871925354004\n",
            "Batch Number: 1415 Loss: 1.8587738275527954 Time taken: 0.3118867874145508\n",
            "Batch Number: 1416 Loss: 1.8729510307312012 Time taken: 0.33760738372802734\n",
            "Batch Number: 1417 Loss: 1.8862524032592773 Time taken: 0.36920952796936035\n",
            "Batch Number: 1418 Loss: 1.8618457317352295 Time taken: 0.38135743141174316\n",
            "Batch Number: 1419 Loss: 1.8917932510375977 Time taken: 0.360231876373291\n",
            "Batch Number: 1420 Loss: 1.8962502479553223 Time taken: 0.2958528995513916\n",
            "Batch Number: 1421 Loss: 1.88297438621521 Time taken: 0.3062858581542969\n",
            "Batch Number: 1422 Loss: 1.8608113527297974 Time taken: 0.3016376495361328\n",
            "Batch Number: 1423 Loss: 1.8559162616729736 Time taken: 0.3064391613006592\n",
            "Batch Number: 1424 Loss: 1.8703826665878296 Time taken: 0.303586483001709\n",
            "Batch Number: 1425 Loss: 1.839760661125183 Time taken: 0.32890820503234863\n",
            "Batch Number: 1426 Loss: 1.8729908466339111 Time taken: 0.34858012199401855\n",
            "Batch Number: 1427 Loss: 1.8526934385299683 Time taken: 0.3719370365142822\n",
            "Batch Number: 1428 Loss: 1.8428226709365845 Time taken: 0.3844764232635498\n",
            "Batch Number: 1429 Loss: 1.8308744430541992 Time taken: 0.30876660346984863\n",
            "Batch Number: 1430 Loss: 1.8451603651046753 Time taken: 0.3008131980895996\n",
            "Batch Number: 1431 Loss: 1.8678170442581177 Time taken: 0.30110740661621094\n",
            "Batch Number: 1432 Loss: 1.847036361694336 Time taken: 0.3030436038970947\n",
            "Batch Number: 1433 Loss: 1.8584095239639282 Time taken: 0.2964508533477783\n",
            "Batch Number: 1434 Loss: 1.8422459363937378 Time taken: 0.30266594886779785\n",
            "Batch Number: 1435 Loss: 1.8448436260223389 Time taken: 0.2957272529602051\n",
            "Batch Number: 1436 Loss: 1.8246787786483765 Time taken: 0.3084588050842285\n",
            "Batch Number: 1437 Loss: 1.8316736221313477 Time taken: 0.3042457103729248\n",
            "Batch Number: 1438 Loss: 1.8346761465072632 Time taken: 0.30760979652404785\n",
            "Batch Number: 1439 Loss: 1.8437385559082031 Time taken: 0.29393434524536133\n",
            "Batch Number: 1440 Loss: 1.8563088178634644 Time taken: 0.29355931282043457\n",
            "Batch Number: 1441 Loss: 1.8590819835662842 Time taken: 0.31848931312561035\n",
            "Batch Number: 1442 Loss: 1.8687665462493896 Time taken: 0.29253292083740234\n",
            "Batch Number: 1443 Loss: 1.8723509311676025 Time taken: 0.2987806797027588\n",
            "Batch Number: 1444 Loss: 1.8442736864089966 Time taken: 0.32096123695373535\n",
            "Batch Number: 1445 Loss: 1.8286420106887817 Time taken: 0.30585455894470215\n",
            "Batch Number: 1446 Loss: 1.8399293422698975 Time taken: 0.3461887836456299\n",
            "Batch Number: 1447 Loss: 1.8170392513275146 Time taken: 0.35425853729248047\n",
            "Batch Number: 1448 Loss: 1.857596516609192 Time taken: 0.3661975860595703\n",
            "Batch Number: 1449 Loss: 1.837595820426941 Time taken: 0.3011152744293213\n",
            "Batch Number: 1450 Loss: 1.839099407196045 Time taken: 0.31977248191833496\n",
            "Batch Number: 1451 Loss: 1.8238493204116821 Time taken: 0.30028462409973145\n",
            "Batch Number: 1452 Loss: 1.845319151878357 Time taken: 0.2918975353240967\n",
            "Batch Number: 1453 Loss: 1.8794918060302734 Time taken: 0.37095117568969727\n",
            "Batch Number: 1454 Loss: 1.8545559644699097 Time taken: 0.3176686763763428\n",
            "Batch Number: 1455 Loss: 1.860376000404358 Time taken: 0.321918249130249\n",
            "Batch Number: 1456 Loss: 1.8617417812347412 Time taken: 0.36721134185791016\n",
            "Batch Number: 1457 Loss: 1.8438029289245605 Time taken: 0.3009674549102783\n",
            "Batch Number: 1458 Loss: 1.8540695905685425 Time taken: 0.31214237213134766\n",
            "Batch Number: 1459 Loss: 1.8413680791854858 Time taken: 0.3470268249511719\n",
            "Batch Number: 1460 Loss: 1.8358460664749146 Time taken: 0.37877345085144043\n",
            "Batch Number: 1461 Loss: 1.8300271034240723 Time taken: 0.4158601760864258\n",
            "Batch Number: 1462 Loss: 1.8306105136871338 Time taken: 0.3507676124572754\n",
            "Batch Number: 1463 Loss: 1.8615673780441284 Time taken: 0.3666839599609375\n",
            "Batch Number: 1464 Loss: 1.8353735208511353 Time taken: 0.3852074146270752\n",
            "Batch Number: 1465 Loss: 1.8307735919952393 Time taken: 0.32103395462036133\n",
            "Batch Number: 1466 Loss: 1.8411223888397217 Time taken: 0.3084404468536377\n",
            "Batch Number: 1467 Loss: 1.8263108730316162 Time taken: 0.3036179542541504\n",
            "Batch Number: 1468 Loss: 1.8297393321990967 Time taken: 0.3063981533050537\n",
            "Batch Number: 1469 Loss: 1.8471214771270752 Time taken: 0.29163455963134766\n",
            "Batch Number: 1470 Loss: 1.8294421434402466 Time taken: 0.3624725341796875\n",
            "Batch Number: 1471 Loss: 1.8387824296951294 Time taken: 0.3247203826904297\n",
            "Batch Number: 1472 Loss: 1.8174209594726562 Time taken: 0.28722691535949707\n",
            "Batch Number: 1473 Loss: 1.86103355884552 Time taken: 0.28847503662109375\n",
            "Batch Number: 1474 Loss: 1.8220716714859009 Time taken: 0.3470432758331299\n",
            "Batch Number: 1475 Loss: 1.841414213180542 Time taken: 0.3135056495666504\n",
            "Batch Number: 1476 Loss: 1.8260406255722046 Time taken: 0.3685920238494873\n",
            "Batch Number: 1477 Loss: 1.8239370584487915 Time taken: 0.38407135009765625\n",
            "Batch Number: 1478 Loss: 1.879409909248352 Time taken: 0.36408543586730957\n",
            "Batch Number: 1479 Loss: 1.8577516078948975 Time taken: 0.3607509136199951\n",
            "Batch Number: 1480 Loss: 1.8738561868667603 Time taken: 0.29683923721313477\n",
            "Batch Number: 1481 Loss: 1.8336365222930908 Time taken: 0.28278160095214844\n",
            "Batch Number: 1482 Loss: 1.8108419179916382 Time taken: 0.2786719799041748\n",
            "Batch Number: 1483 Loss: 1.8524748086929321 Time taken: 0.3423311710357666\n",
            "Batch Number: 1484 Loss: 1.8459270000457764 Time taken: 0.29624485969543457\n",
            "Batch Number: 1485 Loss: 1.8492966890335083 Time taken: 0.3391754627227783\n",
            "Batch Number: 1486 Loss: 1.8446426391601562 Time taken: 0.3179605007171631\n",
            "Batch Number: 1487 Loss: 1.8320022821426392 Time taken: 0.28751635551452637\n",
            "Batch Number: 1488 Loss: 1.8380067348480225 Time taken: 0.3046386241912842\n",
            "Batch Number: 1489 Loss: 1.829783320426941 Time taken: 0.3092958927154541\n",
            "Batch Number: 1490 Loss: 1.8384939432144165 Time taken: 0.3075387477874756\n",
            "Batch Number: 1491 Loss: 1.8542864322662354 Time taken: 0.30513882637023926\n",
            "Batch Number: 1492 Loss: 1.8502787351608276 Time taken: 0.30176806449890137\n",
            "Batch Number: 1493 Loss: 1.8480474948883057 Time taken: 0.3396115303039551\n",
            "Batch Number: 1494 Loss: 1.8507041931152344 Time taken: 0.31540751457214355\n",
            "Batch Number: 1495 Loss: 1.837332844734192 Time taken: 0.3222513198852539\n",
            "Batch Number: 1496 Loss: 1.8214350938796997 Time taken: 0.32478880882263184\n",
            "Batch Number: 1497 Loss: 1.8450161218643188 Time taken: 0.30425167083740234\n",
            "Batch Number: 1498 Loss: 1.8612273931503296 Time taken: 0.292647123336792\n",
            "Batch Number: 1499 Loss: 1.8431793451309204 Time taken: 0.3174419403076172\n",
            "Batch Number: 1500 Loss: 1.8128174543380737 Time taken: 0.2887876033782959\n",
            "Batch Number: 1501 Loss: 1.8238096237182617 Time taken: 0.29199767112731934\n",
            "Batch Number: 1502 Loss: 1.8295446634292603 Time taken: 0.31981825828552246\n",
            "Batch Number: 1503 Loss: 1.8210409879684448 Time taken: 0.35234832763671875\n",
            "Batch Number: 1504 Loss: 1.8023018836975098 Time taken: 0.30260729789733887\n",
            "Batch Number: 1505 Loss: 1.8093595504760742 Time taken: 0.32540345191955566\n",
            "Batch Number: 1506 Loss: 1.8256797790527344 Time taken: 0.292888879776001\n",
            "Batch Number: 1507 Loss: 1.832879900932312 Time taken: 0.29893970489501953\n",
            "Batch Number: 1508 Loss: 1.8233064413070679 Time taken: 0.33752012252807617\n",
            "Batch Number: 1509 Loss: 1.8421680927276611 Time taken: 0.3174901008605957\n",
            "Batch Number: 1510 Loss: 1.8421705961227417 Time taken: 0.30484461784362793\n",
            "Batch Number: 1511 Loss: 1.834917664527893 Time taken: 0.31134533882141113\n",
            "Batch Number: 1512 Loss: 1.8339248895645142 Time taken: 0.31615304946899414\n",
            "Batch Number: 1513 Loss: 1.8360038995742798 Time taken: 0.2927377223968506\n",
            "Batch Number: 1514 Loss: 1.8659629821777344 Time taken: 0.3019580841064453\n",
            "Batch Number: 1515 Loss: 1.8566986322402954 Time taken: 0.31191372871398926\n",
            "Batch Number: 1516 Loss: 1.8371641635894775 Time taken: 0.29259181022644043\n",
            "Batch Number: 1517 Loss: 1.8358750343322754 Time taken: 0.30196166038513184\n",
            "Batch Number: 1518 Loss: 1.849290132522583 Time taken: 0.31111955642700195\n",
            "Batch Number: 1519 Loss: 1.8411229848861694 Time taken: 0.30202269554138184\n",
            "Batch Number: 1520 Loss: 1.830132246017456 Time taken: 0.3102884292602539\n",
            "Batch Number: 1521 Loss: 1.8355814218521118 Time taken: 0.3199336528778076\n",
            "Batch Number: 1522 Loss: 1.8417282104492188 Time taken: 0.3238060474395752\n",
            "Batch Number: 1523 Loss: 1.855675458908081 Time taken: 0.3002285957336426\n",
            "Batch Number: 1524 Loss: 1.8544092178344727 Time taken: 0.3098289966583252\n",
            "Batch Number: 1525 Loss: 1.835070013999939 Time taken: 0.3110661506652832\n",
            "Batch Number: 1526 Loss: 1.8113747835159302 Time taken: 0.30708956718444824\n",
            "Batch Number: 1527 Loss: 1.8081772327423096 Time taken: 0.3288233280181885\n",
            "Batch Number: 1528 Loss: 1.81308913230896 Time taken: 0.34228014945983887\n",
            "Batch Number: 1529 Loss: 1.808672308921814 Time taken: 0.29791784286499023\n",
            "Batch Number: 1530 Loss: 1.8310574293136597 Time taken: 0.31215715408325195\n",
            "Batch Number: 1531 Loss: 1.8098009824752808 Time taken: 0.3107106685638428\n",
            "Batch Number: 1532 Loss: 1.8586934804916382 Time taken: 0.3045196533203125\n",
            "Batch Number: 1533 Loss: 1.8585158586502075 Time taken: 0.33065056800842285\n",
            "Batch Number: 1534 Loss: 1.8497495651245117 Time taken: 0.32336997985839844\n",
            "Batch Number: 1535 Loss: 1.8592979907989502 Time taken: 0.2973604202270508\n",
            "Batch Number: 1536 Loss: 1.8514039516448975 Time taken: 0.30042290687561035\n",
            "Batch Number: 1537 Loss: 1.8753373622894287 Time taken: 0.3104372024536133\n",
            "Batch Number: 1538 Loss: 1.8621562719345093 Time taken: 0.32898569107055664\n",
            "Batch Number: 1539 Loss: 1.8488506078720093 Time taken: 0.30243706703186035\n",
            "Batch Number: 1540 Loss: 1.832354187965393 Time taken: 0.30893993377685547\n",
            "Batch Number: 1541 Loss: 1.8555564880371094 Time taken: 0.314255952835083\n",
            "Batch Number: 1542 Loss: 1.8387072086334229 Time taken: 0.29840660095214844\n",
            "Batch Number: 1543 Loss: 1.8497930765151978 Time taken: 0.30069923400878906\n",
            "Batch Number: 1544 Loss: 1.8562140464782715 Time taken: 0.3168671131134033\n",
            "Batch Number: 1545 Loss: 1.8442484140396118 Time taken: 0.29653000831604004\n",
            "Batch Number: 1546 Loss: 1.8469325304031372 Time taken: 0.3293604850769043\n",
            "Batch Number: 1547 Loss: 1.8606771230697632 Time taken: 0.29594850540161133\n",
            "Batch Number: 1548 Loss: 1.8851114511489868 Time taken: 0.2849459648132324\n",
            "Batch Number: 1549 Loss: 1.845508098602295 Time taken: 0.30101490020751953\n",
            "Batch Number: 1550 Loss: 1.8718546628952026 Time taken: 0.298980712890625\n",
            "Batch Number: 1551 Loss: 1.868367314338684 Time taken: 0.34540653228759766\n",
            "Batch Number: 1552 Loss: 1.8826054334640503 Time taken: 0.3727097511291504\n",
            "Batch Number: 1553 Loss: 1.8449954986572266 Time taken: 0.3751800060272217\n",
            "Batch Number: 1554 Loss: 1.8727362155914307 Time taken: 0.38480353355407715\n",
            "Batch Number: 1555 Loss: 1.8622649908065796 Time taken: 0.3412933349609375\n",
            "Batch Number: 1556 Loss: 1.8664485216140747 Time taken: 0.38584089279174805\n",
            "Batch Number: 1557 Loss: 1.8582854270935059 Time taken: 0.341841459274292\n",
            "Batch Number: 1558 Loss: 1.8247419595718384 Time taken: 0.3816356658935547\n",
            "Batch Number: 1559 Loss: 1.8674768209457397 Time taken: 0.3961930274963379\n",
            "Batch Number: 1560 Loss: 1.8406885862350464 Time taken: 0.31420397758483887\n",
            "Batch Number: 1561 Loss: 1.8279144763946533 Time taken: 0.29666781425476074\n",
            "Batch Number: 1562 Loss: 1.8573893308639526 Time taken: 0.3188819885253906\n",
            "Batch Number: 1563 Loss: 1.8250283002853394 Time taken: 0.3355269432067871\n",
            "Batch Number: 1564 Loss: 1.8405681848526 Time taken: 0.30464816093444824\n",
            "Batch Number: 1565 Loss: 1.8470152616500854 Time taken: 0.3107788562774658\n",
            "Batch Number: 1566 Loss: 1.8729956150054932 Time taken: 0.2948620319366455\n",
            "Batch Number: 1567 Loss: 1.824352741241455 Time taken: 0.30672216415405273\n",
            "Batch Number: 1568 Loss: 1.8125593662261963 Time taken: 0.3017301559448242\n",
            "Batch Number: 1569 Loss: 1.8358455896377563 Time taken: 0.33324503898620605\n",
            "Batch Number: 1570 Loss: 1.8352190256118774 Time taken: 0.36789989471435547\n",
            "Batch Number: 1571 Loss: 1.801240086555481 Time taken: 0.3860659599304199\n",
            "Batch Number: 1572 Loss: 1.8241426944732666 Time taken: 0.36159229278564453\n",
            "Batch Number: 1573 Loss: 1.8392126560211182 Time taken: 0.29608988761901855\n",
            "Batch Number: 1574 Loss: 1.823251485824585 Time taken: 0.3130819797515869\n",
            "Batch Number: 1575 Loss: 1.8330239057540894 Time taken: 0.3158257007598877\n",
            "Batch Number: 1576 Loss: 1.8209216594696045 Time taken: 0.3593435287475586\n",
            "Batch Number: 1577 Loss: 1.7856824398040771 Time taken: 0.3889436721801758\n",
            "Batch Number: 1578 Loss: 1.779595971107483 Time taken: 0.3403170108795166\n",
            "Batch Number: 1579 Loss: 1.801891565322876 Time taken: 0.34006237983703613\n",
            "Batch Number: 1580 Loss: 1.8070164918899536 Time taken: 0.38942432403564453\n",
            "Batch Number: 1581 Loss: 1.8260365724563599 Time taken: 0.3878803253173828\n",
            "Batch Number: 1582 Loss: 1.8576117753982544 Time taken: 0.36488986015319824\n",
            "Batch Number: 1583 Loss: 1.814489722251892 Time taken: 0.35530948638916016\n",
            "Batch Number: 1584 Loss: 1.7998721599578857 Time taken: 0.3696920871734619\n",
            "Batch Number: 1585 Loss: 1.8305257558822632 Time taken: 0.3513765335083008\n",
            "Batch Number: 1586 Loss: 1.8096636533737183 Time taken: 0.33721446990966797\n",
            "Batch Number: 1587 Loss: 1.8296047449111938 Time taken: 0.2894325256347656\n",
            "Batch Number: 1588 Loss: 1.8342941999435425 Time taken: 0.30866312980651855\n",
            "Batch Number: 1589 Loss: 1.8476588726043701 Time taken: 0.3284585475921631\n",
            "Batch Number: 1590 Loss: 1.8201090097427368 Time taken: 0.29636526107788086\n",
            "Batch Number: 1591 Loss: 1.8532202243804932 Time taken: 0.29638195037841797\n",
            "Batch Number: 1592 Loss: 1.850746512413025 Time taken: 0.3357362747192383\n",
            "Batch Number: 1593 Loss: 1.8579274415969849 Time taken: 0.2943117618560791\n",
            "Batch Number: 1594 Loss: 1.8354458808898926 Time taken: 0.30774736404418945\n",
            "Batch Number: 1595 Loss: 1.8554048538208008 Time taken: 0.31688833236694336\n",
            "Batch Number: 1596 Loss: 1.8275057077407837 Time taken: 0.29873108863830566\n",
            "Batch Number: 1597 Loss: 1.8566960096359253 Time taken: 0.2994706630706787\n",
            "Batch Number: 1598 Loss: 1.8419312238693237 Time taken: 0.31931376457214355\n",
            "Batch Number: 1599 Loss: 1.8320971727371216 Time taken: 0.3178415298461914\n",
            "Batch Number: 1600 Loss: 1.8585270643234253 Time taken: 0.2936234474182129\n",
            "Batch Number: 1601 Loss: 1.826887845993042 Time taken: 0.2974996566772461\n",
            "Batch Number: 1602 Loss: 1.8491768836975098 Time taken: 0.30517077445983887\n",
            "Batch Number: 1603 Loss: 1.8280093669891357 Time taken: 0.2971682548522949\n",
            "Batch Number: 1604 Loss: 1.8280516862869263 Time taken: 0.2835428714752197\n",
            "Batch Number: 1605 Loss: 1.835255742073059 Time taken: 0.3598673343658447\n",
            "Batch Number: 1606 Loss: 1.8327311277389526 Time taken: 0.32672953605651855\n",
            "Batch Number: 1607 Loss: 1.8235805034637451 Time taken: 0.37520551681518555\n",
            "Batch Number: 1608 Loss: 1.7987000942230225 Time taken: 0.32581424713134766\n",
            "Batch Number: 1609 Loss: 1.819815993309021 Time taken: 0.30413198471069336\n",
            "Batch Number: 1610 Loss: 1.8226981163024902 Time taken: 0.3122823238372803\n",
            "Batch Number: 1611 Loss: 1.8400143384933472 Time taken: 0.3492012023925781\n",
            "Batch Number: 1612 Loss: 1.799611210823059 Time taken: 0.30328369140625\n",
            "Batch Number: 1613 Loss: 1.814538598060608 Time taken: 0.2997872829437256\n",
            "Batch Number: 1614 Loss: 1.8208166360855103 Time taken: 0.3499166965484619\n",
            "Batch Number: 1615 Loss: 1.8013578653335571 Time taken: 0.30348849296569824\n",
            "Batch Number: 1616 Loss: 1.7954035997390747 Time taken: 0.3155686855316162\n",
            "Batch Number: 1617 Loss: 1.775837779045105 Time taken: 0.3726356029510498\n",
            "Batch Number: 1618 Loss: 1.7948366403579712 Time taken: 0.3042283058166504\n",
            "Batch Number: 1619 Loss: 1.7988700866699219 Time taken: 0.29924988746643066\n",
            "Batch Number: 1620 Loss: 1.8010613918304443 Time taken: 0.32350802421569824\n",
            "Batch Number: 1621 Loss: 1.8084245920181274 Time taken: 0.30138421058654785\n",
            "Batch Number: 1622 Loss: 1.7816165685653687 Time taken: 0.3402066230773926\n",
            "Batch Number: 1623 Loss: 1.7816848754882812 Time taken: 0.3327510356903076\n",
            "Batch Number: 1624 Loss: 1.7783809900283813 Time taken: 0.3223130702972412\n",
            "Batch Number: 1625 Loss: 1.7699620723724365 Time taken: 0.3063316345214844\n",
            "Batch Number: 1626 Loss: 1.7976384162902832 Time taken: 0.323352575302124\n",
            "Batch Number: 1627 Loss: 1.7758501768112183 Time taken: 0.3087127208709717\n",
            "Batch Number: 1628 Loss: 1.7886372804641724 Time taken: 0.29934024810791016\n",
            "Batch Number: 1629 Loss: 1.7931557893753052 Time taken: 0.3210573196411133\n",
            "Batch Number: 1630 Loss: 1.779306173324585 Time taken: 0.31887102127075195\n",
            "Batch Number: 1631 Loss: 1.7950518131256104 Time taken: 0.29822635650634766\n",
            "Batch Number: 1632 Loss: 1.7894501686096191 Time taken: 0.2979085445404053\n",
            "Batch Number: 1633 Loss: 1.8182563781738281 Time taken: 0.362973690032959\n",
            "Batch Number: 1634 Loss: 1.8550543785095215 Time taken: 0.3684074878692627\n",
            "Batch Number: 1635 Loss: 1.7896605730056763 Time taken: 0.3770937919616699\n",
            "Batch Number: 1636 Loss: 1.7908607721328735 Time taken: 0.3826315402984619\n",
            "Batch Number: 1637 Loss: 1.7817432880401611 Time taken: 0.36759495735168457\n",
            "Batch Number: 1638 Loss: 1.8010741472244263 Time taken: 0.33324098587036133\n",
            "Batch Number: 1639 Loss: 1.7520166635513306 Time taken: 0.306262731552124\n",
            "Batch Number: 1640 Loss: 1.7737871408462524 Time taken: 0.29253339767456055\n",
            "Batch Number: 1641 Loss: 1.778876543045044 Time taken: 0.31105732917785645\n",
            "Batch Number: 1642 Loss: 1.803910493850708 Time taken: 0.2990908622741699\n",
            "Batch Number: 1643 Loss: 1.8064707517623901 Time taken: 0.2876455783843994\n",
            "Batch Number: 1644 Loss: 1.8151881694793701 Time taken: 0.32735371589660645\n",
            "Batch Number: 1645 Loss: 1.8077799081802368 Time taken: 0.32317638397216797\n",
            "Batch Number: 1646 Loss: 1.7803913354873657 Time taken: 0.29459381103515625\n",
            "Batch Number: 1647 Loss: 1.7901750802993774 Time taken: 0.2880113124847412\n",
            "Batch Number: 1648 Loss: 1.78692626953125 Time taken: 0.31369972229003906\n",
            "Batch Number: 1649 Loss: 1.798140048980713 Time taken: 0.29892492294311523\n",
            "Batch Number: 1650 Loss: 1.800912618637085 Time taken: 0.2881040573120117\n",
            "Batch Number: 1651 Loss: 1.7789292335510254 Time taken: 0.30932068824768066\n",
            "Batch Number: 1652 Loss: 1.778452754020691 Time taken: 0.3072221279144287\n",
            "Batch Number: 1653 Loss: 1.804263949394226 Time taken: 0.2976551055908203\n",
            "Batch Number: 1654 Loss: 1.8272876739501953 Time taken: 0.33243370056152344\n",
            "Batch Number: 1655 Loss: 1.8022258281707764 Time taken: 0.32931041717529297\n",
            "Batch Number: 1656 Loss: 1.7903387546539307 Time taken: 0.30026674270629883\n",
            "Batch Number: 1657 Loss: 1.8272478580474854 Time taken: 0.31104135513305664\n",
            "Batch Number: 1658 Loss: 1.80646812915802 Time taken: 0.31034278869628906\n",
            "Batch Number: 1659 Loss: 1.8064500093460083 Time taken: 0.2968728542327881\n",
            "Batch Number: 1660 Loss: 1.8036655187606812 Time taken: 0.31052303314208984\n",
            "Batch Number: 1661 Loss: 1.7885545492172241 Time taken: 0.307971715927124\n",
            "Batch Number: 1662 Loss: 1.8117231130599976 Time taken: 0.29681921005249023\n",
            "Batch Number: 1663 Loss: 1.8050060272216797 Time taken: 0.29062581062316895\n",
            "Batch Number: 1664 Loss: 1.7747774124145508 Time taken: 0.3219630718231201\n",
            "Batch Number: 1665 Loss: 1.7919745445251465 Time taken: 0.2948789596557617\n",
            "Batch Number: 1666 Loss: 1.79567551612854 Time taken: 0.33043384552001953\n",
            "Batch Number: 1667 Loss: 1.7977756261825562 Time taken: 0.3345043659210205\n",
            "Batch Number: 1668 Loss: 1.8247087001800537 Time taken: 0.30120372772216797\n",
            "Batch Number: 1669 Loss: 1.7967201471328735 Time taken: 0.2993495464324951\n",
            "Batch Number: 1670 Loss: 1.7909942865371704 Time taken: 0.31859755516052246\n",
            "Batch Number: 1671 Loss: 1.80784010887146 Time taken: 0.3087799549102783\n",
            "Batch Number: 1672 Loss: 1.7913175821304321 Time taken: 0.30143070220947266\n",
            "Batch Number: 1673 Loss: 1.8086702823638916 Time taken: 0.33164119720458984\n",
            "Batch Number: 1674 Loss: 1.8055033683776855 Time taken: 0.31080150604248047\n",
            "Batch Number: 1675 Loss: 1.7725783586502075 Time taken: 0.2980659008026123\n",
            "Batch Number: 1676 Loss: 1.80840003490448 Time taken: 0.3023102283477783\n",
            "Batch Number: 1677 Loss: 1.7986679077148438 Time taken: 0.35426974296569824\n",
            "Batch Number: 1678 Loss: 1.7810320854187012 Time taken: 0.3794710636138916\n",
            "Batch Number: 1679 Loss: 1.7976704835891724 Time taken: 0.34960460662841797\n",
            "Batch Number: 1680 Loss: 1.7847890853881836 Time taken: 0.3076179027557373\n",
            "Batch Number: 1681 Loss: 1.766491413116455 Time taken: 0.2882237434387207\n",
            "Batch Number: 1682 Loss: 1.7950721979141235 Time taken: 0.30417919158935547\n",
            "Batch Number: 1683 Loss: 1.7770999670028687 Time taken: 0.38475584983825684\n",
            "Batch Number: 1684 Loss: 1.7665376663208008 Time taken: 0.37513065338134766\n",
            "Batch Number: 1685 Loss: 1.8052281141281128 Time taken: 0.30211973190307617\n",
            "Batch Number: 1686 Loss: 1.7996916770935059 Time taken: 0.3066272735595703\n",
            "Batch Number: 1687 Loss: 1.7757370471954346 Time taken: 0.36212778091430664\n",
            "Batch Number: 1688 Loss: 1.8022327423095703 Time taken: 0.3019411563873291\n",
            "Batch Number: 1689 Loss: 1.7803356647491455 Time taken: 0.3095576763153076\n",
            "Batch Number: 1690 Loss: 1.7964524030685425 Time taken: 0.3334074020385742\n",
            "Batch Number: 1691 Loss: 1.8170535564422607 Time taken: 0.3379850387573242\n",
            "Batch Number: 1692 Loss: 1.8055307865142822 Time taken: 0.31606531143188477\n",
            "Batch Number: 1693 Loss: 1.8059850931167603 Time taken: 0.31511783599853516\n",
            "Batch Number: 1694 Loss: 1.8146154880523682 Time taken: 0.3049650192260742\n",
            "Batch Number: 1695 Loss: 1.826758861541748 Time taken: 0.3058440685272217\n",
            "Batch Number: 1696 Loss: 1.805024266242981 Time taken: 0.30615854263305664\n",
            "Batch Number: 1697 Loss: 1.802144169807434 Time taken: 0.32959723472595215\n",
            "Batch Number: 1698 Loss: 1.8104956150054932 Time taken: 0.3011312484741211\n",
            "Batch Number: 1699 Loss: 1.791250228881836 Time taken: 0.2995774745941162\n",
            "Batch Number: 1700 Loss: 1.7822730541229248 Time taken: 0.3048408031463623\n",
            "Batch Number: 1701 Loss: 1.7918931245803833 Time taken: 0.3248288631439209\n",
            "Batch Number: 1702 Loss: 1.782293677330017 Time taken: 0.3283576965332031\n",
            "Batch Number: 1703 Loss: 1.7769209146499634 Time taken: 0.33531689643859863\n",
            "Batch Number: 1704 Loss: 1.7886862754821777 Time taken: 0.30982112884521484\n",
            "Batch Number: 1705 Loss: 1.761736273765564 Time taken: 0.31412792205810547\n",
            "Batch Number: 1706 Loss: 1.7737221717834473 Time taken: 0.3276674747467041\n",
            "Batch Number: 1707 Loss: 1.7647807598114014 Time taken: 0.3132596015930176\n",
            "Batch Number: 1708 Loss: 1.7763359546661377 Time taken: 0.29352712631225586\n",
            "Batch Number: 1709 Loss: 1.764870047569275 Time taken: 0.3177628517150879\n",
            "Batch Number: 1710 Loss: 1.7992230653762817 Time taken: 0.32375073432922363\n",
            "Batch Number: 1711 Loss: 1.7987990379333496 Time taken: 0.3029937744140625\n",
            "Batch Number: 1712 Loss: 1.7849754095077515 Time taken: 0.31379151344299316\n",
            "Batch Number: 1713 Loss: 1.8033627271652222 Time taken: 0.30083131790161133\n",
            "Batch Number: 1714 Loss: 1.7898329496383667 Time taken: 0.3152024745941162\n",
            "Batch Number: 1715 Loss: 1.797624945640564 Time taken: 0.3162224292755127\n",
            "Batch Number: 1716 Loss: 1.8254741430282593 Time taken: 0.3151588439941406\n",
            "Batch Number: 1717 Loss: 1.823219895362854 Time taken: 0.3319265842437744\n",
            "Batch Number: 1718 Loss: 1.824807047843933 Time taken: 0.3135809898376465\n",
            "Batch Number: 1719 Loss: 1.8030314445495605 Time taken: 0.3438379764556885\n",
            "Batch Number: 1720 Loss: 1.8166778087615967 Time taken: 0.37906360626220703\n",
            "Batch Number: 1721 Loss: 1.827996850013733 Time taken: 0.3925797939300537\n",
            "Batch Number: 1722 Loss: 1.8308608531951904 Time taken: 0.29735612869262695\n",
            "Batch Number: 1723 Loss: 1.797965407371521 Time taken: 0.30229806900024414\n",
            "Batch Number: 1724 Loss: 1.8094382286071777 Time taken: 0.30803394317626953\n",
            "Batch Number: 1725 Loss: 1.837450623512268 Time taken: 0.30527544021606445\n",
            "Batch Number: 1726 Loss: 1.8449746370315552 Time taken: 0.3006553649902344\n",
            "Batch Number: 1727 Loss: 1.8182722330093384 Time taken: 0.29206132888793945\n",
            "Batch Number: 1728 Loss: 1.823585033416748 Time taken: 0.3222935199737549\n",
            "Batch Number: 1729 Loss: 1.8239531517028809 Time taken: 0.3038771152496338\n",
            "Batch Number: 1730 Loss: 1.8476841449737549 Time taken: 0.3022880554199219\n",
            "Batch Number: 1731 Loss: 1.803607702255249 Time taken: 0.31458425521850586\n",
            "Batch Number: 1732 Loss: 1.8243893384933472 Time taken: 0.301039457321167\n",
            "Batch Number: 1733 Loss: 1.8328182697296143 Time taken: 0.30087804794311523\n",
            "Batch Number: 1734 Loss: 1.8241513967514038 Time taken: 0.3482930660247803\n",
            "Batch Number: 1735 Loss: 1.8003079891204834 Time taken: 0.294543981552124\n",
            "Batch Number: 1736 Loss: 1.7992815971374512 Time taken: 0.28783512115478516\n",
            "Batch Number: 1737 Loss: 1.803760290145874 Time taken: 0.30109739303588867\n",
            "Batch Number: 1738 Loss: 1.8106929063796997 Time taken: 0.30005931854248047\n",
            "Batch Number: 1739 Loss: 1.7776414155960083 Time taken: 0.30516839027404785\n",
            "Batch Number: 1740 Loss: 1.8170456886291504 Time taken: 0.29472899436950684\n",
            "Batch Number: 1741 Loss: 1.8019682168960571 Time taken: 0.37078404426574707\n",
            "Batch Number: 1742 Loss: 1.8090161085128784 Time taken: 0.39005088806152344\n",
            "Batch Number: 1743 Loss: 1.8096076250076294 Time taken: 0.35942554473876953\n",
            "Batch Number: 1744 Loss: 1.808699607849121 Time taken: 0.33172106742858887\n",
            "Batch Number: 1745 Loss: 1.8048523664474487 Time taken: 0.3750574588775635\n",
            "Batch Number: 1746 Loss: 1.788010835647583 Time taken: 0.3295891284942627\n",
            "Batch Number: 1747 Loss: 1.8022959232330322 Time taken: 0.3030712604522705\n",
            "Batch Number: 1748 Loss: 1.789446234703064 Time taken: 0.3102085590362549\n",
            "Batch Number: 1749 Loss: 1.7862571477890015 Time taken: 0.2955043315887451\n",
            "Batch Number: 1750 Loss: 1.795698881149292 Time taken: 0.308394193649292\n",
            "Batch Number: 1751 Loss: 1.7658460140228271 Time taken: 0.30170464515686035\n",
            "Batch Number: 1752 Loss: 1.8042659759521484 Time taken: 0.29444241523742676\n",
            "Batch Number: 1753 Loss: 1.7990988492965698 Time taken: 0.32770705223083496\n",
            "Batch Number: 1754 Loss: 1.792914867401123 Time taken: 0.3062407970428467\n",
            "Batch Number: 1755 Loss: 1.7782094478607178 Time taken: 0.300642728805542\n",
            "Batch Number: 1756 Loss: 1.7473255395889282 Time taken: 0.31178832054138184\n",
            "Batch Number: 1757 Loss: 1.763712763786316 Time taken: 0.30103206634521484\n",
            "Batch Number: 1758 Loss: 1.7957435846328735 Time taken: 0.3256947994232178\n",
            "Batch Number: 1759 Loss: 1.7534291744232178 Time taken: 0.31507277488708496\n",
            "Batch Number: 1760 Loss: 1.8134814500808716 Time taken: 0.300123929977417\n",
            "Batch Number: 1761 Loss: 1.779631495475769 Time taken: 0.3075144290924072\n",
            "Batch Number: 1762 Loss: 1.7962020635604858 Time taken: 0.32187557220458984\n",
            "Batch Number: 1763 Loss: 1.7885777950286865 Time taken: 0.37401819229125977\n",
            "Batch Number: 1764 Loss: 1.8090707063674927 Time taken: 0.3847346305847168\n",
            "Batch Number: 1765 Loss: 1.820603609085083 Time taken: 0.37919044494628906\n",
            "Batch Number: 1766 Loss: 1.8320285081863403 Time taken: 0.3903660774230957\n",
            "Batch Number: 1767 Loss: 1.8504916429519653 Time taken: 0.36863064765930176\n",
            "Batch Number: 1768 Loss: 1.7804179191589355 Time taken: 0.33318305015563965\n",
            "Batch Number: 1769 Loss: 1.803110122680664 Time taken: 0.3116145133972168\n",
            "Batch Number: 1770 Loss: 1.7858901023864746 Time taken: 0.3292412757873535\n",
            "Batch Number: 1771 Loss: 1.7946778535842896 Time taken: 0.38493967056274414\n",
            "Batch Number: 1772 Loss: 1.8176466226577759 Time taken: 0.39476656913757324\n",
            "Batch Number: 1773 Loss: 1.8287187814712524 Time taken: 0.3293635845184326\n",
            "Batch Number: 1774 Loss: 1.7803477048873901 Time taken: 0.3045938014984131\n",
            "Batch Number: 1775 Loss: 1.7738054990768433 Time taken: 0.31252026557922363\n",
            "Batch Number: 1776 Loss: 1.8057509660720825 Time taken: 0.330747127532959\n",
            "Batch Number: 1777 Loss: 1.8212002515792847 Time taken: 0.31977033615112305\n",
            "Batch Number: 1778 Loss: 1.8064744472503662 Time taken: 0.314929723739624\n",
            "Batch Number: 1779 Loss: 1.8173778057098389 Time taken: 0.3048539161682129\n",
            "Batch Number: 1780 Loss: 1.8027740716934204 Time taken: 0.3106667995452881\n",
            "Batch Number: 1781 Loss: 1.8099623918533325 Time taken: 0.3106203079223633\n",
            "Batch Number: 1782 Loss: 1.7785319089889526 Time taken: 0.3456695079803467\n",
            "Batch Number: 1783 Loss: 1.785498857498169 Time taken: 0.31548500061035156\n",
            "Batch Number: 1784 Loss: 1.7882272005081177 Time taken: 0.33513760566711426\n",
            "Batch Number: 1785 Loss: 1.7927449941635132 Time taken: 0.3289480209350586\n",
            "Batch Number: 1786 Loss: 1.7991564273834229 Time taken: 0.31551218032836914\n",
            "Batch Number: 1787 Loss: 1.7646008729934692 Time taken: 0.3048665523529053\n",
            "Batch Number: 1788 Loss: 1.7986770868301392 Time taken: 0.3048861026763916\n",
            "Batch Number: 1789 Loss: 1.788108229637146 Time taken: 0.2950098514556885\n",
            "Batch Number: 1790 Loss: 1.7579247951507568 Time taken: 0.32444119453430176\n",
            "Batch Number: 1791 Loss: 1.7859326601028442 Time taken: 0.31751513481140137\n",
            "Batch Number: 1792 Loss: 1.796041488647461 Time taken: 0.30481982231140137\n",
            "Batch Number: 1793 Loss: 1.79805588722229 Time taken: 0.31075096130371094\n",
            "Batch Number: 1794 Loss: 1.7811280488967896 Time taken: 0.29972100257873535\n",
            "Batch Number: 1795 Loss: 1.7753466367721558 Time taken: 0.31450963020324707\n",
            "Batch Number: 1796 Loss: 1.7644544839859009 Time taken: 0.30680060386657715\n",
            "Batch Number: 1797 Loss: 1.77367103099823 Time taken: 0.3051588535308838\n",
            "Batch Number: 1798 Loss: 1.7687009572982788 Time taken: 0.3017604351043701\n",
            "Batch Number: 1799 Loss: 1.756722092628479 Time taken: 0.34587907791137695\n",
            "Batch Number: 1800 Loss: 1.7440993785858154 Time taken: 0.39266061782836914\n",
            "Batch Number: 1801 Loss: 1.7688803672790527 Time taken: 0.3258628845214844\n",
            "Batch Number: 1802 Loss: 1.7349328994750977 Time taken: 0.30024147033691406\n",
            "Batch Number: 1803 Loss: 1.7795864343643188 Time taken: 0.29497575759887695\n",
            "Batch Number: 1804 Loss: 1.7504123449325562 Time taken: 0.3049905300140381\n",
            "Batch Number: 1805 Loss: 1.7442909479141235 Time taken: 0.30722784996032715\n",
            "Batch Number: 1806 Loss: 1.7445353269577026 Time taken: 0.29674744606018066\n",
            "Batch Number: 1807 Loss: 1.7325305938720703 Time taken: 0.33923816680908203\n",
            "Batch Number: 1808 Loss: 1.7481528520584106 Time taken: 0.377166748046875\n",
            "Batch Number: 1809 Loss: 1.7598309516906738 Time taken: 0.38956713676452637\n",
            "Batch Number: 1810 Loss: 1.7465406656265259 Time taken: 0.30940723419189453\n",
            "Batch Number: 1811 Loss: 1.7516807317733765 Time taken: 0.30643391609191895\n",
            "Batch Number: 1812 Loss: 1.7519114017486572 Time taken: 0.3020615577697754\n",
            "Batch Number: 1813 Loss: 1.7536084651947021 Time taken: 0.34676456451416016\n",
            "Batch Number: 1814 Loss: 1.7626889944076538 Time taken: 0.2997300624847412\n",
            "Batch Number: 1815 Loss: 1.7784208059310913 Time taken: 0.3860325813293457\n",
            "Batch Number: 1816 Loss: 1.7605845928192139 Time taken: 0.33365941047668457\n",
            "Batch Number: 1817 Loss: 1.757353663444519 Time taken: 0.30600404739379883\n",
            "Batch Number: 1818 Loss: 1.759591817855835 Time taken: 0.3070557117462158\n",
            "Batch Number: 1819 Loss: 1.7596317529678345 Time taken: 0.3065497875213623\n",
            "Batch Number: 1820 Loss: 1.7531402111053467 Time taken: 0.2966270446777344\n",
            "Batch Number: 1821 Loss: 1.7295809984207153 Time taken: 0.32654714584350586\n",
            "Batch Number: 1822 Loss: 1.7475675344467163 Time taken: 0.3474006652832031\n",
            "Batch Number: 1823 Loss: 1.7545453310012817 Time taken: 0.36484789848327637\n",
            "Batch Number: 1824 Loss: 1.7627063989639282 Time taken: 0.3946845531463623\n",
            "Batch Number: 1825 Loss: 1.778275489807129 Time taken: 0.3267521858215332\n",
            "Batch Number: 1826 Loss: 1.7316776514053345 Time taken: 0.30368781089782715\n",
            "Batch Number: 1827 Loss: 1.7515003681182861 Time taken: 0.3097503185272217\n",
            "Batch Number: 1828 Loss: 1.7587082386016846 Time taken: 0.35776805877685547\n",
            "Batch Number: 1829 Loss: 1.7635263204574585 Time taken: 0.39006853103637695\n",
            "Batch Number: 1830 Loss: 1.7611042261123657 Time taken: 0.36633896827697754\n",
            "Batch Number: 1831 Loss: 1.7483655214309692 Time taken: 0.3327977657318115\n",
            "Batch Number: 1832 Loss: 1.7551151514053345 Time taken: 0.37073540687561035\n",
            "Batch Number: 1833 Loss: 1.7278966903686523 Time taken: 0.3317584991455078\n",
            "Batch Number: 1834 Loss: 1.777449131011963 Time taken: 0.29227638244628906\n",
            "Batch Number: 1835 Loss: 1.7435882091522217 Time taken: 0.30305957794189453\n",
            "Batch Number: 1836 Loss: 1.7691009044647217 Time taken: 0.3207368850708008\n",
            "Batch Number: 1837 Loss: 1.7744109630584717 Time taken: 0.30803942680358887\n",
            "Batch Number: 1838 Loss: 1.7778329849243164 Time taken: 0.292344331741333\n",
            "Batch Number: 1839 Loss: 1.800097942352295 Time taken: 0.31099605560302734\n",
            "Batch Number: 1840 Loss: 1.7653381824493408 Time taken: 0.3119969367980957\n",
            "Batch Number: 1841 Loss: 1.7745097875595093 Time taken: 0.29825353622436523\n",
            "Batch Number: 1842 Loss: 1.7732657194137573 Time taken: 0.3096439838409424\n",
            "Batch Number: 1843 Loss: 1.7658123970031738 Time taken: 0.3274815082550049\n",
            "Batch Number: 1844 Loss: 1.7648340463638306 Time taken: 0.3013150691986084\n",
            "Batch Number: 1845 Loss: 1.7440186738967896 Time taken: 0.31064462661743164\n",
            "Batch Number: 1846 Loss: 1.7691184282302856 Time taken: 0.302370548248291\n",
            "Batch Number: 1847 Loss: 1.7451376914978027 Time taken: 0.306995153427124\n",
            "Batch Number: 1848 Loss: 1.7526220083236694 Time taken: 0.3344738483428955\n",
            "Batch Number: 1849 Loss: 1.7426668405532837 Time taken: 0.34250926971435547\n",
            "Batch Number: 1850 Loss: 1.7527862787246704 Time taken: 0.29628419876098633\n",
            "Batch Number: 1851 Loss: 1.7608473300933838 Time taken: 0.29674434661865234\n",
            "Batch Number: 1852 Loss: 1.7679458856582642 Time taken: 0.31426572799682617\n",
            "Batch Number: 1853 Loss: 1.7574150562286377 Time taken: 0.29123806953430176\n",
            "Batch Number: 1854 Loss: 1.7535158395767212 Time taken: 0.2867715358734131\n",
            "Batch Number: 1855 Loss: 1.7758275270462036 Time taken: 0.3049201965332031\n",
            "Batch Number: 1856 Loss: 1.7411911487579346 Time taken: 0.31366515159606934\n",
            "Batch Number: 1857 Loss: 1.7298108339309692 Time taken: 0.29535818099975586\n",
            "Batch Number: 1858 Loss: 1.7543901205062866 Time taken: 0.32181453704833984\n",
            "Batch Number: 1859 Loss: 1.7639940977096558 Time taken: 0.30620908737182617\n",
            "Batch Number: 1860 Loss: 1.7666759490966797 Time taken: 0.2954277992248535\n",
            "Batch Number: 1861 Loss: 1.7656420469284058 Time taken: 0.3149275779724121\n",
            "Batch Number: 1862 Loss: 1.7299391031265259 Time taken: 0.3298013210296631\n",
            "Batch Number: 1863 Loss: 1.7476558685302734 Time taken: 0.30234670639038086\n",
            "Batch Number: 1864 Loss: 1.7316733598709106 Time taken: 0.287463903427124\n",
            "Batch Number: 1865 Loss: 1.7260243892669678 Time taken: 0.3110651969909668\n",
            "Batch Number: 1866 Loss: 1.7579039335250854 Time taken: 0.31054043769836426\n",
            "Batch Number: 1867 Loss: 1.7696291208267212 Time taken: 0.30730319023132324\n",
            "Batch Number: 1868 Loss: 1.7670838832855225 Time taken: 0.3532073497772217\n",
            "Batch Number: 1869 Loss: 1.7548507452011108 Time taken: 0.2929363250732422\n",
            "Batch Number: 1870 Loss: 1.7795555591583252 Time taken: 0.2927742004394531\n",
            "Batch Number: 1871 Loss: 1.761622428894043 Time taken: 0.346834659576416\n",
            "Batch Number: 1872 Loss: 1.7711129188537598 Time taken: 0.30774664878845215\n",
            "Batch Number: 1873 Loss: 1.785479187965393 Time taken: 0.32960987091064453\n",
            "Batch Number: 1874 Loss: 1.784200668334961 Time taken: 0.37332582473754883\n",
            "Batch Number: 1875 Loss: 1.7673673629760742 Time taken: 0.3137476444244385\n",
            "Batch Number: 1876 Loss: 1.7859811782836914 Time taken: 0.2999553680419922\n",
            "Batch Number: 1877 Loss: 1.7861311435699463 Time taken: 0.31084179878234863\n",
            "Batch Number: 1878 Loss: 1.759881854057312 Time taken: 0.30457401275634766\n",
            "Batch Number: 1879 Loss: 1.7628333568572998 Time taken: 0.29826927185058594\n",
            "Batch Number: 1880 Loss: 1.737741231918335 Time taken: 0.3160083293914795\n",
            "Batch Number: 1881 Loss: 1.751799464225769 Time taken: 0.34731197357177734\n",
            "Batch Number: 1882 Loss: 1.749822735786438 Time taken: 0.2895841598510742\n",
            "Batch Number: 1883 Loss: 1.731960415840149 Time taken: 0.30895376205444336\n",
            "Batch Number: 1884 Loss: 1.7435308694839478 Time taken: 0.3161885738372803\n",
            "Batch Number: 1885 Loss: 1.716676950454712 Time taken: 0.3346700668334961\n",
            "Batch Number: 1886 Loss: 1.7455697059631348 Time taken: 0.3699355125427246\n",
            "Batch Number: 1887 Loss: 1.734236240386963 Time taken: 0.32954835891723633\n",
            "Batch Number: 1888 Loss: 1.741076946258545 Time taken: 0.2975308895111084\n",
            "Batch Number: 1889 Loss: 1.7346912622451782 Time taken: 0.34897303581237793\n",
            "Batch Number: 1890 Loss: 1.7426395416259766 Time taken: 0.3261911869049072\n",
            "Batch Number: 1891 Loss: 1.7653025388717651 Time taken: 0.29915452003479004\n",
            "Batch Number: 1892 Loss: 1.7511780261993408 Time taken: 0.30448436737060547\n",
            "Batch Number: 1893 Loss: 1.747274398803711 Time taken: 0.35150718688964844\n",
            "Batch Number: 1894 Loss: 1.77837073802948 Time taken: 0.3017446994781494\n",
            "Batch Number: 1895 Loss: 1.7897611856460571 Time taken: 0.2992525100708008\n",
            "Batch Number: 1896 Loss: 1.7809877395629883 Time taken: 0.3078727722167969\n",
            "Batch Number: 1897 Loss: 1.7842820882797241 Time taken: 0.30640363693237305\n",
            "Batch Number: 1898 Loss: 1.78558349609375 Time taken: 0.29751062393188477\n",
            "Batch Number: 1899 Loss: 1.7589572668075562 Time taken: 0.3421778678894043\n",
            "Batch Number: 1900 Loss: 1.7562298774719238 Time taken: 0.3066747188568115\n",
            "Batch Number: 1901 Loss: 1.7793197631835938 Time taken: 0.29502058029174805\n",
            "Batch Number: 1902 Loss: 1.7934010028839111 Time taken: 0.3434615135192871\n",
            "Batch Number: 1903 Loss: 1.7811353206634521 Time taken: 0.308243989944458\n",
            "Batch Number: 1904 Loss: 1.770911455154419 Time taken: 0.30362367630004883\n",
            "Batch Number: 1905 Loss: 1.7801347970962524 Time taken: 0.3319664001464844\n",
            "Batch Number: 1906 Loss: 1.796852946281433 Time taken: 0.3701756000518799\n",
            "Batch Number: 1907 Loss: 1.78664231300354 Time taken: 0.36465001106262207\n",
            "Batch Number: 1908 Loss: 1.7818048000335693 Time taken: 0.3508765697479248\n",
            "Batch Number: 1909 Loss: 1.7878689765930176 Time taken: 0.3717496395111084\n",
            "Batch Number: 1910 Loss: 1.7963874340057373 Time taken: 0.3288228511810303\n",
            "Batch Number: 1911 Loss: 1.800531029701233 Time taken: 0.31256937980651855\n",
            "Batch Number: 1912 Loss: 1.7914096117019653 Time taken: 0.3187084197998047\n",
            "Batch Number: 1913 Loss: 1.7780650854110718 Time taken: 0.32612085342407227\n",
            "Batch Number: 1914 Loss: 1.7625993490219116 Time taken: 0.34972286224365234\n",
            "Batch Number: 1915 Loss: 1.7708559036254883 Time taken: 0.39047694206237793\n",
            "Batch Number: 1916 Loss: 1.7809797525405884 Time taken: 0.34992289543151855\n",
            "Batch Number: 1917 Loss: 1.7710106372833252 Time taken: 0.384746789932251\n",
            "Batch Number: 1918 Loss: 1.764329433441162 Time taken: 0.36903882026672363\n",
            "Batch Number: 1919 Loss: 1.768640160560608 Time taken: 0.32027101516723633\n",
            "Batch Number: 1920 Loss: 1.7888760566711426 Time taken: 0.3247373104095459\n",
            "Batch Number: 1921 Loss: 1.889021396636963 Time taken: 0.304537296295166\n",
            "Batch Number: 1922 Loss: 1.8296856880187988 Time taken: 0.340731143951416\n",
            "Batch Number: 1923 Loss: 1.80769944190979 Time taken: 0.4061145782470703\n",
            "Batch Number: 1924 Loss: 1.7935346364974976 Time taken: 0.37268781661987305\n",
            "Batch Number: 1925 Loss: 1.8065615892410278 Time taken: 0.31520533561706543\n",
            "Batch Number: 1926 Loss: 1.8002537488937378 Time taken: 0.3161909580230713\n",
            "Batch Number: 1927 Loss: 1.7765134572982788 Time taken: 0.29998230934143066\n",
            "Batch Number: 1928 Loss: 1.812103271484375 Time taken: 0.3018009662628174\n",
            "Batch Number: 1929 Loss: 1.8018792867660522 Time taken: 0.39099884033203125\n",
            "Batch Number: 1930 Loss: 1.7986090183258057 Time taken: 0.37281107902526855\n",
            "Batch Number: 1931 Loss: 1.775586485862732 Time taken: 0.3264195919036865\n",
            "Batch Number: 1932 Loss: 1.7708446979522705 Time taken: 0.39411473274230957\n",
            "Batch Number: 1933 Loss: 1.78514564037323 Time taken: 0.3732268810272217\n",
            "Batch Number: 1934 Loss: 1.7839772701263428 Time taken: 0.30467963218688965\n",
            "Batch Number: 1935 Loss: 1.760960578918457 Time taken: 0.3157827854156494\n",
            "Batch Number: 1936 Loss: 1.7218583822250366 Time taken: 0.299299955368042\n",
            "Batch Number: 1937 Loss: 1.7515268325805664 Time taken: 0.30139636993408203\n",
            "Batch Number: 1938 Loss: 1.7302461862564087 Time taken: 0.31855177879333496\n",
            "Batch Number: 1939 Loss: 1.7611727714538574 Time taken: 0.29827880859375\n",
            "Batch Number: 1940 Loss: 1.7522335052490234 Time taken: 0.29007577896118164\n",
            "Batch Number: 1941 Loss: 1.76607084274292 Time taken: 0.3107490539550781\n",
            "Batch Number: 1942 Loss: 1.7677932977676392 Time taken: 0.2929689884185791\n",
            "Batch Number: 1943 Loss: 1.7921210527420044 Time taken: 0.3010694980621338\n",
            "Batch Number: 1944 Loss: 1.7805391550064087 Time taken: 0.31310606002807617\n",
            "Batch Number: 1945 Loss: 1.76887047290802 Time taken: 0.31421613693237305\n",
            "Batch Number: 1946 Loss: 1.7736746072769165 Time taken: 0.3258655071258545\n",
            "Batch Number: 1947 Loss: 1.752445101737976 Time taken: 0.2939262390136719\n",
            "Batch Number: 1948 Loss: 1.7402957677841187 Time taken: 0.3195791244506836\n",
            "Batch Number: 1949 Loss: 1.756294846534729 Time taken: 0.30267858505249023\n",
            "Batch Number: 1950 Loss: 1.7465307712554932 Time taken: 0.2966451644897461\n",
            "Batch Number: 1951 Loss: 1.7498384714126587 Time taken: 0.3178751468658447\n",
            "Batch Number: 1952 Loss: 1.762652039527893 Time taken: 0.30510807037353516\n",
            "Batch Number: 1953 Loss: 1.7594736814498901 Time taken: 0.3163948059082031\n",
            "Batch Number: 1954 Loss: 1.7521127462387085 Time taken: 0.3160526752471924\n",
            "Batch Number: 1955 Loss: 1.767138123512268 Time taken: 0.3045518398284912\n",
            "Batch Number: 1956 Loss: 1.737902045249939 Time taken: 0.307420015335083\n",
            "Batch Number: 1957 Loss: 1.7579079866409302 Time taken: 0.2954676151275635\n",
            "Batch Number: 1958 Loss: 1.7613097429275513 Time taken: 0.3037607669830322\n",
            "Batch Number: 1959 Loss: 1.7815825939178467 Time taken: 0.3002333641052246\n",
            "Batch Number: 1960 Loss: 1.7527576684951782 Time taken: 0.28269362449645996\n",
            "Batch Number: 1961 Loss: 1.7485668659210205 Time taken: 0.30324506759643555\n",
            "Batch Number: 1962 Loss: 1.7754470109939575 Time taken: 0.2886924743652344\n",
            "Batch Number: 1963 Loss: 1.7427752017974854 Time taken: 0.29651665687561035\n",
            "Batch Number: 1964 Loss: 1.7501118183135986 Time taken: 0.3001132011413574\n",
            "Batch Number: 1965 Loss: 1.748112678527832 Time taken: 0.31345200538635254\n",
            "Batch Number: 1966 Loss: 1.763951301574707 Time taken: 0.33782315254211426\n",
            "Batch Number: 1967 Loss: 1.735670804977417 Time taken: 0.38989925384521484\n",
            "Batch Number: 1968 Loss: 1.732633352279663 Time taken: 0.32885098457336426\n",
            "Batch Number: 1969 Loss: 1.7408586740493774 Time taken: 0.29926323890686035\n",
            "Batch Number: 1970 Loss: 1.7634880542755127 Time taken: 0.30824971199035645\n",
            "Batch Number: 1971 Loss: 1.748095154762268 Time taken: 0.28505778312683105\n",
            "Batch Number: 1972 Loss: 1.7255134582519531 Time taken: 0.2943124771118164\n",
            "Batch Number: 1973 Loss: 1.7367913722991943 Time taken: 0.29654884338378906\n",
            "Batch Number: 1974 Loss: 1.7115862369537354 Time taken: 0.33377838134765625\n",
            "Batch Number: 1975 Loss: 1.7358295917510986 Time taken: 0.2946441173553467\n",
            "Batch Number: 1976 Loss: 1.727546215057373 Time taken: 0.3337981700897217\n",
            "Batch Number: 1977 Loss: 1.752951741218567 Time taken: 0.40544629096984863\n",
            "Batch Number: 1978 Loss: 1.7329293489456177 Time taken: 0.3820791244506836\n",
            "Batch Number: 1979 Loss: 1.7344316244125366 Time taken: 0.36359286308288574\n",
            "Batch Number: 1980 Loss: 1.7193026542663574 Time taken: 0.3687121868133545\n",
            "Batch Number: 1981 Loss: 1.7401400804519653 Time taken: 0.2951529026031494\n",
            "Batch Number: 1982 Loss: 1.699385166168213 Time taken: 0.3870840072631836\n",
            "Batch Number: 1983 Loss: 1.7074851989746094 Time taken: 0.34038877487182617\n",
            "Batch Number: 1984 Loss: 1.7046250104904175 Time taken: 0.3054955005645752\n",
            "Batch Number: 1985 Loss: 1.7005646228790283 Time taken: 0.2987051010131836\n",
            "Batch Number: 1986 Loss: 1.7357711791992188 Time taken: 0.3031461238861084\n",
            "Batch Number: 1987 Loss: 1.7311151027679443 Time taken: 0.3002190589904785\n",
            "Batch Number: 1988 Loss: 1.700217843055725 Time taken: 0.3032522201538086\n",
            "Batch Number: 1989 Loss: 1.7218166589736938 Time taken: 0.3321542739868164\n",
            "Batch Number: 1990 Loss: 1.6924757957458496 Time taken: 0.3074369430541992\n",
            "Batch Number: 1991 Loss: 1.7543809413909912 Time taken: 0.2999439239501953\n",
            "Batch Number: 1992 Loss: 1.756345510482788 Time taken: 0.3318605422973633\n",
            "Batch Number: 1993 Loss: 1.7435977458953857 Time taken: 0.3138296604156494\n",
            "Batch Number: 1994 Loss: 1.7340024709701538 Time taken: 0.30071282386779785\n",
            "Batch Number: 1995 Loss: 1.730474591255188 Time taken: 0.3169524669647217\n",
            "Batch Number: 1996 Loss: 1.7525866031646729 Time taken: 0.31623053550720215\n",
            "Batch Number: 1997 Loss: 1.7137211561203003 Time taken: 0.3147008419036865\n",
            "Batch Number: 1998 Loss: 1.7405117750167847 Time taken: 0.31958532333374023\n",
            "Batch Number: 1999 Loss: 1.7208178043365479 Time taken: 0.29435086250305176\n",
            "Batch Number: 2000 Loss: 1.7097879648208618 Time taken: 0.2964351177215576\n",
            "Batch Number: 2001 Loss: 1.722082495689392 Time taken: 0.28688740730285645\n",
            "Batch Number: 2002 Loss: 1.7325612306594849 Time taken: 0.33298206329345703\n",
            "Batch Number: 2003 Loss: 1.729089379310608 Time taken: 0.2854292392730713\n",
            "Batch Number: 2004 Loss: 1.7044645547866821 Time taken: 0.29148125648498535\n",
            "Batch Number: 2005 Loss: 1.7722032070159912 Time taken: 0.3199794292449951\n",
            "Batch Number: 2006 Loss: 1.7060595750808716 Time taken: 0.2885470390319824\n",
            "Batch Number: 2007 Loss: 1.7165248394012451 Time taken: 0.29886293411254883\n",
            "Batch Number: 2008 Loss: 1.7211170196533203 Time taken: 0.3001983165740967\n",
            "Batch Number: 2009 Loss: 1.7170445919036865 Time taken: 0.30011868476867676\n",
            "Batch Number: 2010 Loss: 1.7003344297409058 Time taken: 0.3179922103881836\n",
            "Batch Number: 2011 Loss: 1.7234036922454834 Time taken: 0.3025848865509033\n",
            "Batch Number: 2012 Loss: 1.7255194187164307 Time taken: 0.3160393238067627\n",
            "Batch Number: 2013 Loss: 1.7210172414779663 Time taken: 0.30149149894714355\n",
            "Batch Number: 2014 Loss: 1.7286450862884521 Time taken: 0.3296847343444824\n",
            "Batch Number: 2015 Loss: 1.7239041328430176 Time taken: 0.34149765968322754\n",
            "Batch Number: 2016 Loss: 1.7698678970336914 Time taken: 0.2915356159210205\n",
            "Batch Number: 2017 Loss: 1.7427898645401 Time taken: 0.29176902770996094\n",
            "Batch Number: 2018 Loss: 1.7650810480117798 Time taken: 0.3131723403930664\n",
            "Batch Number: 2019 Loss: 1.758523941040039 Time taken: 0.2907900810241699\n",
            "Batch Number: 2020 Loss: 1.7435826063156128 Time taken: 0.28909754753112793\n",
            "Batch Number: 2021 Loss: 1.7210328578948975 Time taken: 0.3130502700805664\n",
            "Batch Number: 2022 Loss: 1.7382892370224 Time taken: 0.31870579719543457\n",
            "Batch Number: 2023 Loss: 1.7417336702346802 Time taken: 0.3000330924987793\n",
            "Batch Number: 2024 Loss: 1.7200636863708496 Time taken: 0.34101390838623047\n",
            "Batch Number: 2025 Loss: 1.7320995330810547 Time taken: 0.37161922454833984\n",
            "Batch Number: 2026 Loss: 1.6721030473709106 Time taken: 0.36827516555786133\n",
            "Batch Number: 2027 Loss: 1.744765281677246 Time taken: 0.37462902069091797\n",
            "Batch Number: 2028 Loss: 1.7331640720367432 Time taken: 0.37236881256103516\n",
            "Batch Number: 2029 Loss: 1.7132819890975952 Time taken: 0.36478090286254883\n",
            "Batch Number: 2030 Loss: 1.7269713878631592 Time taken: 0.3109288215637207\n",
            "Batch Number: 2031 Loss: 1.7667961120605469 Time taken: 0.29197239875793457\n",
            "Batch Number: 2032 Loss: 1.7222329378128052 Time taken: 0.296738862991333\n",
            "Batch Number: 2033 Loss: 1.7265366315841675 Time taken: 0.34798502922058105\n",
            "Batch Number: 2034 Loss: 1.7264291048049927 Time taken: 0.2979729175567627\n",
            "Batch Number: 2035 Loss: 1.7132636308670044 Time taken: 0.29351162910461426\n",
            "Batch Number: 2036 Loss: 1.7355036735534668 Time taken: 0.31556272506713867\n",
            "Batch Number: 2037 Loss: 1.7261029481887817 Time taken: 0.3296189308166504\n",
            "Batch Number: 2038 Loss: 1.7103173732757568 Time taken: 0.30278825759887695\n",
            "Batch Number: 2039 Loss: 1.7254414558410645 Time taken: 0.3192298412322998\n",
            "Batch Number: 2040 Loss: 1.7045576572418213 Time taken: 0.32872986793518066\n",
            "Batch Number: 2041 Loss: 1.6881611347198486 Time taken: 0.3000755310058594\n",
            "Batch Number: 2042 Loss: 1.6973834037780762 Time taken: 0.30503177642822266\n",
            "Batch Number: 2043 Loss: 1.7060723304748535 Time taken: 0.37338829040527344\n",
            "Batch Number: 2044 Loss: 1.7209738492965698 Time taken: 0.2948455810546875\n",
            "Batch Number: 2045 Loss: 1.7079092264175415 Time taken: 0.31691479682922363\n",
            "Batch Number: 2046 Loss: 1.7219383716583252 Time taken: 0.3071451187133789\n",
            "Batch Number: 2047 Loss: 1.740646481513977 Time taken: 0.28456807136535645\n",
            "Batch Number: 2048 Loss: 1.7370685338974 Time taken: 0.29460978507995605\n",
            "Batch Number: 2049 Loss: 1.7039467096328735 Time taken: 0.32405948638916016\n",
            "Batch Number: 2050 Loss: 1.7139543294906616 Time taken: 0.30367040634155273\n",
            "Batch Number: 2051 Loss: 1.7506932020187378 Time taken: 0.29949212074279785\n",
            "Batch Number: 2052 Loss: 1.7501088380813599 Time taken: 0.33612966537475586\n",
            "Batch Number: 2053 Loss: 1.7876248359680176 Time taken: 0.30073070526123047\n",
            "Batch Number: 2054 Loss: 1.737484097480774 Time taken: 0.2979702949523926\n",
            "Batch Number: 2055 Loss: 1.7796581983566284 Time taken: 0.3143494129180908\n",
            "Batch Number: 2056 Loss: 1.7272529602050781 Time taken: 0.308971643447876\n",
            "Batch Number: 2057 Loss: 1.7229582071304321 Time taken: 0.30206775665283203\n",
            "Batch Number: 2058 Loss: 1.7392635345458984 Time taken: 0.304760217666626\n",
            "Batch Number: 2059 Loss: 1.7634668350219727 Time taken: 0.3147144317626953\n",
            "Batch Number: 2060 Loss: 1.7386462688446045 Time taken: 0.29663729667663574\n",
            "Batch Number: 2061 Loss: 1.7604963779449463 Time taken: 0.29776644706726074\n",
            "Batch Number: 2062 Loss: 1.722726821899414 Time taken: 0.32128024101257324\n",
            "Batch Number: 2063 Loss: 1.7008010149002075 Time taken: 0.30332350730895996\n",
            "Batch Number: 2064 Loss: 1.720778465270996 Time taken: 0.30922842025756836\n",
            "Batch Number: 2065 Loss: 1.7102746963500977 Time taken: 0.32843565940856934\n",
            "Batch Number: 2066 Loss: 1.6985242366790771 Time taken: 0.311232328414917\n",
            "Batch Number: 2067 Loss: 1.66379976272583 Time taken: 0.3039424419403076\n",
            "Batch Number: 2068 Loss: 1.699546456336975 Time taken: 0.30776333808898926\n",
            "Batch Number: 2069 Loss: 1.7038521766662598 Time taken: 0.3188743591308594\n",
            "Batch Number: 2070 Loss: 1.71347975730896 Time taken: 0.30024266242980957\n",
            "Batch Number: 2071 Loss: 1.701802372932434 Time taken: 0.32270359992980957\n",
            "Batch Number: 2072 Loss: 1.7751904726028442 Time taken: 0.3019068241119385\n",
            "Batch Number: 2073 Loss: 1.6945757865905762 Time taken: 0.2934892177581787\n",
            "Batch Number: 2074 Loss: 1.7443912029266357 Time taken: 0.3120765686035156\n",
            "Batch Number: 2075 Loss: 1.7534676790237427 Time taken: 0.320481538772583\n",
            "Batch Number: 2076 Loss: 1.7559821605682373 Time taken: 0.31179022789001465\n",
            "Batch Number: 2077 Loss: 1.727216362953186 Time taken: 0.31148791313171387\n",
            "Batch Number: 2078 Loss: 1.7566817998886108 Time taken: 0.31523776054382324\n",
            "Batch Number: 2079 Loss: 1.7283662557601929 Time taken: 0.3145935535430908\n",
            "Batch Number: 2080 Loss: 1.7535052299499512 Time taken: 0.30663466453552246\n",
            "Batch Number: 2081 Loss: 1.737122654914856 Time taken: 0.31671881675720215\n",
            "Batch Number: 2082 Loss: 1.7296173572540283 Time taken: 0.3054804801940918\n",
            "Batch Number: 2083 Loss: 1.7519028186798096 Time taken: 0.32372426986694336\n",
            "Batch Number: 2084 Loss: 1.7324581146240234 Time taken: 0.3157918453216553\n",
            "Batch Number: 2085 Loss: 1.755643606185913 Time taken: 0.30379271507263184\n",
            "Batch Number: 2086 Loss: 1.7671372890472412 Time taken: 0.30245447158813477\n",
            "Batch Number: 2087 Loss: 1.7385237216949463 Time taken: 0.3041956424713135\n",
            "Batch Number: 2088 Loss: 1.7537974119186401 Time taken: 0.30410099029541016\n",
            "Batch Number: 2089 Loss: 1.7722809314727783 Time taken: 0.3081984519958496\n",
            "Batch Number: 2090 Loss: 1.7669512033462524 Time taken: 0.35921406745910645\n",
            "Batch Number: 2091 Loss: 1.749653697013855 Time taken: 0.3949899673461914\n",
            "Batch Number: 2092 Loss: 1.7785500288009644 Time taken: 0.3429605960845947\n",
            "Batch Number: 2093 Loss: 1.7620669603347778 Time taken: 0.2888507843017578\n",
            "Batch Number: 2094 Loss: 1.762932300567627 Time taken: 0.30488038063049316\n",
            "Batch Number: 2095 Loss: 1.7370010614395142 Time taken: 0.2944178581237793\n",
            "Batch Number: 2096 Loss: 1.7592332363128662 Time taken: 0.379453182220459\n",
            "Batch Number: 2097 Loss: 1.7182049751281738 Time taken: 0.3687276840209961\n",
            "Batch Number: 2098 Loss: 1.7368992567062378 Time taken: 0.34388184547424316\n",
            "Batch Number: 2099 Loss: 1.7237805128097534 Time taken: 0.30196595191955566\n",
            "Batch Number: 2100 Loss: 1.7415471076965332 Time taken: 0.3029632568359375\n",
            "Batch Number: 2101 Loss: 1.7312569618225098 Time taken: 0.30185699462890625\n",
            "Batch Number: 2102 Loss: 1.7196228504180908 Time taken: 0.35705137252807617\n",
            "Batch Number: 2103 Loss: 1.7495486736297607 Time taken: 0.3893730640411377\n",
            "Batch Number: 2104 Loss: 1.7126753330230713 Time taken: 0.35303401947021484\n",
            "Batch Number: 2105 Loss: 1.7242416143417358 Time taken: 0.29238295555114746\n",
            "Batch Number: 2106 Loss: 1.7350802421569824 Time taken: 0.28911924362182617\n",
            "Batch Number: 2107 Loss: 1.742661714553833 Time taken: 0.30424928665161133\n",
            "Batch Number: 2108 Loss: 1.7236676216125488 Time taken: 0.31882333755493164\n",
            "Batch Number: 2109 Loss: 1.711943507194519 Time taken: 0.2903401851654053\n",
            "Batch Number: 2110 Loss: 1.7115980386734009 Time taken: 0.3056223392486572\n",
            "Batch Number: 2111 Loss: 1.7550363540649414 Time taken: 0.2909812927246094\n",
            "Batch Number: 2112 Loss: 1.7319053411483765 Time taken: 0.29735326766967773\n",
            "Batch Number: 2113 Loss: 1.7292066812515259 Time taken: 0.30039238929748535\n",
            "Batch Number: 2114 Loss: 1.7216503620147705 Time taken: 0.311962366104126\n",
            "Batch Number: 2115 Loss: 1.7364556789398193 Time taken: 0.28758955001831055\n",
            "Batch Number: 2116 Loss: 1.6982109546661377 Time taken: 0.3623635768890381\n",
            "Batch Number: 2117 Loss: 1.675840139389038 Time taken: 0.3153567314147949\n",
            "Batch Number: 2118 Loss: 1.7015186548233032 Time taken: 0.30121469497680664\n",
            "Batch Number: 2119 Loss: 1.7311310768127441 Time taken: 0.28913044929504395\n",
            "Batch Number: 2120 Loss: 1.7160815000534058 Time taken: 0.32395148277282715\n",
            "Batch Number: 2121 Loss: 1.7317149639129639 Time taken: 0.2826991081237793\n",
            "Batch Number: 2122 Loss: 1.724619746208191 Time taken: 0.2919154167175293\n",
            "Batch Number: 2123 Loss: 1.7746411561965942 Time taken: 0.294619083404541\n",
            "Batch Number: 2124 Loss: 1.7193667888641357 Time taken: 0.2878994941711426\n",
            "Batch Number: 2125 Loss: 1.746748924255371 Time taken: 0.28947877883911133\n",
            "Batch Number: 2126 Loss: 1.7255148887634277 Time taken: 0.29534411430358887\n",
            "Batch Number: 2127 Loss: 1.7140145301818848 Time taken: 0.2922499179840088\n",
            "Batch Number: 2128 Loss: 1.725440502166748 Time taken: 0.3033618927001953\n",
            "Batch Number: 2129 Loss: 1.7385210990905762 Time taken: 0.31790995597839355\n",
            "Batch Number: 2130 Loss: 1.721378207206726 Time taken: 0.32064032554626465\n",
            "Batch Number: 2131 Loss: 1.7555912733078003 Time taken: 0.30138373374938965\n",
            "Batch Number: 2132 Loss: 1.7426722049713135 Time taken: 0.3048114776611328\n",
            "Batch Number: 2133 Loss: 1.734933853149414 Time taken: 0.2995898723602295\n",
            "Batch Number: 2134 Loss: 1.7461929321289062 Time taken: 0.3041963577270508\n",
            "Batch Number: 2135 Loss: 1.7510251998901367 Time taken: 0.3134143352508545\n",
            "Batch Number: 2136 Loss: 1.7448092699050903 Time taken: 0.33010339736938477\n",
            "Batch Number: 2137 Loss: 1.7423292398452759 Time taken: 0.2966346740722656\n",
            "Batch Number: 2138 Loss: 1.7695616483688354 Time taken: 0.3139016628265381\n",
            "Batch Number: 2139 Loss: 1.7231693267822266 Time taken: 0.30544185638427734\n",
            "Batch Number: 2140 Loss: 1.71927809715271 Time taken: 0.29375767707824707\n",
            "Batch Number: 2141 Loss: 1.7684520483016968 Time taken: 0.2829885482788086\n",
            "Batch Number: 2142 Loss: 1.706708312034607 Time taken: 0.31583189964294434\n",
            "Batch Number: 2143 Loss: 1.7416398525238037 Time taken: 0.3252081871032715\n",
            "Batch Number: 2144 Loss: 1.7294505834579468 Time taken: 0.2910175323486328\n",
            "Batch Number: 2145 Loss: 1.7182955741882324 Time taken: 0.29848241806030273\n",
            "Batch Number: 2146 Loss: 1.742952585220337 Time taken: 0.32622241973876953\n",
            "Batch Number: 2147 Loss: 1.71058189868927 Time taken: 0.2954411506652832\n",
            "Batch Number: 2148 Loss: 1.7076282501220703 Time taken: 0.30636048316955566\n",
            "Batch Number: 2149 Loss: 1.7296315431594849 Time taken: 0.30687379837036133\n",
            "Batch Number: 2150 Loss: 1.7447097301483154 Time taken: 0.27741527557373047\n",
            "Batch Number: 2151 Loss: 1.7023476362228394 Time taken: 0.29871153831481934\n",
            "Batch Number: 2152 Loss: 1.7001967430114746 Time taken: 0.29163646697998047\n",
            "Batch Number: 2153 Loss: 1.7095528841018677 Time taken: 0.34761738777160645\n",
            "Batch Number: 2154 Loss: 1.7058830261230469 Time taken: 0.3680915832519531\n",
            "Batch Number: 2155 Loss: 1.7151167392730713 Time taken: 0.37102198600769043\n",
            "Batch Number: 2156 Loss: 1.7107495069503784 Time taken: 0.29222846031188965\n",
            "Batch Number: 2157 Loss: 1.72137451171875 Time taken: 0.27904701232910156\n",
            "Batch Number: 2158 Loss: 1.685983657836914 Time taken: 0.2975029945373535\n",
            "Batch Number: 2159 Loss: 1.720124363899231 Time taken: 0.31630468368530273\n",
            "Batch Number: 2160 Loss: 1.6912943124771118 Time taken: 0.2933533191680908\n",
            "Batch Number: 2161 Loss: 1.6960872411727905 Time taken: 0.31523966789245605\n",
            "Batch Number: 2162 Loss: 1.6909719705581665 Time taken: 0.3220558166503906\n",
            "Batch Number: 2163 Loss: 1.6665908098220825 Time taken: 0.29675912857055664\n",
            "Batch Number: 2164 Loss: 1.6717969179153442 Time taken: 0.3172895908355713\n",
            "Batch Number: 2165 Loss: 1.6763509511947632 Time taken: 0.3268134593963623\n",
            "Batch Number: 2166 Loss: 1.6885156631469727 Time taken: 0.2989213466644287\n",
            "Batch Number: 2167 Loss: 1.6635371446609497 Time taken: 0.3073747158050537\n",
            "Batch Number: 2168 Loss: 1.6626428365707397 Time taken: 0.29936981201171875\n",
            "Batch Number: 2169 Loss: 1.692626953125 Time taken: 0.3013429641723633\n",
            "Batch Number: 2170 Loss: 1.6798646450042725 Time taken: 0.30436062812805176\n",
            "Batch Number: 2171 Loss: 1.6906664371490479 Time taken: 0.305448055267334\n",
            "Batch Number: 2172 Loss: 1.6901051998138428 Time taken: 0.32290196418762207\n",
            "Batch Number: 2173 Loss: 1.6798367500305176 Time taken: 0.3024940490722656\n",
            "Batch Number: 2174 Loss: 1.7026665210723877 Time taken: 0.3127272129058838\n",
            "Batch Number: 2175 Loss: 1.727118968963623 Time taken: 0.30634498596191406\n",
            "Batch Number: 2176 Loss: 1.7017794847488403 Time taken: 0.30851197242736816\n",
            "Batch Number: 2177 Loss: 1.7038203477859497 Time taken: 0.2927100658416748\n",
            "Batch Number: 2178 Loss: 1.7104429006576538 Time taken: 0.3052339553833008\n",
            "Batch Number: 2179 Loss: 1.6844403743743896 Time taken: 0.3710806369781494\n",
            "Batch Number: 2180 Loss: 1.6831711530685425 Time taken: 0.37134671211242676\n",
            "Batch Number: 2181 Loss: 1.6924266815185547 Time taken: 0.31232547760009766\n",
            "Batch Number: 2182 Loss: 1.716995120048523 Time taken: 0.3499159812927246\n",
            "Batch Number: 2183 Loss: 1.6955658197402954 Time taken: 0.3743000030517578\n",
            "Batch Number: 2184 Loss: 1.6951984167099 Time taken: 0.3785982131958008\n",
            "Batch Number: 2185 Loss: 1.7022850513458252 Time taken: 0.32211828231811523\n",
            "Batch Number: 2186 Loss: 1.6859163045883179 Time taken: 0.3108634948730469\n",
            "Batch Number: 2187 Loss: 1.7048285007476807 Time taken: 0.3045825958251953\n",
            "Batch Number: 2188 Loss: 1.6802748441696167 Time taken: 0.3457510471343994\n",
            "Batch Number: 2189 Loss: 1.6809204816818237 Time taken: 0.368910551071167\n",
            "Batch Number: 2190 Loss: 1.6699782609939575 Time taken: 0.29627299308776855\n",
            "Batch Number: 2191 Loss: 1.6696847677230835 Time taken: 0.28735780715942383\n",
            "Batch Number: 2192 Loss: 1.691535234451294 Time taken: 0.30561256408691406\n",
            "Batch Number: 2193 Loss: 1.7200343608856201 Time taken: 0.3017418384552002\n",
            "Batch Number: 2194 Loss: 1.6864001750946045 Time taken: 0.2942938804626465\n",
            "Batch Number: 2195 Loss: 1.695913553237915 Time taken: 0.29897212982177734\n",
            "Batch Number: 2196 Loss: 1.6832102537155151 Time taken: 0.32794189453125\n",
            "Batch Number: 2197 Loss: 1.6763330698013306 Time taken: 0.3026154041290283\n",
            "Batch Number: 2198 Loss: 1.7269245386123657 Time taken: 0.30565476417541504\n",
            "Batch Number: 2199 Loss: 1.6995842456817627 Time taken: 0.310819149017334\n",
            "Batch Number: 2200 Loss: 1.7011322975158691 Time taken: 0.30248141288757324\n",
            "Batch Number: 2201 Loss: 1.7318861484527588 Time taken: 0.29921746253967285\n",
            "Batch Number: 2202 Loss: 1.722804069519043 Time taken: 0.3087446689605713\n",
            "Batch Number: 2203 Loss: 1.711355447769165 Time taken: 0.3148365020751953\n",
            "Batch Number: 2204 Loss: 1.7015620470046997 Time taken: 0.3006572723388672\n",
            "Batch Number: 2205 Loss: 1.7087968587875366 Time taken: 0.3203754425048828\n",
            "Batch Number: 2206 Loss: 1.699657678604126 Time taken: 0.2885768413543701\n",
            "Batch Number: 2207 Loss: 1.7188501358032227 Time taken: 0.3599400520324707\n",
            "Batch Number: 2208 Loss: 1.6892048120498657 Time taken: 0.3489205837249756\n",
            "Batch Number: 2209 Loss: 1.683867335319519 Time taken: 0.3080747127532959\n",
            "Batch Number: 2210 Loss: 1.690891146659851 Time taken: 0.30052900314331055\n",
            "Batch Number: 2211 Loss: 1.7041935920715332 Time taken: 0.30838608741760254\n",
            "Batch Number: 2212 Loss: 1.6900863647460938 Time taken: 0.30666160583496094\n",
            "Batch Number: 2213 Loss: 1.6865503787994385 Time taken: 0.3151884078979492\n",
            "Batch Number: 2214 Loss: 1.7231130599975586 Time taken: 0.32523226737976074\n",
            "Batch Number: 2215 Loss: 1.6836591958999634 Time taken: 0.3261144161224365\n",
            "Batch Number: 2216 Loss: 1.6833299398422241 Time taken: 0.31180429458618164\n",
            "Batch Number: 2217 Loss: 1.6948133707046509 Time taken: 0.3026702404022217\n",
            "Batch Number: 2218 Loss: 1.6699262857437134 Time taken: 0.3365814685821533\n",
            "Batch Number: 2219 Loss: 1.7151858806610107 Time taken: 0.37847399711608887\n",
            "Batch Number: 2220 Loss: 1.6796671152114868 Time taken: 0.3628041744232178\n",
            "Batch Number: 2221 Loss: 1.6804070472717285 Time taken: 0.3101468086242676\n",
            "Batch Number: 2222 Loss: 1.6746482849121094 Time taken: 0.30387401580810547\n",
            "Batch Number: 2223 Loss: 1.676218032836914 Time taken: 0.3504819869995117\n",
            "Batch Number: 2224 Loss: 1.6793047189712524 Time taken: 0.3395533561706543\n",
            "Batch Number: 2225 Loss: 1.6868600845336914 Time taken: 0.370603084564209\n",
            "Batch Number: 2226 Loss: 1.686518669128418 Time taken: 0.37799668312072754\n",
            "Batch Number: 2227 Loss: 1.6863586902618408 Time taken: 0.30841660499572754\n",
            "Batch Number: 2228 Loss: 1.6923669576644897 Time taken: 0.37165164947509766\n",
            "Batch Number: 2229 Loss: 1.6838459968566895 Time taken: 0.3341982364654541\n",
            "Batch Number: 2230 Loss: 1.7073551416397095 Time taken: 0.29496026039123535\n",
            "Batch Number: 2231 Loss: 1.6853662729263306 Time taken: 0.3076803684234619\n",
            "Batch Number: 2232 Loss: 1.7061678171157837 Time taken: 0.3480041027069092\n",
            "Batch Number: 2233 Loss: 1.7047977447509766 Time taken: 0.3010435104370117\n",
            "Batch Number: 2234 Loss: 1.692221760749817 Time taken: 0.33675432205200195\n",
            "Batch Number: 2235 Loss: 1.7122292518615723 Time taken: 0.3232402801513672\n",
            "Batch Number: 2236 Loss: 1.733594536781311 Time taken: 0.3023109436035156\n",
            "Batch Number: 2237 Loss: 1.7599732875823975 Time taken: 0.30182385444641113\n",
            "Batch Number: 2238 Loss: 1.7105271816253662 Time taken: 0.32115769386291504\n",
            "Batch Number: 2239 Loss: 1.720826268196106 Time taken: 0.3670840263366699\n",
            "Batch Number: 2240 Loss: 1.735300898551941 Time taken: 0.40253567695617676\n",
            "Batch Number: 2241 Loss: 1.7199780941009521 Time taken: 0.3124701976776123\n",
            "Batch Number: 2242 Loss: 1.7050411701202393 Time taken: 0.29990100860595703\n",
            "Batch Number: 2243 Loss: 1.7208847999572754 Time taken: 0.31981611251831055\n",
            "Batch Number: 2244 Loss: 1.7156614065170288 Time taken: 0.34156370162963867\n",
            "Batch Number: 2245 Loss: 1.706796646118164 Time taken: 0.30806899070739746\n",
            "Batch Number: 2246 Loss: 1.7060105800628662 Time taken: 0.29336977005004883\n",
            "Batch Number: 2247 Loss: 1.6740515232086182 Time taken: 0.29436206817626953\n",
            "Batch Number: 2248 Loss: 1.709041714668274 Time taken: 0.2848527431488037\n",
            "Batch Number: 2249 Loss: 1.7085875272750854 Time taken: 0.2892923355102539\n",
            "Batch Number: 2250 Loss: 1.6742838621139526 Time taken: 0.34223246574401855\n",
            "Batch Number: 2251 Loss: 1.6936265230178833 Time taken: 0.3252079486846924\n",
            "Batch Number: 2252 Loss: 1.701703429222107 Time taken: 0.28394389152526855\n",
            "Batch Number: 2253 Loss: 1.6988403797149658 Time taken: 0.31412506103515625\n",
            "Batch Number: 2254 Loss: 1.7101856470108032 Time taken: 0.2939112186431885\n",
            "Batch Number: 2255 Loss: 1.691941261291504 Time taken: 0.30038905143737793\n",
            "Batch Number: 2256 Loss: 1.7377774715423584 Time taken: 0.30228161811828613\n",
            "Batch Number: 2257 Loss: 1.7390254735946655 Time taken: 0.32856059074401855\n",
            "Batch Number: 2258 Loss: 1.7460660934448242 Time taken: 0.2931041717529297\n",
            "Batch Number: 2259 Loss: 1.730962872505188 Time taken: 0.2986330986022949\n",
            "Batch Number: 2260 Loss: 1.7309212684631348 Time taken: 0.28826045989990234\n",
            "Batch Number: 2261 Loss: 1.7276636362075806 Time taken: 0.37019824981689453\n",
            "Batch Number: 2262 Loss: 1.7192243337631226 Time taken: 0.3873591423034668\n",
            "Batch Number: 2263 Loss: 1.6917879581451416 Time taken: 0.3774886131286621\n",
            "Batch Number: 2264 Loss: 1.727286458015442 Time taken: 0.36215710639953613\n",
            "Batch Number: 2265 Loss: 1.7211856842041016 Time taken: 0.36315155029296875\n",
            "Batch Number: 2266 Loss: 1.7233668565750122 Time taken: 0.29962611198425293\n",
            "Batch Number: 2267 Loss: 1.7331777811050415 Time taken: 0.28290891647338867\n",
            "Batch Number: 2268 Loss: 1.7164814472198486 Time taken: 0.295041561126709\n",
            "Batch Number: 2269 Loss: 1.7232279777526855 Time taken: 0.2993497848510742\n",
            "Batch Number: 2270 Loss: 1.7276921272277832 Time taken: 0.3022632598876953\n",
            "Batch Number: 2271 Loss: 1.745064377784729 Time taken: 0.28778815269470215\n",
            "Batch Number: 2272 Loss: 1.734340786933899 Time taken: 0.3166344165802002\n",
            "Batch Number: 2273 Loss: 1.7474218606948853 Time taken: 0.3438577651977539\n",
            "Batch Number: 2274 Loss: 1.7220168113708496 Time taken: 0.364532470703125\n",
            "Batch Number: 2275 Loss: 1.7186739444732666 Time taken: 0.32395052909851074\n",
            "Batch Number: 2276 Loss: 1.7135677337646484 Time taken: 0.2983560562133789\n",
            "Batch Number: 2277 Loss: 1.6879501342773438 Time taken: 0.3003880977630615\n",
            "Batch Number: 2278 Loss: 1.7331119775772095 Time taken: 0.3182029724121094\n",
            "Batch Number: 2279 Loss: 1.684200406074524 Time taken: 0.2967085838317871\n",
            "Batch Number: 2280 Loss: 1.7122092247009277 Time taken: 0.2838256359100342\n",
            "Batch Number: 2281 Loss: 1.7107555866241455 Time taken: 0.29334068298339844\n",
            "Batch Number: 2282 Loss: 1.7074837684631348 Time taken: 0.3047366142272949\n",
            "Batch Number: 2283 Loss: 1.67136812210083 Time taken: 0.2838470935821533\n",
            "Batch Number: 2284 Loss: 1.6754839420318604 Time taken: 0.2934441566467285\n",
            "Batch Number: 2285 Loss: 1.7069309949874878 Time taken: 0.3262200355529785\n",
            "Batch Number: 2286 Loss: 1.6883372068405151 Time taken: 0.2965688705444336\n",
            "Batch Number: 2287 Loss: 1.6965152025222778 Time taken: 0.2845325469970703\n",
            "Batch Number: 2288 Loss: 1.698870301246643 Time taken: 0.3201460838317871\n",
            "Batch Number: 2289 Loss: 1.7139427661895752 Time taken: 0.3043363094329834\n",
            "Batch Number: 2290 Loss: 1.6987953186035156 Time taken: 0.29320859909057617\n",
            "Batch Number: 2291 Loss: 1.7035874128341675 Time taken: 0.323117733001709\n",
            "Batch Number: 2292 Loss: 1.711328148841858 Time taken: 0.3012678623199463\n",
            "Batch Number: 2293 Loss: 1.695899486541748 Time taken: 0.28929567337036133\n",
            "Batch Number: 2294 Loss: 1.699938178062439 Time taken: 0.31407594680786133\n",
            "Batch Number: 2295 Loss: 1.6909006834030151 Time taken: 0.32357072830200195\n",
            "Batch Number: 2296 Loss: 1.6666131019592285 Time taken: 0.2946889400482178\n",
            "Batch Number: 2297 Loss: 1.6622968912124634 Time taken: 0.30187463760375977\n",
            "Batch Number: 2298 Loss: 1.651816487312317 Time taken: 0.3489236831665039\n",
            "Batch Number: 2299 Loss: 1.6646628379821777 Time taken: 0.37993526458740234\n",
            "Batch Number: 2300 Loss: 1.7251160144805908 Time taken: 0.3809363842010498\n",
            "Batch Number: 2301 Loss: 1.6943082809448242 Time taken: 0.31383609771728516\n",
            "Batch Number: 2302 Loss: 1.7226542234420776 Time taken: 0.29933714866638184\n",
            "Batch Number: 2303 Loss: 1.7275404930114746 Time taken: 0.2928321361541748\n",
            "Batch Number: 2304 Loss: 1.682073950767517 Time taken: 0.30167150497436523\n",
            "Batch Number: 2305 Loss: 1.7016351222991943 Time taken: 0.313507080078125\n",
            "Batch Number: 2306 Loss: 1.7312698364257812 Time taken: 0.2935352325439453\n",
            "Batch Number: 2307 Loss: 1.7121500968933105 Time taken: 0.3366231918334961\n",
            "Batch Number: 2308 Loss: 1.653894305229187 Time taken: 0.29988837242126465\n",
            "Batch Number: 2309 Loss: 1.6970797777175903 Time taken: 0.30083584785461426\n",
            "Batch Number: 2310 Loss: 1.6789005994796753 Time taken: 0.30202555656433105\n",
            "Batch Number: 2311 Loss: 1.673421859741211 Time taken: 0.3030407428741455\n",
            "Batch Number: 2312 Loss: 1.663830280303955 Time taken: 0.2918705940246582\n",
            "Batch Number: 2313 Loss: 1.6892048120498657 Time taken: 0.34279417991638184\n",
            "Batch Number: 2314 Loss: 1.6960233449935913 Time taken: 0.29088473320007324\n",
            "Batch Number: 2315 Loss: 1.6998639106750488 Time taken: 0.31040310859680176\n",
            "Batch Number: 2316 Loss: 1.728971242904663 Time taken: 0.2917141914367676\n",
            "Batch Number: 2317 Loss: 1.6896576881408691 Time taken: 0.30246472358703613\n",
            "Batch Number: 2318 Loss: 1.6806164979934692 Time taken: 0.2980034351348877\n",
            "Batch Number: 2319 Loss: 1.6901658773422241 Time taken: 0.29479217529296875\n",
            "Batch Number: 2320 Loss: 1.6979528665542603 Time taken: 0.3029351234436035\n",
            "Batch Number: 2321 Loss: 1.7079837322235107 Time taken: 0.3043491840362549\n",
            "Batch Number: 2322 Loss: 1.6924372911453247 Time taken: 0.2929050922393799\n",
            "Batch Number: 2323 Loss: 1.7064489126205444 Time taken: 0.2998669147491455\n",
            "Batch Number: 2324 Loss: 1.6999127864837646 Time taken: 0.37359118461608887\n",
            "Batch Number: 2325 Loss: 1.6933989524841309 Time taken: 0.37131404876708984\n",
            "Batch Number: 2326 Loss: 1.6932072639465332 Time taken: 0.37846899032592773\n",
            "Batch Number: 2327 Loss: 1.6707489490509033 Time taken: 0.3700838088989258\n",
            "Batch Number: 2328 Loss: 1.6838173866271973 Time taken: 0.3365323543548584\n",
            "Batch Number: 2329 Loss: 1.6895439624786377 Time taken: 0.30106091499328613\n",
            "Batch Number: 2330 Loss: 1.683759093284607 Time taken: 0.28856754302978516\n",
            "Batch Number: 2331 Loss: 1.6704210042953491 Time taken: 0.314725399017334\n",
            "Batch Number: 2332 Loss: 1.6752831935882568 Time taken: 0.3800487518310547\n",
            "Batch Number: 2333 Loss: 1.6967488527297974 Time taken: 0.39057469367980957\n",
            "Batch Number: 2334 Loss: 1.67657470703125 Time taken: 0.33005332946777344\n",
            "Batch Number: 2335 Loss: 1.6512702703475952 Time taken: 0.3017096519470215\n",
            "Batch Number: 2336 Loss: 1.6584638357162476 Time taken: 0.301347017288208\n",
            "Batch Number: 2337 Loss: 1.6763646602630615 Time taken: 0.29003024101257324\n",
            "Batch Number: 2338 Loss: 1.7031612396240234 Time taken: 0.3918740749359131\n",
            "Batch Number: 2339 Loss: 1.6596496105194092 Time taken: 0.36316704750061035\n",
            "Batch Number: 2340 Loss: 1.6705610752105713 Time taken: 0.3363492488861084\n",
            "Batch Number: 2341 Loss: 1.6617683172225952 Time taken: 0.3159928321838379\n",
            "Batch Number: 2342 Loss: 1.663684606552124 Time taken: 0.29320740699768066\n",
            "Batch Number: 2343 Loss: 1.6435751914978027 Time taken: 0.3177456855773926\n",
            "Batch Number: 2344 Loss: 1.6465070247650146 Time taken: 0.30275797843933105\n",
            "Batch Number: 2345 Loss: 1.647452473640442 Time taken: 0.3002350330352783\n",
            "Batch Number: 2346 Loss: 1.638136386871338 Time taken: 0.2911550998687744\n",
            "Batch Number: 2347 Loss: 1.6548103094100952 Time taken: 0.29968786239624023\n",
            "Batch Number: 2348 Loss: 1.685555100440979 Time taken: 0.306840181350708\n",
            "Batch Number: 2349 Loss: 1.6497809886932373 Time taken: 0.30003905296325684\n",
            "Batch Number: 2350 Loss: 1.6308667659759521 Time taken: 0.30898499488830566\n",
            "Batch Number: 2351 Loss: 1.6742757558822632 Time taken: 0.2984950542449951\n",
            "Batch Number: 2352 Loss: 1.6813390254974365 Time taken: 0.2976090908050537\n",
            "Batch Number: 2353 Loss: 1.7013441324234009 Time taken: 0.3138558864593506\n",
            "Batch Number: 2354 Loss: 1.6600172519683838 Time taken: 0.30089831352233887\n",
            "Batch Number: 2355 Loss: 1.6707231998443604 Time taken: 0.3220388889312744\n",
            "Batch Number: 2356 Loss: 1.6707683801651 Time taken: 0.3562436103820801\n",
            "Batch Number: 2357 Loss: 1.6690748929977417 Time taken: 0.29651331901550293\n",
            "Batch Number: 2358 Loss: 1.6564725637435913 Time taken: 0.29787492752075195\n",
            "Batch Number: 2359 Loss: 1.6441209316253662 Time taken: 0.2936520576477051\n",
            "Batch Number: 2360 Loss: 1.6726100444793701 Time taken: 0.30119824409484863\n",
            "Batch Number: 2361 Loss: 1.6426316499710083 Time taken: 0.2972254753112793\n",
            "Batch Number: 2362 Loss: 1.6555736064910889 Time taken: 0.2951810359954834\n",
            "Batch Number: 2363 Loss: 1.6399788856506348 Time taken: 0.3386201858520508\n",
            "Batch Number: 2364 Loss: 1.646018147468567 Time taken: 0.3865346908569336\n",
            "Batch Number: 2365 Loss: 1.6686252355575562 Time taken: 0.3633577823638916\n",
            "Batch Number: 2366 Loss: 1.704693078994751 Time taken: 0.3430197238922119\n",
            "Batch Number: 2367 Loss: 1.7453737258911133 Time taken: 0.38620924949645996\n",
            "Batch Number: 2368 Loss: 1.7282887697219849 Time taken: 0.35022830963134766\n",
            "Batch Number: 2369 Loss: 1.7480522394180298 Time taken: 0.37668824195861816\n",
            "Batch Number: 2370 Loss: 1.7072283029556274 Time taken: 0.3840172290802002\n",
            "Batch Number: 2371 Loss: 1.6997382640838623 Time taken: 0.29329609870910645\n",
            "Batch Number: 2372 Loss: 1.679452896118164 Time taken: 0.3206048011779785\n",
            "Batch Number: 2373 Loss: 1.6910291910171509 Time taken: 0.38427281379699707\n",
            "Batch Number: 2374 Loss: 1.6966383457183838 Time taken: 0.3084235191345215\n",
            "Batch Number: 2375 Loss: 1.705651044845581 Time taken: 0.2980983257293701\n",
            "Batch Number: 2376 Loss: 1.6944562196731567 Time taken: 0.299227237701416\n",
            "Batch Number: 2377 Loss: 1.7195457220077515 Time taken: 0.2929646968841553\n",
            "Batch Number: 2378 Loss: 1.6895710229873657 Time taken: 0.2842752933502197\n",
            "Batch Number: 2379 Loss: 1.7257943153381348 Time taken: 0.297595739364624\n",
            "Batch Number: 2380 Loss: 1.6933337450027466 Time taken: 0.3470284938812256\n",
            "Batch Number: 2381 Loss: 1.6969748735427856 Time taken: 0.2961399555206299\n",
            "Batch Number: 2382 Loss: 1.7069393396377563 Time taken: 0.31060171127319336\n",
            "Batch Number: 2383 Loss: 1.6804267168045044 Time taken: 0.3237590789794922\n",
            "Batch Number: 2384 Loss: 1.6616827249526978 Time taken: 0.29561543464660645\n",
            "Batch Number: 2385 Loss: 1.6972030401229858 Time taken: 0.3104841709136963\n",
            "Batch Number: 2386 Loss: 1.6698435544967651 Time taken: 0.34305644035339355\n",
            "Batch Number: 2387 Loss: 1.6672372817993164 Time taken: 0.3731534481048584\n",
            "Batch Number: 2388 Loss: 1.667404294013977 Time taken: 0.3938446044921875\n",
            "Batch Number: 2389 Loss: 1.6676398515701294 Time taken: 0.3319664001464844\n",
            "Batch Number: 2390 Loss: 1.6684736013412476 Time taken: 0.3210763931274414\n",
            "Batch Number: 2391 Loss: 1.6582074165344238 Time taken: 0.39548611640930176\n",
            "Batch Number: 2392 Loss: 1.708173394203186 Time taken: 0.33812403678894043\n",
            "Batch Number: 2393 Loss: 1.685346007347107 Time taken: 0.30106687545776367\n",
            "Batch Number: 2394 Loss: 1.6765143871307373 Time taken: 0.3073863983154297\n",
            "Batch Number: 2395 Loss: 1.6669093370437622 Time taken: 0.34111523628234863\n",
            "Batch Number: 2396 Loss: 1.6714624166488647 Time taken: 0.3100104331970215\n",
            "Batch Number: 2397 Loss: 1.6500883102416992 Time taken: 0.32215094566345215\n",
            "Batch Number: 2398 Loss: 1.6530792713165283 Time taken: 0.3398439884185791\n",
            "Batch Number: 2399 Loss: 1.6454297304153442 Time taken: 0.29204535484313965\n",
            "Batch Number: 2400 Loss: 1.6499885320663452 Time taken: 0.3052701950073242\n",
            "Batch Number: 2401 Loss: 1.6412041187286377 Time taken: 0.30928945541381836\n",
            "Batch Number: 2402 Loss: 1.6771363019943237 Time taken: 0.3160679340362549\n",
            "Batch Number: 2403 Loss: 1.6846654415130615 Time taken: 0.29796552658081055\n",
            "Batch Number: 2404 Loss: 1.6811275482177734 Time taken: 0.3166379928588867\n",
            "Batch Number: 2405 Loss: 1.6453336477279663 Time taken: 0.2874124050140381\n",
            "Batch Number: 2406 Loss: 1.6574958562850952 Time taken: 0.2919900417327881\n",
            "Batch Number: 2407 Loss: 1.6728194952011108 Time taken: 0.30377936363220215\n",
            "Batch Number: 2408 Loss: 1.689038634300232 Time taken: 0.30237317085266113\n",
            "Batch Number: 2409 Loss: 1.664010763168335 Time taken: 0.2984578609466553\n",
            "Batch Number: 2410 Loss: 1.6767520904541016 Time taken: 0.29949069023132324\n",
            "Batch Number: 2411 Loss: 1.6716567277908325 Time taken: 0.34677958488464355\n",
            "Batch Number: 2412 Loss: 1.662474513053894 Time taken: 0.36463403701782227\n",
            "Batch Number: 2413 Loss: 1.6830358505249023 Time taken: 0.3725597858428955\n",
            "Batch Number: 2414 Loss: 1.7097055912017822 Time taken: 0.3102376461029053\n",
            "Batch Number: 2415 Loss: 1.682700276374817 Time taken: 0.3253486156463623\n",
            "Batch Number: 2416 Loss: 1.6967120170593262 Time taken: 0.3661491870880127\n",
            "Batch Number: 2417 Loss: 1.6818692684173584 Time taken: 0.3077371120452881\n",
            "Batch Number: 2418 Loss: 1.6596089601516724 Time taken: 0.28759145736694336\n",
            "Batch Number: 2419 Loss: 1.6469981670379639 Time taken: 0.2977464199066162\n",
            "Batch Number: 2420 Loss: 1.6768978834152222 Time taken: 0.29588961601257324\n",
            "Batch Number: 2421 Loss: 1.6583482027053833 Time taken: 0.29503774642944336\n",
            "Batch Number: 2422 Loss: 1.670974850654602 Time taken: 0.293964147567749\n",
            "Batch Number: 2423 Loss: 1.6362464427947998 Time taken: 0.30582404136657715\n",
            "Batch Number: 2424 Loss: 1.642171025276184 Time taken: 0.2967514991760254\n",
            "Batch Number: 2425 Loss: 1.6548631191253662 Time taken: 0.3587021827697754\n",
            "Batch Number: 2426 Loss: 1.6426777839660645 Time taken: 0.33348584175109863\n",
            "Batch Number: 2427 Loss: 1.6501706838607788 Time taken: 0.30277347564697266\n",
            "Batch Number: 2428 Loss: 1.6892979145050049 Time taken: 0.29857826232910156\n",
            "Batch Number: 2429 Loss: 1.6512649059295654 Time taken: 0.3364987373352051\n",
            "Batch Number: 2430 Loss: 1.659805178642273 Time taken: 0.29456067085266113\n",
            "Batch Number: 2431 Loss: 1.6577271223068237 Time taken: 0.2924926280975342\n",
            "Batch Number: 2432 Loss: 1.6818554401397705 Time taken: 0.32466936111450195\n",
            "Batch Number: 2433 Loss: 1.6762831211090088 Time taken: 0.3675975799560547\n",
            "Batch Number: 2434 Loss: 1.675121545791626 Time taken: 0.37825775146484375\n",
            "Batch Number: 2435 Loss: 1.6710433959960938 Time taken: 0.35505223274230957\n",
            "Batch Number: 2436 Loss: 1.6652870178222656 Time taken: 0.2924375534057617\n",
            "Batch Number: 2437 Loss: 1.7090908288955688 Time taken: 0.2909684181213379\n",
            "Batch Number: 2438 Loss: 1.6942753791809082 Time taken: 0.3085482120513916\n",
            "Batch Number: 2439 Loss: 1.6682987213134766 Time taken: 0.30145812034606934\n",
            "Batch Number: 2440 Loss: 1.6751432418823242 Time taken: 0.2924787998199463\n",
            "Batch Number: 2441 Loss: 1.6595814228057861 Time taken: 0.29105567932128906\n",
            "Batch Number: 2442 Loss: 1.6678144931793213 Time taken: 0.3213033676147461\n",
            "Batch Number: 2443 Loss: 1.7232383489608765 Time taken: 0.28815293312072754\n",
            "Batch Number: 2444 Loss: 1.7173516750335693 Time taken: 0.28612565994262695\n",
            "Batch Number: 2445 Loss: 1.6945875883102417 Time taken: 0.3316001892089844\n",
            "Batch Number: 2446 Loss: 1.6976258754730225 Time taken: 0.2939009666442871\n",
            "Batch Number: 2447 Loss: 1.6877799034118652 Time taken: 0.28867626190185547\n",
            "Batch Number: 2448 Loss: 1.6814841032028198 Time taken: 0.302201509475708\n",
            "Batch Number: 2449 Loss: 1.7029070854187012 Time taken: 0.3017241954803467\n",
            "Batch Number: 2450 Loss: 1.7463716268539429 Time taken: 0.290341854095459\n",
            "Batch Number: 2451 Loss: 1.6836869716644287 Time taken: 0.2964901924133301\n",
            "Batch Number: 2452 Loss: 1.7279545068740845 Time taken: 0.3209114074707031\n",
            "Batch Number: 2453 Loss: 1.7172491550445557 Time taken: 0.29433703422546387\n",
            "Batch Number: 2454 Loss: 1.7030863761901855 Time taken: 0.3306283950805664\n",
            "Batch Number: 2455 Loss: 1.7023171186447144 Time taken: 0.31919312477111816\n",
            "Batch Number: 2456 Loss: 1.678633451461792 Time taken: 0.28743505477905273\n",
            "Batch Number: 2457 Loss: 1.6747677326202393 Time taken: 0.28580665588378906\n",
            "Batch Number: 2458 Loss: 1.6861428022384644 Time taken: 0.3057258129119873\n",
            "Batch Number: 2459 Loss: 1.6703988313674927 Time taken: 0.3019440174102783\n",
            "Batch Number: 2460 Loss: 1.6932421922683716 Time taken: 0.2953643798828125\n",
            "Batch Number: 2461 Loss: 1.6719870567321777 Time taken: 0.3633244037628174\n",
            "Batch Number: 2462 Loss: 1.6780415773391724 Time taken: 0.37450671195983887\n",
            "Batch Number: 2463 Loss: 1.675034523010254 Time taken: 0.31503915786743164\n",
            "Batch Number: 2464 Loss: 1.668248176574707 Time taken: 0.3338775634765625\n",
            "Batch Number: 2465 Loss: 1.6897245645523071 Time taken: 0.30840229988098145\n",
            "Batch Number: 2466 Loss: 1.6727491617202759 Time taken: 0.2889378070831299\n",
            "Batch Number: 2467 Loss: 1.6521936655044556 Time taken: 0.30025529861450195\n",
            "Batch Number: 2468 Loss: 1.669648289680481 Time taken: 0.33791494369506836\n",
            "Batch Number: 2469 Loss: 1.6756027936935425 Time taken: 0.32129478454589844\n",
            "Batch Number: 2470 Loss: 1.67560613155365 Time taken: 0.31081271171569824\n",
            "Batch Number: 2471 Loss: 1.6433992385864258 Time taken: 0.312103271484375\n",
            "Batch Number: 2472 Loss: 1.6550993919372559 Time taken: 0.29034900665283203\n",
            "Batch Number: 2473 Loss: 1.6423298120498657 Time taken: 0.3034203052520752\n",
            "Batch Number: 2474 Loss: 1.673891305923462 Time taken: 0.32325053215026855\n",
            "Batch Number: 2475 Loss: 1.693595051765442 Time taken: 0.2946898937225342\n",
            "Batch Number: 2476 Loss: 1.6318583488464355 Time taken: 0.28943347930908203\n",
            "Batch Number: 2477 Loss: 1.6484878063201904 Time taken: 0.30589747428894043\n",
            "Batch Number: 2478 Loss: 1.6551761627197266 Time taken: 0.29978466033935547\n",
            "Batch Number: 2479 Loss: 1.6540019512176514 Time taken: 0.29892778396606445\n",
            "Batch Number: 2480 Loss: 1.6938724517822266 Time taken: 0.29580116271972656\n",
            "Batch Number: 2481 Loss: 1.6849660873413086 Time taken: 0.307567834854126\n",
            "Batch Number: 2482 Loss: 1.678797721862793 Time taken: 0.30034422874450684\n",
            "Batch Number: 2483 Loss: 1.6897188425064087 Time taken: 0.30101871490478516\n",
            "Batch Number: 2484 Loss: 1.6841124296188354 Time taken: 0.3119933605194092\n",
            "Batch Number: 2485 Loss: 1.6629951000213623 Time taken: 0.2867431640625\n",
            "Batch Number: 2486 Loss: 1.674324870109558 Time taken: 0.34098243713378906\n",
            "Batch Number: 2487 Loss: 1.6428565979003906 Time taken: 0.3768148422241211\n",
            "Batch Number: 2488 Loss: 1.6821223497390747 Time taken: 0.3844926357269287\n",
            "Batch Number: 2489 Loss: 1.6451058387756348 Time taken: 0.2907097339630127\n",
            "Batch Number: 2490 Loss: 1.7049068212509155 Time taken: 0.31399011611938477\n",
            "Batch Number: 2491 Loss: 1.7471802234649658 Time taken: 0.32502055168151855\n",
            "Batch Number: 2492 Loss: 1.6882120370864868 Time taken: 0.33796167373657227\n",
            "Batch Number: 2493 Loss: 1.6922193765640259 Time taken: 0.30303096771240234\n",
            "Batch Number: 2494 Loss: 1.6966500282287598 Time taken: 0.2908055782318115\n",
            "Batch Number: 2495 Loss: 1.7100498676300049 Time taken: 0.30503058433532715\n",
            "Batch Number: 2496 Loss: 1.6874655485153198 Time taken: 0.28575873374938965\n",
            "Batch Number: 2497 Loss: 1.6868606805801392 Time taken: 0.30484986305236816\n",
            "Batch Number: 2498 Loss: 1.7154380083084106 Time taken: 0.3287055492401123\n",
            "Batch Number: 2499 Loss: 1.6978318691253662 Time taken: 0.36867260932922363\n",
            "Batch Number: 2500 Loss: 1.6777905225753784 Time taken: 0.3892951011657715\n",
            "Batch Number: 2501 Loss: 1.6888021230697632 Time taken: 0.31890869140625\n",
            "Batch Number: 2502 Loss: 1.6912579536437988 Time taken: 0.29485034942626953\n",
            "Batch Number: 2503 Loss: 1.696875810623169 Time taken: 0.29995083808898926\n",
            "Batch Number: 2504 Loss: 1.6901898384094238 Time taken: 0.3159451484680176\n",
            "Batch Number: 2505 Loss: 1.6774381399154663 Time taken: 0.29163098335266113\n",
            "Batch Number: 2506 Loss: 1.6686408519744873 Time taken: 0.3113443851470947\n",
            "Batch Number: 2507 Loss: 1.6649914979934692 Time taken: 0.30112695693969727\n",
            "Batch Number: 2508 Loss: 1.6746515035629272 Time taken: 0.34159326553344727\n",
            "Batch Number: 2509 Loss: 1.669054627418518 Time taken: 0.3815925121307373\n",
            "Batch Number: 2510 Loss: 1.6689088344573975 Time taken: 0.34977030754089355\n",
            "Batch Number: 2511 Loss: 1.666101336479187 Time taken: 0.2906320095062256\n",
            "Batch Number: 2512 Loss: 1.6924407482147217 Time taken: 0.3093545436859131\n",
            "Batch Number: 2513 Loss: 1.654043436050415 Time taken: 0.2859008312225342\n",
            "Batch Number: 2514 Loss: 1.657517671585083 Time taken: 0.280148983001709\n",
            "Batch Number: 2515 Loss: 1.6496495008468628 Time taken: 0.2873344421386719\n",
            "Batch Number: 2516 Loss: 1.6571061611175537 Time taken: 0.2896406650543213\n",
            "Batch Number: 2517 Loss: 1.6575307846069336 Time taken: 0.28557586669921875\n",
            "Batch Number: 2518 Loss: 1.6633261442184448 Time taken: 0.29969167709350586\n",
            "Batch Number: 2519 Loss: 1.6399061679840088 Time taken: 0.31445980072021484\n",
            "Batch Number: 2520 Loss: 1.626207709312439 Time taken: 0.30095982551574707\n",
            "Batch Number: 2521 Loss: 1.6494556665420532 Time taken: 0.3013417720794678\n",
            "Batch Number: 2522 Loss: 1.6487507820129395 Time taken: 0.30572938919067383\n",
            "Batch Number: 2523 Loss: 1.5999337434768677 Time taken: 0.2920989990234375\n",
            "Batch Number: 2524 Loss: 1.6256097555160522 Time taken: 0.29397106170654297\n",
            "Batch Number: 2525 Loss: 1.6161538362503052 Time taken: 0.30043816566467285\n",
            "Batch Number: 2526 Loss: 1.6231882572174072 Time taken: 0.3027157783508301\n",
            "Batch Number: 2527 Loss: 1.602026104927063 Time taken: 0.29086899757385254\n",
            "Batch Number: 2528 Loss: 1.6385096311569214 Time taken: 0.29677295684814453\n",
            "Batch Number: 2529 Loss: 1.6355894804000854 Time taken: 0.32430481910705566\n",
            "Batch Number: 2530 Loss: 1.6024881601333618 Time taken: 0.3225843906402588\n",
            "Batch Number: 2531 Loss: 1.6389689445495605 Time taken: 0.33144664764404297\n",
            "Batch Number: 2532 Loss: 1.6458542346954346 Time taken: 0.3799257278442383\n",
            "Batch Number: 2533 Loss: 1.6461552381515503 Time taken: 0.31943225860595703\n",
            "Batch Number: 2534 Loss: 1.6279078722000122 Time taken: 0.29879069328308105\n",
            "Batch Number: 2535 Loss: 1.6723294258117676 Time taken: 0.30771803855895996\n",
            "Batch Number: 2536 Loss: 1.6495965719223022 Time taken: 0.321530818939209\n",
            "Batch Number: 2537 Loss: 1.641937255859375 Time taken: 0.37192821502685547\n",
            "Batch Number: 2538 Loss: 1.6283454895019531 Time taken: 0.37752699851989746\n",
            "Batch Number: 2539 Loss: 1.6482409238815308 Time taken: 0.3098132610321045\n",
            "Batch Number: 2540 Loss: 1.6695358753204346 Time taken: 0.37023377418518066\n",
            "Batch Number: 2541 Loss: 1.6345959901809692 Time taken: 0.38635993003845215\n",
            "Batch Number: 2542 Loss: 1.664698600769043 Time taken: 0.30826711654663086\n",
            "Batch Number: 2543 Loss: 1.6547285318374634 Time taken: 0.2875046730041504\n",
            "Batch Number: 2544 Loss: 1.6649951934814453 Time taken: 0.30277252197265625\n",
            "Batch Number: 2545 Loss: 1.623430609703064 Time taken: 0.29967522621154785\n",
            "Batch Number: 2546 Loss: 1.6642723083496094 Time taken: 0.2934391498565674\n",
            "Batch Number: 2547 Loss: 1.6390998363494873 Time taken: 0.30991029739379883\n",
            "Batch Number: 2548 Loss: 1.6519421339035034 Time taken: 0.29746174812316895\n",
            "Batch Number: 2549 Loss: 1.6645983457565308 Time taken: 0.2907745838165283\n",
            "Batch Number: 2550 Loss: 1.6294957399368286 Time taken: 0.29982781410217285\n",
            "Batch Number: 2551 Loss: 1.6518007516860962 Time taken: 0.312746524810791\n",
            "Batch Number: 2552 Loss: 1.6318848133087158 Time taken: 0.38181495666503906\n",
            "Batch Number: 2553 Loss: 1.6561691761016846 Time taken: 0.37979841232299805\n",
            "Batch Number: 2554 Loss: 1.664638638496399 Time taken: 0.36476802825927734\n",
            "Batch Number: 2555 Loss: 1.651037335395813 Time taken: 0.366741418838501\n",
            "Batch Number: 2556 Loss: 1.6525936126708984 Time taken: 0.3693835735321045\n",
            "Batch Number: 2557 Loss: 1.6462478637695312 Time taken: 0.2873349189758301\n",
            "Batch Number: 2558 Loss: 1.6553618907928467 Time taken: 0.2917594909667969\n",
            "Batch Number: 2559 Loss: 1.6765961647033691 Time taken: 0.32166409492492676\n",
            "Batch Number: 2560 Loss: 1.6761494874954224 Time taken: 0.32117247581481934\n",
            "Batch Number: 2561 Loss: 1.6377102136611938 Time taken: 0.2915987968444824\n",
            "Batch Number: 2562 Loss: 1.63712477684021 Time taken: 0.3065986633300781\n",
            "Batch Number: 2563 Loss: 1.6286379098892212 Time taken: 0.33981943130493164\n",
            "Batch Number: 2564 Loss: 1.6427605152130127 Time taken: 0.3021829128265381\n",
            "Batch Number: 2565 Loss: 1.6302516460418701 Time taken: 0.323972225189209\n",
            "Batch Number: 2566 Loss: 1.6460481882095337 Time taken: 0.3621847629547119\n",
            "Batch Number: 2567 Loss: 1.6525722742080688 Time taken: 0.2919182777404785\n",
            "Batch Number: 2568 Loss: 1.6372729539871216 Time taken: 0.29191136360168457\n",
            "Batch Number: 2569 Loss: 1.6739581823349 Time taken: 0.32024049758911133\n",
            "Batch Number: 2570 Loss: 1.6449717283248901 Time taken: 0.29393720626831055\n",
            "Batch Number: 2571 Loss: 1.6577872037887573 Time taken: 0.300189733505249\n",
            "Batch Number: 2572 Loss: 1.6482218503952026 Time taken: 0.30706286430358887\n",
            "Batch Number: 2573 Loss: 1.678611397743225 Time taken: 0.30460071563720703\n",
            "Batch Number: 2574 Loss: 1.661712646484375 Time taken: 0.2995433807373047\n",
            "Batch Number: 2575 Loss: 1.6815389394760132 Time taken: 0.3013153076171875\n",
            "Batch Number: 2576 Loss: 1.6469016075134277 Time taken: 0.3240647315979004\n",
            "Batch Number: 2577 Loss: 1.6705325841903687 Time taken: 0.296372652053833\n",
            "Batch Number: 2578 Loss: 1.6341259479522705 Time taken: 0.3012411594390869\n",
            "Batch Number: 2579 Loss: 1.6699492931365967 Time taken: 0.3226931095123291\n",
            "Batch Number: 2580 Loss: 1.6647104024887085 Time taken: 0.3378152847290039\n",
            "Batch Number: 2581 Loss: 1.6344119310379028 Time taken: 0.3600502014160156\n",
            "Batch Number: 2582 Loss: 1.650708794593811 Time taken: 0.31151247024536133\n",
            "Batch Number: 2583 Loss: 1.6284245252609253 Time taken: 0.29482293128967285\n",
            "Batch Number: 2584 Loss: 1.642770528793335 Time taken: 0.303661584854126\n",
            "Batch Number: 2585 Loss: 1.6503429412841797 Time taken: 0.37493038177490234\n",
            "Batch Number: 2586 Loss: 1.6377596855163574 Time taken: 0.36417150497436523\n",
            "Batch Number: 2587 Loss: 1.635453701019287 Time taken: 0.35907983779907227\n",
            "Batch Number: 2588 Loss: 1.6376112699508667 Time taken: 0.30496692657470703\n",
            "Batch Number: 2589 Loss: 1.6321791410446167 Time taken: 0.29162073135375977\n",
            "Batch Number: 2590 Loss: 1.6696804761886597 Time taken: 0.29868221282958984\n",
            "Batch Number: 2591 Loss: 1.6568511724472046 Time taken: 0.3271596431732178\n",
            "Batch Number: 2592 Loss: 1.6751188039779663 Time taken: 0.29724764823913574\n",
            "Batch Number: 2593 Loss: 1.6742719411849976 Time taken: 0.2935044765472412\n",
            "Batch Number: 2594 Loss: 1.6406917572021484 Time taken: 0.3178129196166992\n",
            "Batch Number: 2595 Loss: 1.6678895950317383 Time taken: 0.2936713695526123\n",
            "Batch Number: 2596 Loss: 1.6385860443115234 Time taken: 0.2919433116912842\n",
            "Batch Number: 2597 Loss: 1.648073434829712 Time taken: 0.30908703804016113\n",
            "Batch Number: 2598 Loss: 1.6495100259780884 Time taken: 0.3215818405151367\n",
            "Batch Number: 2599 Loss: 1.6371787786483765 Time taken: 0.2920420169830322\n",
            "Batch Number: 2600 Loss: 1.6287821531295776 Time taken: 0.294783353805542\n",
            "Batch Number: 2601 Loss: 1.640332818031311 Time taken: 0.30208444595336914\n",
            "Batch Number: 2602 Loss: 1.6295942068099976 Time taken: 0.29838061332702637\n",
            "Batch Number: 2603 Loss: 1.6406031847000122 Time taken: 0.31102705001831055\n",
            "Batch Number: 2604 Loss: 1.6208244562149048 Time taken: 0.3286783695220947\n",
            "Batch Number: 2605 Loss: 1.6173443794250488 Time taken: 0.2885265350341797\n",
            "Batch Number: 2606 Loss: 1.6324880123138428 Time taken: 0.2912559509277344\n",
            "Batch Number: 2607 Loss: 1.6552542448043823 Time taken: 0.317676305770874\n",
            "Batch Number: 2608 Loss: 1.6358767747879028 Time taken: 0.30486249923706055\n",
            "Batch Number: 2609 Loss: 1.644390344619751 Time taken: 0.29061007499694824\n",
            "Batch Number: 2610 Loss: 1.6389013528823853 Time taken: 0.33144164085388184\n",
            "Batch Number: 2611 Loss: 1.6761280298233032 Time taken: 0.3013882637023926\n",
            "Batch Number: 2612 Loss: 1.669206142425537 Time taken: 0.2991001605987549\n",
            "Batch Number: 2613 Loss: 1.6896302700042725 Time taken: 0.30472445487976074\n",
            "Batch Number: 2614 Loss: 1.6432290077209473 Time taken: 0.31008315086364746\n",
            "Batch Number: 2615 Loss: 1.6554371118545532 Time taken: 0.30099058151245117\n",
            "Batch Number: 2616 Loss: 1.6454557180404663 Time taken: 0.32143449783325195\n",
            "Batch Number: 2617 Loss: 1.6994715929031372 Time taken: 0.3610055446624756\n",
            "Batch Number: 2618 Loss: 1.6541424989700317 Time taken: 0.3683912754058838\n",
            "Batch Number: 2619 Loss: 1.6675094366073608 Time taken: 0.30423998832702637\n",
            "Batch Number: 2620 Loss: 1.652410626411438 Time taken: 0.31392598152160645\n",
            "Batch Number: 2621 Loss: 1.6754955053329468 Time taken: 0.29206299781799316\n",
            "Batch Number: 2622 Loss: 1.6620265245437622 Time taken: 0.3022923469543457\n",
            "Batch Number: 2623 Loss: 1.666551947593689 Time taken: 0.31777119636535645\n",
            "Batch Number: 2624 Loss: 1.6559085845947266 Time taken: 0.30632781982421875\n",
            "Batch Number: 2625 Loss: 1.6788415908813477 Time taken: 0.29377126693725586\n",
            "Batch Number: 2626 Loss: 1.678752064704895 Time taken: 0.3034062385559082\n",
            "Batch Number: 2627 Loss: 1.685987949371338 Time taken: 0.30861902236938477\n",
            "Batch Number: 2628 Loss: 1.682344675064087 Time taken: 0.2927074432373047\n",
            "Batch Number: 2629 Loss: 1.6901673078536987 Time taken: 0.3367936611175537\n",
            "Batch Number: 2630 Loss: 1.692147135734558 Time taken: 0.31055355072021484\n",
            "Batch Number: 2631 Loss: 1.6763434410095215 Time taken: 0.29373836517333984\n",
            "Batch Number: 2632 Loss: 1.6735364198684692 Time taken: 0.3396880626678467\n",
            "Batch Number: 2633 Loss: 1.6911975145339966 Time taken: 0.3001677989959717\n",
            "Batch Number: 2634 Loss: 1.6615679264068604 Time taken: 0.2965879440307617\n",
            "Batch Number: 2635 Loss: 1.7003118991851807 Time taken: 0.30211615562438965\n",
            "Batch Number: 2636 Loss: 1.6944323778152466 Time taken: 0.3143882751464844\n",
            "Batch Number: 2637 Loss: 1.680413842201233 Time taken: 0.30066847801208496\n",
            "Batch Number: 2638 Loss: 1.6388518810272217 Time taken: 0.29693007469177246\n",
            "Batch Number: 2639 Loss: 1.6564711332321167 Time taken: 0.3100566864013672\n",
            "Batch Number: 2640 Loss: 1.6656221151351929 Time taken: 0.3144564628601074\n",
            "Batch Number: 2641 Loss: 1.6605240106582642 Time taken: 0.3007643222808838\n",
            "Batch Number: 2642 Loss: 1.6691703796386719 Time taken: 0.30410027503967285\n",
            "Batch Number: 2643 Loss: 1.6661537885665894 Time taken: 0.30689477920532227\n",
            "Batch Number: 2644 Loss: 1.6378955841064453 Time taken: 0.29451966285705566\n",
            "Batch Number: 2645 Loss: 1.6684796810150146 Time taken: 0.29537010192871094\n",
            "Batch Number: 2646 Loss: 1.6671013832092285 Time taken: 0.31073999404907227\n",
            "Batch Number: 2647 Loss: 1.669148325920105 Time taken: 0.29909467697143555\n",
            "Batch Number: 2648 Loss: 1.6461032629013062 Time taken: 0.28997349739074707\n",
            "Batch Number: 2649 Loss: 1.6663142442703247 Time taken: 0.30089402198791504\n",
            "Batch Number: 2650 Loss: 1.6189836263656616 Time taken: 0.3007361888885498\n",
            "Batch Number: 2651 Loss: 1.6413851976394653 Time taken: 0.287137508392334\n",
            "Batch Number: 2652 Loss: 1.6675163507461548 Time taken: 0.30248260498046875\n",
            "Batch Number: 2653 Loss: 1.6446071863174438 Time taken: 0.29325079917907715\n",
            "Batch Number: 2654 Loss: 1.649248719215393 Time taken: 0.29561638832092285\n",
            "Batch Number: 2655 Loss: 1.6221768856048584 Time taken: 0.32782530784606934\n",
            "Batch Number: 2656 Loss: 1.608843445777893 Time taken: 0.33849573135375977\n",
            "Batch Number: 2657 Loss: 1.5981643199920654 Time taken: 0.36338138580322266\n",
            "Batch Number: 2658 Loss: 1.6365623474121094 Time taken: 0.302020788192749\n",
            "Batch Number: 2659 Loss: 1.6388555765151978 Time taken: 0.2867770195007324\n",
            "Batch Number: 2660 Loss: 1.6008819341659546 Time taken: 0.30126333236694336\n",
            "Batch Number: 2661 Loss: 1.6397521495819092 Time taken: 0.313678503036499\n",
            "Batch Number: 2662 Loss: 1.6866860389709473 Time taken: 0.3063540458679199\n",
            "Batch Number: 2663 Loss: 1.664177417755127 Time taken: 0.3004908561706543\n",
            "Batch Number: 2664 Loss: 1.6411882638931274 Time taken: 0.31302881240844727\n",
            "Batch Number: 2665 Loss: 1.6436970233917236 Time taken: 0.31554293632507324\n",
            "Batch Number: 2666 Loss: 1.6571214199066162 Time taken: 0.30925631523132324\n",
            "Batch Number: 2667 Loss: 1.6756248474121094 Time taken: 0.31014418601989746\n",
            "Batch Number: 2668 Loss: 1.6850467920303345 Time taken: 0.3013741970062256\n",
            "Batch Number: 2669 Loss: 1.680783987045288 Time taken: 0.29301881790161133\n",
            "Batch Number: 2670 Loss: 1.6403627395629883 Time taken: 0.2794497013092041\n",
            "Batch Number: 2671 Loss: 1.64295494556427 Time taken: 0.3147449493408203\n",
            "Batch Number: 2672 Loss: 1.6589412689208984 Time taken: 0.30092406272888184\n",
            "Batch Number: 2673 Loss: 1.664297103881836 Time taken: 0.30178165435791016\n",
            "Batch Number: 2674 Loss: 1.6502434015274048 Time taken: 0.31426095962524414\n",
            "Batch Number: 2675 Loss: 1.6375677585601807 Time taken: 0.38680481910705566\n",
            "Batch Number: 2676 Loss: 1.6631982326507568 Time taken: 0.3623616695404053\n",
            "Batch Number: 2677 Loss: 1.644453763961792 Time taken: 0.3052394390106201\n",
            "Batch Number: 2678 Loss: 1.6573320627212524 Time taken: 0.2948787212371826\n",
            "Batch Number: 2679 Loss: 1.6708064079284668 Time taken: 0.31037044525146484\n",
            "Batch Number: 2680 Loss: 1.6794438362121582 Time taken: 0.3213222026824951\n",
            "Batch Number: 2681 Loss: 1.6205675601959229 Time taken: 0.3054008483886719\n",
            "Batch Number: 2682 Loss: 1.6668245792388916 Time taken: 0.3060891628265381\n",
            "Batch Number: 2683 Loss: 1.634849190711975 Time taken: 0.2911515235900879\n",
            "Batch Number: 2684 Loss: 1.6562275886535645 Time taken: 0.29819655418395996\n",
            "Batch Number: 2685 Loss: 1.6459095478057861 Time taken: 0.31262946128845215\n",
            "Batch Number: 2686 Loss: 1.628836989402771 Time taken: 0.32801032066345215\n",
            "Batch Number: 2687 Loss: 1.632401704788208 Time taken: 0.29195666313171387\n",
            "Batch Number: 2688 Loss: 1.6419663429260254 Time taken: 0.28830790519714355\n",
            "Batch Number: 2689 Loss: 1.6346114873886108 Time taken: 0.3018782138824463\n",
            "Batch Number: 2690 Loss: 1.6461631059646606 Time taken: 0.2983715534210205\n",
            "Batch Number: 2691 Loss: 1.6528528928756714 Time taken: 0.30432558059692383\n",
            "Batch Number: 2692 Loss: 1.633264422416687 Time taken: 0.30234718322753906\n",
            "Batch Number: 2693 Loss: 1.6475412845611572 Time taken: 0.3097522258758545\n",
            "Batch Number: 2694 Loss: 1.6268330812454224 Time taken: 0.3123140335083008\n",
            "Batch Number: 2695 Loss: 1.6176693439483643 Time taken: 0.31316113471984863\n",
            "Batch Number: 2696 Loss: 1.6176670789718628 Time taken: 0.2936711311340332\n",
            "Batch Number: 2697 Loss: 1.612120509147644 Time taken: 0.30397844314575195\n",
            "Batch Number: 2698 Loss: 1.6162623167037964 Time taken: 0.29443883895874023\n",
            "Batch Number: 2699 Loss: 1.6320216655731201 Time taken: 0.3531050682067871\n",
            "Batch Number: 2700 Loss: 1.612011194229126 Time taken: 0.29174280166625977\n",
            "Batch Number: 2701 Loss: 1.5950901508331299 Time taken: 0.2898387908935547\n",
            "Batch Number: 2702 Loss: 1.625166416168213 Time taken: 0.3042936325073242\n",
            "Batch Number: 2703 Loss: 1.5981541872024536 Time taken: 0.3363058567047119\n",
            "Batch Number: 2704 Loss: 1.5985571146011353 Time taken: 0.3675410747528076\n",
            "Batch Number: 2705 Loss: 1.582653522491455 Time taken: 0.3558926582336426\n",
            "Batch Number: 2706 Loss: 1.6224113702774048 Time taken: 0.3056018352508545\n",
            "Batch Number: 2707 Loss: 1.5858042240142822 Time taken: 0.29317641258239746\n",
            "Batch Number: 2708 Loss: 1.6330152750015259 Time taken: 0.31707334518432617\n",
            "Batch Number: 2709 Loss: 1.6008918285369873 Time taken: 0.2865333557128906\n",
            "Batch Number: 2710 Loss: 1.5919017791748047 Time taken: 0.301224946975708\n",
            "Batch Number: 2711 Loss: 1.618661880493164 Time taken: 0.30841970443725586\n",
            "Batch Number: 2712 Loss: 1.6202811002731323 Time taken: 0.3045814037322998\n",
            "Batch Number: 2713 Loss: 1.6327275037765503 Time taken: 0.29938364028930664\n",
            "Batch Number: 2714 Loss: 1.6255847215652466 Time taken: 0.2968170642852783\n",
            "Batch Number: 2715 Loss: 1.6213791370391846 Time taken: 0.3242518901824951\n",
            "Batch Number: 2716 Loss: 1.632415771484375 Time taken: 0.2947697639465332\n",
            "Batch Number: 2717 Loss: 1.6292835474014282 Time taken: 0.29102134704589844\n",
            "Batch Number: 2718 Loss: 1.619769811630249 Time taken: 0.3369903564453125\n",
            "Batch Number: 2719 Loss: 1.6067698001861572 Time taken: 0.3199291229248047\n",
            "Batch Number: 2720 Loss: 1.6029658317565918 Time taken: 0.3017294406890869\n",
            "Batch Number: 2721 Loss: 1.608604907989502 Time taken: 0.30844712257385254\n",
            "Batch Number: 2722 Loss: 1.6284157037734985 Time taken: 0.30200839042663574\n",
            "Batch Number: 2723 Loss: 1.6199431419372559 Time taken: 0.310899019241333\n",
            "Batch Number: 2724 Loss: 1.6402000188827515 Time taken: 0.3091259002685547\n",
            "Batch Number: 2725 Loss: 1.640639066696167 Time taken: 0.2964038848876953\n",
            "Batch Number: 2726 Loss: 1.6113810539245605 Time taken: 0.3599538803100586\n",
            "Batch Number: 2727 Loss: 1.631570816040039 Time taken: 0.30592823028564453\n",
            "Batch Number: 2728 Loss: 1.6299909353256226 Time taken: 0.31121349334716797\n",
            "Batch Number: 2729 Loss: 1.6137999296188354 Time taken: 0.3221604824066162\n",
            "Batch Number: 2730 Loss: 1.6339223384857178 Time taken: 0.3220958709716797\n",
            "Batch Number: 2731 Loss: 1.6254531145095825 Time taken: 0.3222219944000244\n",
            "Batch Number: 2732 Loss: 1.6215674877166748 Time taken: 0.333437442779541\n",
            "Batch Number: 2733 Loss: 1.6281689405441284 Time taken: 0.3047513961791992\n",
            "Batch Number: 2734 Loss: 1.6086981296539307 Time taken: 0.3158586025238037\n",
            "Batch Number: 2735 Loss: 1.616786003112793 Time taken: 0.3049473762512207\n",
            "Batch Number: 2736 Loss: 1.643455982208252 Time taken: 0.30141544342041016\n",
            "Batch Number: 2737 Loss: 1.653098464012146 Time taken: 0.34099292755126953\n",
            "Batch Number: 2738 Loss: 1.680281162261963 Time taken: 0.3000621795654297\n",
            "Batch Number: 2739 Loss: 1.6535382270812988 Time taken: 0.29221105575561523\n",
            "Batch Number: 2740 Loss: 1.6139578819274902 Time taken: 0.33214807510375977\n",
            "Batch Number: 2741 Loss: 1.6626157760620117 Time taken: 0.31535816192626953\n",
            "Batch Number: 2742 Loss: 1.64902663230896 Time taken: 0.32193851470947266\n",
            "Batch Number: 2743 Loss: 1.6364731788635254 Time taken: 0.3977348804473877\n",
            "Batch Number: 2744 Loss: 1.6197351217269897 Time taken: 0.3866434097290039\n",
            "Batch Number: 2745 Loss: 1.6187517642974854 Time taken: 0.3610374927520752\n",
            "Batch Number: 2746 Loss: 1.5858385562896729 Time taken: 0.30660009384155273\n",
            "Batch Number: 2747 Loss: 1.6283124685287476 Time taken: 0.2973661422729492\n",
            "Batch Number: 2748 Loss: 1.5942846536636353 Time taken: 0.2967100143432617\n",
            "Batch Number: 2749 Loss: 1.6362844705581665 Time taken: 0.32358241081237793\n",
            "Batch Number: 2750 Loss: 1.6251012086868286 Time taken: 0.2936265468597412\n",
            "Batch Number: 2751 Loss: 1.642374038696289 Time taken: 0.31062984466552734\n",
            "Batch Number: 2752 Loss: 1.6191884279251099 Time taken: 0.29375386238098145\n",
            "Batch Number: 2753 Loss: 1.6253256797790527 Time taken: 0.3089148998260498\n",
            "Batch Number: 2754 Loss: 1.6199995279312134 Time taken: 0.2998361587524414\n",
            "Batch Number: 2755 Loss: 1.6147925853729248 Time taken: 0.32342052459716797\n",
            "Batch Number: 2756 Loss: 1.6266008615493774 Time taken: 0.3009035587310791\n",
            "Batch Number: 2757 Loss: 1.6152207851409912 Time taken: 0.28826165199279785\n",
            "Batch Number: 2758 Loss: 1.6064399480819702 Time taken: 0.30735230445861816\n",
            "Batch Number: 2759 Loss: 1.596027135848999 Time taken: 0.3105757236480713\n",
            "Batch Number: 2760 Loss: 1.5984293222427368 Time taken: 0.37684202194213867\n",
            "Batch Number: 2761 Loss: 1.6032956838607788 Time taken: 0.34937262535095215\n",
            "Batch Number: 2762 Loss: 1.611700177192688 Time taken: 0.31081366539001465\n",
            "Batch Number: 2763 Loss: 1.6194827556610107 Time taken: 0.2817044258117676\n",
            "Batch Number: 2764 Loss: 1.6099573373794556 Time taken: 0.30498719215393066\n",
            "Batch Number: 2765 Loss: 1.605790138244629 Time taken: 0.31995344161987305\n",
            "Batch Number: 2766 Loss: 1.5995343923568726 Time taken: 0.3071749210357666\n",
            "Batch Number: 2767 Loss: 1.6148521900177002 Time taken: 0.3204009532928467\n",
            "Batch Number: 2768 Loss: 1.631480097770691 Time taken: 0.30469179153442383\n",
            "Batch Number: 2769 Loss: 1.6424733400344849 Time taken: 0.30629706382751465\n",
            "Batch Number: 2770 Loss: 1.6605150699615479 Time taken: 0.3000633716583252\n",
            "Batch Number: 2771 Loss: 1.6622573137283325 Time taken: 0.3069610595703125\n",
            "Batch Number: 2772 Loss: 1.6848942041397095 Time taken: 0.30501747131347656\n",
            "Batch Number: 2773 Loss: 1.689118504524231 Time taken: 0.3003556728363037\n",
            "Batch Number: 2774 Loss: 1.6721335649490356 Time taken: 0.3585338592529297\n",
            "Batch Number: 2775 Loss: 1.676007866859436 Time taken: 0.376556396484375\n",
            "Batch Number: 2776 Loss: 1.6666266918182373 Time taken: 0.3656454086303711\n",
            "Batch Number: 2777 Loss: 1.6780203580856323 Time taken: 0.3135111331939697\n",
            "Batch Number: 2778 Loss: 1.6674166917800903 Time taken: 0.3733694553375244\n",
            "Batch Number: 2779 Loss: 1.6511403322219849 Time taken: 0.369718074798584\n",
            "Batch Number: 2780 Loss: 1.662987470626831 Time taken: 0.3020777702331543\n",
            "Batch Number: 2781 Loss: 1.6301894187927246 Time taken: 0.2996654510498047\n",
            "Batch Number: 2782 Loss: 1.617789626121521 Time taken: 0.29704928398132324\n",
            "Batch Number: 2783 Loss: 1.6501975059509277 Time taken: 0.3029007911682129\n",
            "Batch Number: 2784 Loss: 1.6128287315368652 Time taken: 0.30121922492980957\n",
            "Batch Number: 2785 Loss: 1.6479239463806152 Time taken: 0.3021876811981201\n",
            "Batch Number: 2786 Loss: 1.619275450706482 Time taken: 0.3284449577331543\n",
            "Batch Number: 2787 Loss: 1.6230014562606812 Time taken: 0.3771953582763672\n",
            "Batch Number: 2788 Loss: 1.6189082860946655 Time taken: 0.35324549674987793\n",
            "Batch Number: 2789 Loss: 1.6265994310379028 Time taken: 0.29779481887817383\n",
            "Batch Number: 2790 Loss: 1.595932960510254 Time taken: 0.305767297744751\n",
            "Batch Number: 2791 Loss: 1.6320191621780396 Time taken: 0.3004622459411621\n",
            "Batch Number: 2792 Loss: 1.6306017637252808 Time taken: 0.31845784187316895\n",
            "Batch Number: 2793 Loss: 1.657049536705017 Time taken: 0.3090677261352539\n",
            "Batch Number: 2794 Loss: 1.6605322360992432 Time taken: 0.30037546157836914\n",
            "Batch Number: 2795 Loss: 1.63646399974823 Time taken: 0.3087728023529053\n",
            "Batch Number: 2796 Loss: 1.6541318893432617 Time taken: 0.29787111282348633\n",
            "Batch Number: 2797 Loss: 1.641058087348938 Time taken: 0.33777785301208496\n",
            "Batch Number: 2798 Loss: 1.652677297592163 Time taken: 0.3110995292663574\n",
            "Batch Number: 2799 Loss: 1.659361720085144 Time taken: 0.3072700500488281\n",
            "Batch Number: 2800 Loss: 1.6411356925964355 Time taken: 0.3077211380004883\n",
            "Batch Number: 2801 Loss: 1.6613993644714355 Time taken: 0.3149385452270508\n",
            "Batch Number: 2802 Loss: 1.6387927532196045 Time taken: 0.2981886863708496\n",
            "Batch Number: 2803 Loss: 1.641647458076477 Time taken: 0.31258487701416016\n",
            "Batch Number: 2804 Loss: 1.6583980321884155 Time taken: 0.3144567012786865\n",
            "Batch Number: 2805 Loss: 1.6523010730743408 Time taken: 0.3116469383239746\n",
            "Batch Number: 2806 Loss: 1.6536206007003784 Time taken: 0.30127763748168945\n",
            "Batch Number: 2807 Loss: 1.6378536224365234 Time taken: 0.30933070182800293\n",
            "Batch Number: 2808 Loss: 1.6687926054000854 Time taken: 0.2880678176879883\n",
            "Batch Number: 2809 Loss: 1.6641793251037598 Time taken: 0.29735755920410156\n",
            "Batch Number: 2810 Loss: 1.6901978254318237 Time taken: 0.3028066158294678\n",
            "Batch Number: 2811 Loss: 1.671280860900879 Time taken: 0.34244418144226074\n",
            "Batch Number: 2812 Loss: 1.6731364727020264 Time taken: 0.29846954345703125\n",
            "Batch Number: 2813 Loss: 1.6782525777816772 Time taken: 0.30965709686279297\n",
            "Batch Number: 2814 Loss: 1.6573470830917358 Time taken: 0.32268524169921875\n",
            "Batch Number: 2815 Loss: 1.641348958015442 Time taken: 0.29743456840515137\n",
            "Batch Number: 2816 Loss: 1.6425431966781616 Time taken: 0.3040437698364258\n",
            "Batch Number: 2817 Loss: 1.6551129817962646 Time taken: 0.31764650344848633\n",
            "Batch Number: 2818 Loss: 1.6386370658874512 Time taken: 0.2954866886138916\n",
            "Batch Number: 2819 Loss: 1.640879511833191 Time taken: 0.28809618949890137\n",
            "Batch Number: 2820 Loss: 1.6249134540557861 Time taken: 0.3067655563354492\n",
            "Batch Number: 2821 Loss: 1.652205467224121 Time taken: 0.3304882049560547\n",
            "Batch Number: 2822 Loss: 1.6684521436691284 Time taken: 0.36650633811950684\n",
            "Batch Number: 2823 Loss: 1.6380974054336548 Time taken: 0.37647485733032227\n",
            "Batch Number: 2824 Loss: 1.6186308860778809 Time taken: 0.37069010734558105\n",
            "Batch Number: 2825 Loss: 1.6303739547729492 Time taken: 0.3828909397125244\n",
            "Batch Number: 2826 Loss: 1.640555739402771 Time taken: 0.31816625595092773\n",
            "Batch Number: 2827 Loss: 1.6109790802001953 Time taken: 0.29995036125183105\n",
            "Batch Number: 2828 Loss: 1.6166939735412598 Time taken: 0.2985515594482422\n",
            "Batch Number: 2829 Loss: 1.639186978340149 Time taken: 0.334475040435791\n",
            "Batch Number: 2830 Loss: 1.6236095428466797 Time taken: 0.3120272159576416\n",
            "Batch Number: 2831 Loss: 1.6286320686340332 Time taken: 0.29115843772888184\n",
            "Batch Number: 2832 Loss: 1.6176577806472778 Time taken: 0.32100462913513184\n",
            "Batch Number: 2833 Loss: 1.6192435026168823 Time taken: 0.30501484870910645\n",
            "Batch Number: 2834 Loss: 1.6203941106796265 Time taken: 0.293637752532959\n",
            "Batch Number: 2835 Loss: 1.622011423110962 Time taken: 0.33110785484313965\n",
            "Batch Number: 2836 Loss: 1.5790154933929443 Time taken: 0.30509042739868164\n",
            "Batch Number: 2837 Loss: 1.6173113584518433 Time taken: 0.2927677631378174\n",
            "Batch Number: 2838 Loss: 1.6292222738265991 Time taken: 0.3071410655975342\n",
            "Batch Number: 2839 Loss: 1.6243836879730225 Time taken: 0.30811214447021484\n",
            "Batch Number: 2840 Loss: 1.6378098726272583 Time taken: 0.3617269992828369\n",
            "Batch Number: 2841 Loss: 1.6689928770065308 Time taken: 0.38763976097106934\n",
            "Batch Number: 2842 Loss: 1.620355248451233 Time taken: 0.32818031311035156\n",
            "Batch Number: 2843 Loss: 1.6462031602859497 Time taken: 0.29660511016845703\n",
            "Batch Number: 2844 Loss: 1.6380261182785034 Time taken: 0.3029654026031494\n",
            "Batch Number: 2845 Loss: 1.6594727039337158 Time taken: 0.30463552474975586\n",
            "Batch Number: 2846 Loss: 1.6421785354614258 Time taken: 0.2940177917480469\n",
            "Batch Number: 2847 Loss: 1.631240963935852 Time taken: 0.29421019554138184\n",
            "Batch Number: 2848 Loss: 1.6053906679153442 Time taken: 0.3034064769744873\n",
            "Batch Number: 2849 Loss: 1.6077462434768677 Time taken: 0.299907922744751\n",
            "Batch Number: 2850 Loss: 1.6078051328659058 Time taken: 0.3000626564025879\n",
            "Batch Number: 2851 Loss: 1.6487728357315063 Time taken: 0.3084218502044678\n",
            "Batch Number: 2852 Loss: 1.624309778213501 Time taken: 0.3545386791229248\n",
            "Batch Number: 2853 Loss: 1.6202481985092163 Time taken: 0.3597695827484131\n",
            "Batch Number: 2854 Loss: 1.6506255865097046 Time taken: 0.34919238090515137\n",
            "Batch Number: 2855 Loss: 1.6269750595092773 Time taken: 0.29453301429748535\n",
            "Batch Number: 2856 Loss: 1.645732045173645 Time taken: 0.2942237854003906\n",
            "Batch Number: 2857 Loss: 1.6411505937576294 Time taken: 0.30489563941955566\n",
            "Batch Number: 2858 Loss: 1.6636351346969604 Time taken: 0.29277849197387695\n",
            "Batch Number: 2859 Loss: 1.6484344005584717 Time taken: 0.2962963581085205\n",
            "Batch Number: 2860 Loss: 1.6161757707595825 Time taken: 0.32878684997558594\n",
            "Batch Number: 2861 Loss: 1.605159878730774 Time taken: 0.3835639953613281\n",
            "Batch Number: 2862 Loss: 1.6317024230957031 Time taken: 0.3866560459136963\n",
            "Batch Number: 2863 Loss: 1.65192449092865 Time taken: 0.37624096870422363\n",
            "Batch Number: 2864 Loss: 1.6473580598831177 Time taken: 0.3725459575653076\n",
            "Batch Number: 2865 Loss: 1.626968502998352 Time taken: 0.3669288158416748\n",
            "Batch Number: 2866 Loss: 1.5885964632034302 Time taken: 0.32341933250427246\n",
            "Batch Number: 2867 Loss: 1.652762532234192 Time taken: 0.2911207675933838\n",
            "Batch Number: 2868 Loss: 1.6279655694961548 Time taken: 0.29436445236206055\n",
            "Batch Number: 2869 Loss: 1.5869865417480469 Time taken: 0.317018985748291\n",
            "Batch Number: 2870 Loss: 1.6111514568328857 Time taken: 0.2948267459869385\n",
            "Batch Number: 2871 Loss: 1.6279131174087524 Time taken: 0.2969820499420166\n",
            "Batch Number: 2872 Loss: 1.6209388971328735 Time taken: 0.33976078033447266\n",
            "Batch Number: 2873 Loss: 1.602943778038025 Time taken: 0.3031628131866455\n",
            "Batch Number: 2874 Loss: 1.6183652877807617 Time taken: 0.2887458801269531\n",
            "Batch Number: 2875 Loss: 1.6223267316818237 Time taken: 0.29706525802612305\n",
            "Batch Number: 2876 Loss: 1.5994665622711182 Time taken: 0.29657626152038574\n",
            "Batch Number: 2877 Loss: 1.625132441520691 Time taken: 0.27610206604003906\n",
            "Batch Number: 2878 Loss: 1.6189268827438354 Time taken: 0.3322176933288574\n",
            "Batch Number: 2879 Loss: 1.6172410249710083 Time taken: 0.31217074394226074\n",
            "Batch Number: 2880 Loss: 1.580322265625 Time taken: 0.29619312286376953\n",
            "Batch Number: 2881 Loss: 1.5898690223693848 Time taken: 0.3062260150909424\n",
            "Batch Number: 2882 Loss: 1.591834306716919 Time taken: 0.33077096939086914\n",
            "Batch Number: 2883 Loss: 1.5982142686843872 Time taken: 0.2928004264831543\n",
            "Batch Number: 2884 Loss: 1.5909996032714844 Time taken: 0.2942347526550293\n",
            "Batch Number: 2885 Loss: 1.591478705406189 Time taken: 0.3476896286010742\n",
            "Batch Number: 2886 Loss: 1.6012167930603027 Time taken: 0.30168676376342773\n",
            "Batch Number: 2887 Loss: 1.6206938028335571 Time taken: 0.2885007858276367\n",
            "Batch Number: 2888 Loss: 1.6114952564239502 Time taken: 0.29845571517944336\n",
            "Batch Number: 2889 Loss: 1.6302111148834229 Time taken: 0.3253054618835449\n",
            "Batch Number: 2890 Loss: 1.5892324447631836 Time taken: 0.2922630310058594\n",
            "Batch Number: 2891 Loss: 1.6050482988357544 Time taken: 0.3351776599884033\n",
            "Batch Number: 2892 Loss: 1.6374988555908203 Time taken: 0.31826043128967285\n",
            "Batch Number: 2893 Loss: 1.5902336835861206 Time taken: 0.2924504280090332\n",
            "Batch Number: 2894 Loss: 1.597955346107483 Time taken: 0.3227875232696533\n",
            "Batch Number: 2895 Loss: 1.6334565877914429 Time taken: 0.3406665325164795\n",
            "Batch Number: 2896 Loss: 1.5822839736938477 Time taken: 0.29392457008361816\n",
            "Batch Number: 2897 Loss: 1.6066343784332275 Time taken: 0.3061707019805908\n",
            "Batch Number: 2898 Loss: 1.6094051599502563 Time taken: 0.31714510917663574\n",
            "Batch Number: 2899 Loss: 1.5834062099456787 Time taken: 0.32601380348205566\n",
            "Batch Number: 2900 Loss: 1.6038471460342407 Time taken: 0.2966887950897217\n",
            "Batch Number: 2901 Loss: 1.603700876235962 Time taken: 0.2867698669433594\n",
            "Batch Number: 2902 Loss: 1.5822428464889526 Time taken: 0.29170894622802734\n",
            "Batch Number: 2903 Loss: 1.6160776615142822 Time taken: 0.2834198474884033\n",
            "Batch Number: 2904 Loss: 1.6254191398620605 Time taken: 0.33838772773742676\n",
            "Batch Number: 2905 Loss: 1.60718834400177 Time taken: 0.31137967109680176\n",
            "Batch Number: 2906 Loss: 1.5871111154556274 Time taken: 0.2916598320007324\n",
            "Batch Number: 2907 Loss: 1.6173121929168701 Time taken: 0.2994668483734131\n",
            "Batch Number: 2908 Loss: 1.5870766639709473 Time taken: 0.32190442085266113\n",
            "Batch Number: 2909 Loss: 1.593735933303833 Time taken: 0.291140079498291\n",
            "Batch Number: 2910 Loss: 1.5911716222763062 Time taken: 0.2992279529571533\n",
            "Batch Number: 2911 Loss: 1.5949039459228516 Time taken: 0.30628180503845215\n",
            "Batch Number: 2912 Loss: 1.598805546760559 Time taken: 0.31392931938171387\n",
            "Batch Number: 2913 Loss: 1.6110678911209106 Time taken: 0.30577826499938965\n",
            "Batch Number: 2914 Loss: 1.6043412685394287 Time taken: 0.2999861240386963\n",
            "Batch Number: 2915 Loss: 1.5841647386550903 Time taken: 0.30849194526672363\n",
            "Batch Number: 2916 Loss: 1.6299169063568115 Time taken: 0.2929549217224121\n",
            "Batch Number: 2917 Loss: 1.624730110168457 Time taken: 0.32219791412353516\n",
            "Batch Number: 2918 Loss: 1.6351052522659302 Time taken: 0.31893038749694824\n",
            "Batch Number: 2919 Loss: 1.6533331871032715 Time taken: 0.33306002616882324\n",
            "Batch Number: 2920 Loss: 1.596783995628357 Time taken: 0.3252694606781006\n",
            "Batch Number: 2921 Loss: 1.583368182182312 Time taken: 0.35030031204223633\n",
            "Batch Number: 2922 Loss: 1.5993099212646484 Time taken: 0.29758405685424805\n",
            "Batch Number: 2923 Loss: 1.595404028892517 Time taken: 0.3189122676849365\n",
            "Batch Number: 2924 Loss: 1.631527066230774 Time taken: 0.3196547031402588\n",
            "Batch Number: 2925 Loss: 1.6096880435943604 Time taken: 0.2965822219848633\n",
            "Batch Number: 2926 Loss: 1.5841585397720337 Time taken: 0.28723764419555664\n",
            "Batch Number: 2927 Loss: 1.5809003114700317 Time taken: 0.2934994697570801\n",
            "Batch Number: 2928 Loss: 1.6372745037078857 Time taken: 0.28917789459228516\n",
            "Batch Number: 2929 Loss: 1.6161144971847534 Time taken: 0.3321669101715088\n",
            "Batch Number: 2930 Loss: 1.606229543685913 Time taken: 0.33121418952941895\n",
            "Batch Number: 2931 Loss: 1.5856640338897705 Time taken: 0.30965185165405273\n",
            "Batch Number: 2932 Loss: 1.6265302896499634 Time taken: 0.2950112819671631\n",
            "Batch Number: 2933 Loss: 1.608744502067566 Time taken: 0.29630327224731445\n",
            "Batch Number: 2934 Loss: 1.5986406803131104 Time taken: 0.3114590644836426\n",
            "Batch Number: 2935 Loss: 1.61360764503479 Time taken: 0.2944324016571045\n",
            "Batch Number: 2936 Loss: 1.593151569366455 Time taken: 0.32589077949523926\n",
            "Batch Number: 2937 Loss: 1.6138252019882202 Time taken: 0.325549840927124\n",
            "Batch Number: 2938 Loss: 1.5930174589157104 Time taken: 0.29360342025756836\n",
            "Batch Number: 2939 Loss: 1.6207574605941772 Time taken: 0.32782745361328125\n",
            "Batch Number: 2940 Loss: 1.5923168659210205 Time taken: 0.36621665954589844\n",
            "Batch Number: 2941 Loss: 1.6120927333831787 Time taken: 0.34970784187316895\n",
            "Batch Number: 2942 Loss: 1.5832089185714722 Time taken: 0.28661298751831055\n",
            "Batch Number: 2943 Loss: 1.5712859630584717 Time taken: 0.2960844039916992\n",
            "Batch Number: 2944 Loss: 1.5981765985488892 Time taken: 0.28903722763061523\n",
            "Batch Number: 2945 Loss: 1.6087892055511475 Time taken: 0.32088232040405273\n",
            "Batch Number: 2946 Loss: 1.6212340593338013 Time taken: 0.3106517791748047\n",
            "Batch Number: 2947 Loss: 1.5838733911514282 Time taken: 0.30838894844055176\n",
            "Batch Number: 2948 Loss: 1.5936764478683472 Time taken: 0.3096628189086914\n",
            "Batch Number: 2949 Loss: 1.5918738842010498 Time taken: 0.2973940372467041\n",
            "Batch Number: 2950 Loss: 1.6026058197021484 Time taken: 0.3136167526245117\n",
            "Batch Number: 2951 Loss: 1.635629653930664 Time taken: 0.3020761013031006\n",
            "Batch Number: 2952 Loss: 1.627206802368164 Time taken: 0.35599613189697266\n",
            "Batch Number: 2953 Loss: 1.6487390995025635 Time taken: 0.38640785217285156\n",
            "Batch Number: 2954 Loss: 1.6241668462753296 Time taken: 0.3381917476654053\n",
            "Batch Number: 2955 Loss: 1.6144455671310425 Time taken: 0.2976360321044922\n",
            "Batch Number: 2956 Loss: 1.6306253671646118 Time taken: 0.30594563484191895\n",
            "Batch Number: 2957 Loss: 1.6243749856948853 Time taken: 0.30419254302978516\n",
            "Batch Number: 2958 Loss: 1.694800615310669 Time taken: 0.2909564971923828\n",
            "Batch Number: 2959 Loss: 1.6210848093032837 Time taken: 0.30776166915893555\n",
            "Batch Number: 2960 Loss: 1.645522117614746 Time taken: 0.31368541717529297\n",
            "Batch Number: 2961 Loss: 1.6180616617202759 Time taken: 0.2763700485229492\n",
            "Batch Number: 2962 Loss: 1.6333322525024414 Time taken: 0.29212212562561035\n",
            "Batch Number: 2963 Loss: 1.6284908056259155 Time taken: 0.34414243698120117\n",
            "Batch Number: 2964 Loss: 1.638440728187561 Time taken: 0.29242801666259766\n",
            "Batch Number: 2965 Loss: 1.5984824895858765 Time taken: 0.3017463684082031\n",
            "Batch Number: 2966 Loss: 1.6104328632354736 Time taken: 0.31337404251098633\n",
            "Batch Number: 2967 Loss: 1.6301369667053223 Time taken: 0.28812551498413086\n",
            "Batch Number: 2968 Loss: 1.587220311164856 Time taken: 0.3014335632324219\n",
            "Batch Number: 2969 Loss: 1.6076594591140747 Time taken: 0.30340075492858887\n",
            "Batch Number: 2970 Loss: 1.599966287612915 Time taken: 0.3165280818939209\n",
            "Batch Number: 2971 Loss: 1.6019620895385742 Time taken: 0.29346680641174316\n",
            "Batch Number: 2972 Loss: 1.6166110038757324 Time taken: 0.2961845397949219\n",
            "Batch Number: 2973 Loss: 1.6085758209228516 Time taken: 0.3037233352661133\n",
            "Batch Number: 2974 Loss: 1.629721999168396 Time taken: 0.29196739196777344\n",
            "Batch Number: 2975 Loss: 1.629072666168213 Time taken: 0.29483532905578613\n",
            "Batch Number: 2976 Loss: 1.616229772567749 Time taken: 0.30734729766845703\n",
            "Batch Number: 2977 Loss: 1.6459693908691406 Time taken: 0.3096015453338623\n",
            "Batch Number: 2978 Loss: 1.64439058303833 Time taken: 0.29270052909851074\n",
            "Batch Number: 2979 Loss: 1.629643440246582 Time taken: 0.3055381774902344\n",
            "Batch Number: 2980 Loss: 1.624096155166626 Time taken: 0.3208909034729004\n",
            "Batch Number: 2981 Loss: 1.6118865013122559 Time taken: 0.300936222076416\n",
            "Batch Number: 2982 Loss: 1.633376121520996 Time taken: 0.35718202590942383\n",
            "Batch Number: 2983 Loss: 1.6476032733917236 Time taken: 0.3206443786621094\n",
            "Batch Number: 2984 Loss: 1.6261838674545288 Time taken: 0.2971816062927246\n",
            "Batch Number: 2985 Loss: 1.6353106498718262 Time taken: 0.33031558990478516\n",
            "Batch Number: 2986 Loss: 1.66621994972229 Time taken: 0.32479166984558105\n",
            "Batch Number: 2987 Loss: 1.669089674949646 Time taken: 0.2947425842285156\n",
            "Batch Number: 2988 Loss: 1.655237078666687 Time taken: 0.3031272888183594\n",
            "Batch Number: 2989 Loss: 1.6245547533035278 Time taken: 0.30621862411499023\n",
            "Batch Number: 2990 Loss: 1.6568931341171265 Time taken: 0.29848408699035645\n",
            "Batch Number: 2991 Loss: 1.6623797416687012 Time taken: 0.2998313903808594\n",
            "Batch Number: 2992 Loss: 1.633405089378357 Time taken: 0.3773789405822754\n",
            "Batch Number: 2993 Loss: 1.666366457939148 Time taken: 0.30147504806518555\n",
            "Batch Number: 2994 Loss: 1.6423553228378296 Time taken: 0.29996323585510254\n",
            "Batch Number: 2995 Loss: 1.6229078769683838 Time taken: 0.31333351135253906\n",
            "Batch Number: 2996 Loss: 1.5982776880264282 Time taken: 0.2978835105895996\n",
            "Batch Number: 2997 Loss: 1.6151021718978882 Time taken: 0.31378650665283203\n",
            "Batch Number: 2998 Loss: 1.6046491861343384 Time taken: 0.3220179080963135\n",
            "Batch Number: 2999 Loss: 1.600089192390442 Time taken: 0.2983667850494385\n",
            "Batch Number: 3000 Loss: 1.625378131866455 Time taken: 0.29541850090026855\n",
            "Batch Number: 3001 Loss: 1.6215211153030396 Time taken: 0.289813756942749\n",
            "Batch Number: 3002 Loss: 1.6313453912734985 Time taken: 0.31705784797668457\n",
            "Batch Number: 3003 Loss: 1.617618441581726 Time taken: 0.35367655754089355\n",
            "Batch Number: 3004 Loss: 1.5989820957183838 Time taken: 0.3663628101348877\n",
            "Batch Number: 3005 Loss: 1.6161928176879883 Time taken: 0.306380033493042\n",
            "Batch Number: 3006 Loss: 1.6129207611083984 Time taken: 0.2975771427154541\n",
            "Batch Number: 3007 Loss: 1.6052603721618652 Time taken: 0.29688334465026855\n",
            "Batch Number: 3008 Loss: 1.605865478515625 Time taken: 0.3216538429260254\n",
            "Batch Number: 3009 Loss: 1.610489010810852 Time taken: 0.2939720153808594\n",
            "Batch Number: 3010 Loss: 1.6142011880874634 Time taken: 0.297260046005249\n",
            "Batch Number: 3011 Loss: 1.643275260925293 Time taken: 0.323408842086792\n",
            "Batch Number: 3012 Loss: 1.5994218587875366 Time taken: 0.29319286346435547\n",
            "Batch Number: 3013 Loss: 1.6143090724945068 Time taken: 0.2945883274078369\n",
            "Batch Number: 3014 Loss: 1.6273386478424072 Time taken: 0.3042733669281006\n",
            "Batch Number: 3015 Loss: 1.61335027217865 Time taken: 0.30141305923461914\n",
            "Batch Number: 3016 Loss: 1.5775697231292725 Time taken: 0.29390621185302734\n",
            "Batch Number: 3017 Loss: 1.6246285438537598 Time taken: 0.29857563972473145\n",
            "Batch Number: 3018 Loss: 1.575417160987854 Time taken: 0.31626152992248535\n",
            "Batch Number: 3019 Loss: 1.6414077281951904 Time taken: 0.29522705078125\n",
            "Batch Number: 3020 Loss: 1.599129557609558 Time taken: 0.29148244857788086\n",
            "Batch Number: 3021 Loss: 1.6477413177490234 Time taken: 0.31653499603271484\n",
            "Batch Number: 3022 Loss: 1.6316280364990234 Time taken: 0.2907116413116455\n",
            "Batch Number: 3023 Loss: 1.654213786125183 Time taken: 0.29610204696655273\n",
            "Batch Number: 3024 Loss: 1.6388585567474365 Time taken: 0.3252108097076416\n",
            "Batch Number: 3025 Loss: 1.648815631866455 Time taken: 0.3737208843231201\n",
            "Batch Number: 3026 Loss: 1.627984881401062 Time taken: 0.35834717750549316\n",
            "Batch Number: 3027 Loss: 1.6138756275177002 Time taken: 0.3616344928741455\n",
            "Batch Number: 3028 Loss: 1.6090774536132812 Time taken: 0.3641519546508789\n",
            "Batch Number: 3029 Loss: 1.602236032485962 Time taken: 0.38851189613342285\n",
            "Batch Number: 3030 Loss: 1.5990498065948486 Time taken: 0.3157327175140381\n",
            "Batch Number: 3031 Loss: 1.6032756567001343 Time taken: 0.30356454849243164\n",
            "Batch Number: 3032 Loss: 1.5941890478134155 Time taken: 0.3014945983886719\n",
            "Batch Number: 3033 Loss: 1.6044867038726807 Time taken: 0.3164675235748291\n",
            "Batch Number: 3034 Loss: 1.5987361669540405 Time taken: 0.2967996597290039\n",
            "Batch Number: 3035 Loss: 1.6129118204116821 Time taken: 0.31273508071899414\n",
            "Batch Number: 3036 Loss: 1.6013412475585938 Time taken: 0.3050556182861328\n",
            "Batch Number: 3037 Loss: 1.625298261642456 Time taken: 0.30772972106933594\n",
            "Batch Number: 3038 Loss: 1.6258492469787598 Time taken: 0.2859179973602295\n",
            "Batch Number: 3039 Loss: 1.6513158082962036 Time taken: 0.3136868476867676\n",
            "Batch Number: 3040 Loss: 1.6262508630752563 Time taken: 0.30709314346313477\n",
            "Batch Number: 3041 Loss: 1.6020337343215942 Time taken: 0.2910921573638916\n",
            "Batch Number: 3042 Loss: 1.6098580360412598 Time taken: 0.3036036491394043\n",
            "Batch Number: 3043 Loss: 1.6104307174682617 Time taken: 0.3753697872161865\n",
            "Batch Number: 3044 Loss: 1.6364719867706299 Time taken: 0.37297582626342773\n",
            "Batch Number: 3045 Loss: 1.6266615390777588 Time taken: 0.3175685405731201\n",
            "Batch Number: 3046 Loss: 1.6290032863616943 Time taken: 0.35297298431396484\n",
            "Batch Number: 3047 Loss: 1.5846327543258667 Time taken: 0.35887956619262695\n",
            "Batch Number: 3048 Loss: 1.5957896709442139 Time taken: 0.3440287113189697\n",
            "Batch Number: 3049 Loss: 1.6117966175079346 Time taken: 0.3096487522125244\n",
            "Batch Number: 3050 Loss: 1.5864598751068115 Time taken: 0.2937815189361572\n",
            "Batch Number: 3051 Loss: 1.6108448505401611 Time taken: 0.29952335357666016\n",
            "Batch Number: 3052 Loss: 1.6027745008468628 Time taken: 0.30092644691467285\n",
            "Batch Number: 3053 Loss: 1.5840067863464355 Time taken: 0.2988128662109375\n",
            "Batch Number: 3054 Loss: 1.598387598991394 Time taken: 0.3074631690979004\n",
            "Batch Number: 3055 Loss: 1.5857058763504028 Time taken: 0.30261778831481934\n",
            "Batch Number: 3056 Loss: 1.5954850912094116 Time taken: 0.3302042484283447\n",
            "Batch Number: 3057 Loss: 1.6130489110946655 Time taken: 0.29035329818725586\n",
            "Batch Number: 3058 Loss: 1.6080272197723389 Time taken: 0.30584073066711426\n",
            "Batch Number: 3059 Loss: 1.5907598733901978 Time taken: 0.3141047954559326\n",
            "Batch Number: 3060 Loss: 1.6085846424102783 Time taken: 0.30024194717407227\n",
            "Batch Number: 3061 Loss: 1.6142593622207642 Time taken: 0.3696279525756836\n",
            "Batch Number: 3062 Loss: 1.5830281972885132 Time taken: 0.3976712226867676\n",
            "Batch Number: 3063 Loss: 1.566595196723938 Time taken: 0.35455751419067383\n",
            "Batch Number: 3064 Loss: 1.5871762037277222 Time taken: 0.31487441062927246\n",
            "Batch Number: 3065 Loss: 1.5597763061523438 Time taken: 0.3043205738067627\n",
            "Batch Number: 3066 Loss: 1.5812804698944092 Time taken: 0.30938220024108887\n",
            "Batch Number: 3067 Loss: 1.5782523155212402 Time taken: 0.30014657974243164\n",
            "Batch Number: 3068 Loss: 1.571005940437317 Time taken: 0.3071932792663574\n",
            "Batch Number: 3069 Loss: 1.5462175607681274 Time taken: 0.30342745780944824\n",
            "Batch Number: 3070 Loss: 1.5640122890472412 Time taken: 0.30323052406311035\n",
            "Batch Number: 3071 Loss: 1.5817458629608154 Time taken: 0.30755162239074707\n",
            "Batch Number: 3072 Loss: 1.57758629322052 Time taken: 0.31016039848327637\n",
            "Batch Number: 3073 Loss: 1.5956028699874878 Time taken: 0.32503628730773926\n",
            "Batch Number: 3074 Loss: 1.6135259866714478 Time taken: 0.305560827255249\n",
            "Batch Number: 3075 Loss: 1.584525465965271 Time taken: 0.29404187202453613\n",
            "Batch Number: 3076 Loss: 1.5945541858673096 Time taken: 0.3074920177459717\n",
            "Batch Number: 3077 Loss: 1.5907832384109497 Time taken: 0.38138341903686523\n",
            "Batch Number: 3078 Loss: 1.5704643726348877 Time taken: 0.3597393035888672\n",
            "Batch Number: 3079 Loss: 1.563971996307373 Time taken: 0.32666563987731934\n",
            "Batch Number: 3080 Loss: 1.5806479454040527 Time taken: 0.3560013771057129\n",
            "Batch Number: 3081 Loss: 1.5840637683868408 Time taken: 0.3582444190979004\n",
            "Batch Number: 3082 Loss: 1.5662113428115845 Time taken: 0.35164952278137207\n",
            "Batch Number: 3083 Loss: 1.5751625299453735 Time taken: 0.373870849609375\n",
            "Batch Number: 3084 Loss: 1.5889207124710083 Time taken: 0.36151552200317383\n",
            "Batch Number: 3085 Loss: 1.5824942588806152 Time taken: 0.3004140853881836\n",
            "Batch Number: 3086 Loss: 1.61090886592865 Time taken: 0.3034350872039795\n",
            "Batch Number: 3087 Loss: 1.5995194911956787 Time taken: 0.29633593559265137\n",
            "Batch Number: 3088 Loss: 1.6003856658935547 Time taken: 0.30997538566589355\n",
            "Batch Number: 3089 Loss: 1.5772794485092163 Time taken: 0.3018367290496826\n",
            "Batch Number: 3090 Loss: 1.6020351648330688 Time taken: 0.30747008323669434\n",
            "Batch Number: 3091 Loss: 1.5951855182647705 Time taken: 0.3181610107421875\n",
            "Batch Number: 3092 Loss: 1.5895094871520996 Time taken: 0.2953481674194336\n",
            "Batch Number: 3093 Loss: 1.5735416412353516 Time taken: 0.3086256980895996\n",
            "Batch Number: 3094 Loss: 1.6024872064590454 Time taken: 0.31653261184692383\n",
            "Batch Number: 3095 Loss: 1.5692166090011597 Time taken: 0.29784488677978516\n",
            "Batch Number: 3096 Loss: 1.5924429893493652 Time taken: 0.3053905963897705\n",
            "Batch Number: 3097 Loss: 1.5832059383392334 Time taken: 0.3028433322906494\n",
            "Batch Number: 3098 Loss: 1.628552794456482 Time taken: 0.29868125915527344\n",
            "Batch Number: 3099 Loss: 1.626282811164856 Time taken: 0.2959566116333008\n",
            "Batch Number: 3100 Loss: 1.5911859273910522 Time taken: 0.29059839248657227\n",
            "Batch Number: 3101 Loss: 1.6141982078552246 Time taken: 0.2892909049987793\n",
            "Batch Number: 3102 Loss: 1.590277075767517 Time taken: 0.3021738529205322\n",
            "Batch Number: 3103 Loss: 1.598291039466858 Time taken: 0.3319435119628906\n",
            "Batch Number: 3104 Loss: 1.5835731029510498 Time taken: 0.3099989891052246\n",
            "Batch Number: 3105 Loss: 1.5915707349777222 Time taken: 0.2991485595703125\n",
            "Batch Number: 3106 Loss: 1.5807147026062012 Time taken: 0.30315589904785156\n",
            "Batch Number: 3107 Loss: 1.5922558307647705 Time taken: 0.2946450710296631\n",
            "Batch Number: 3108 Loss: 1.588430404663086 Time taken: 0.3127713203430176\n",
            "Batch Number: 3109 Loss: 1.574101448059082 Time taken: 0.30448341369628906\n",
            "Batch Number: 3110 Loss: 1.5794495344161987 Time taken: 0.31780076026916504\n",
            "Batch Number: 3111 Loss: 1.5835344791412354 Time taken: 0.3642728328704834\n",
            "Batch Number: 3112 Loss: 1.594075083732605 Time taken: 0.3751795291900635\n",
            "Batch Number: 3113 Loss: 1.568300485610962 Time taken: 0.322310209274292\n",
            "Batch Number: 3114 Loss: 1.5604913234710693 Time taken: 0.3709132671356201\n",
            "Batch Number: 3115 Loss: 1.5727647542953491 Time taken: 0.3833765983581543\n",
            "Batch Number: 3116 Loss: 1.5986071825027466 Time taken: 0.31621503829956055\n",
            "Batch Number: 3117 Loss: 1.5778529644012451 Time taken: 0.2959742546081543\n",
            "Batch Number: 3118 Loss: 1.5778487920761108 Time taken: 0.2949504852294922\n",
            "Batch Number: 3119 Loss: 1.5785092115402222 Time taken: 0.2901909351348877\n",
            "Batch Number: 3120 Loss: 1.5874860286712646 Time taken: 0.31036853790283203\n",
            "Batch Number: 3121 Loss: 1.5684279203414917 Time taken: 0.3736405372619629\n",
            "Batch Number: 3122 Loss: 1.5859529972076416 Time taken: 0.3405177593231201\n",
            "Batch Number: 3123 Loss: 1.5770434141159058 Time taken: 0.36429738998413086\n",
            "Batch Number: 3124 Loss: 1.5785704851150513 Time taken: 0.3715784549713135\n",
            "Batch Number: 3125 Loss: 1.606827735900879 Time taken: 0.2951505184173584\n",
            "Batch Number: 3126 Loss: 1.5864888429641724 Time taken: 0.3185694217681885\n",
            "Batch Number: 3127 Loss: 1.5864393711090088 Time taken: 0.3080730438232422\n",
            "Batch Number: 3128 Loss: 1.597137451171875 Time taken: 0.3558375835418701\n",
            "Batch Number: 3129 Loss: 1.5834695100784302 Time taken: 0.37220335006713867\n",
            "Batch Number: 3130 Loss: 1.6098299026489258 Time taken: 0.3515334129333496\n",
            "Batch Number: 3131 Loss: 1.5856927633285522 Time taken: 0.2932605743408203\n",
            "Batch Number: 3132 Loss: 1.6106541156768799 Time taken: 0.30283451080322266\n",
            "Batch Number: 3133 Loss: 1.6166857481002808 Time taken: 0.29866528511047363\n",
            "Batch Number: 3134 Loss: 1.6081713438034058 Time taken: 0.29987311363220215\n",
            "Batch Number: 3135 Loss: 1.6297520399093628 Time taken: 0.2868363857269287\n",
            "Batch Number: 3136 Loss: 1.6145384311676025 Time taken: 0.29390716552734375\n",
            "Batch Number: 3137 Loss: 1.6073200702667236 Time taken: 0.30985045433044434\n",
            "Batch Number: 3138 Loss: 1.5940649509429932 Time taken: 0.2866814136505127\n",
            "Batch Number: 3139 Loss: 1.5947870016098022 Time taken: 0.29962825775146484\n",
            "Batch Number: 3140 Loss: 1.5835376977920532 Time taken: 0.3296627998352051\n",
            "Batch Number: 3141 Loss: 1.5751540660858154 Time taken: 0.28644442558288574\n",
            "Batch Number: 3142 Loss: 1.5956119298934937 Time taken: 0.3048703670501709\n",
            "Batch Number: 3143 Loss: 1.585219144821167 Time taken: 0.31237149238586426\n",
            "Batch Number: 3144 Loss: 1.5422712564468384 Time taken: 0.28916311264038086\n",
            "Batch Number: 3145 Loss: 1.5831013917922974 Time taken: 0.3013172149658203\n",
            "Batch Number: 3146 Loss: 1.5439484119415283 Time taken: 0.3042261600494385\n",
            "Batch Number: 3147 Loss: 1.5687642097473145 Time taken: 0.3157029151916504\n",
            "Batch Number: 3148 Loss: 1.5688533782958984 Time taken: 0.2958643436431885\n",
            "Batch Number: 3149 Loss: 1.5818263292312622 Time taken: 0.282412052154541\n",
            "Batch Number: 3150 Loss: 1.568090558052063 Time taken: 0.3065979480743408\n",
            "Batch Number: 3151 Loss: 1.5804433822631836 Time taken: 0.29908013343811035\n",
            "Batch Number: 3152 Loss: 1.5961551666259766 Time taken: 0.3106386661529541\n",
            "Batch Number: 3153 Loss: 1.5920195579528809 Time taken: 0.29759931564331055\n",
            "Batch Number: 3154 Loss: 1.5991162061691284 Time taken: 0.28851747512817383\n",
            "Batch Number: 3155 Loss: 1.6239397525787354 Time taken: 0.292452335357666\n",
            "Batch Number: 3156 Loss: 1.6300047636032104 Time taken: 0.30217432975769043\n",
            "Batch Number: 3157 Loss: 1.5988061428070068 Time taken: 0.3090810775756836\n",
            "Batch Number: 3158 Loss: 1.6277920007705688 Time taken: 0.2937614917755127\n",
            "Batch Number: 3159 Loss: 1.5771563053131104 Time taken: 0.2756965160369873\n",
            "Batch Number: 3160 Loss: 1.6112369298934937 Time taken: 0.4102442264556885\n",
            "Batch Number: 3161 Loss: 1.5887168645858765 Time taken: 0.3766140937805176\n",
            "Batch Number: 3162 Loss: 1.626275897026062 Time taken: 0.3419923782348633\n",
            "Batch Number: 3163 Loss: 1.6088932752609253 Time taken: 0.30896925926208496\n",
            "Batch Number: 3164 Loss: 1.609241008758545 Time taken: 0.2901442050933838\n",
            "Batch Number: 3165 Loss: 1.6284363269805908 Time taken: 0.2761712074279785\n",
            "Batch Number: 3166 Loss: 1.616342544555664 Time taken: 0.29616403579711914\n",
            "Batch Number: 3167 Loss: 1.6433439254760742 Time taken: 0.2853739261627197\n",
            "Batch Number: 3168 Loss: 1.6256787776947021 Time taken: 0.3004889488220215\n",
            "Batch Number: 3169 Loss: 1.6392834186553955 Time taken: 0.3021268844604492\n",
            "Batch Number: 3170 Loss: 1.6313272714614868 Time taken: 0.3065500259399414\n",
            "Batch Number: 3171 Loss: 1.648700475692749 Time taken: 0.29584813117980957\n",
            "Batch Number: 3172 Loss: 1.615883469581604 Time taken: 0.299694299697876\n",
            "Batch Number: 3173 Loss: 1.615709662437439 Time taken: 0.31266307830810547\n",
            "Batch Number: 3174 Loss: 1.6364666223526 Time taken: 0.3026266098022461\n",
            "Batch Number: 3175 Loss: 1.6305897235870361 Time taken: 0.2934439182281494\n",
            "Batch Number: 3176 Loss: 1.5837525129318237 Time taken: 0.2957477569580078\n",
            "Batch Number: 3177 Loss: 1.6234568357467651 Time taken: 0.29631543159484863\n",
            "Batch Number: 3178 Loss: 1.6023074388504028 Time taken: 0.29720568656921387\n",
            "Batch Number: 3179 Loss: 1.6097631454467773 Time taken: 0.30952930450439453\n",
            "Batch Number: 3180 Loss: 1.6153982877731323 Time taken: 0.30028510093688965\n",
            "Batch Number: 3181 Loss: 1.643535852432251 Time taken: 0.29363131523132324\n",
            "Batch Number: 3182 Loss: 1.603134274482727 Time taken: 0.31462574005126953\n",
            "Batch Number: 3183 Loss: 1.6367385387420654 Time taken: 0.2982628345489502\n",
            "Batch Number: 3184 Loss: 1.5767818689346313 Time taken: 0.29891228675842285\n",
            "Batch Number: 3185 Loss: 1.587119698524475 Time taken: 0.33554649353027344\n",
            "Batch Number: 3186 Loss: 1.5981690883636475 Time taken: 0.3050577640533447\n",
            "Batch Number: 3187 Loss: 1.6051344871520996 Time taken: 0.29707980155944824\n",
            "Batch Number: 3188 Loss: 1.612829327583313 Time taken: 0.30943775177001953\n",
            "Batch Number: 3189 Loss: 1.6146830320358276 Time taken: 0.30158257484436035\n",
            "Batch Number: 3190 Loss: 1.6137230396270752 Time taken: 0.3027677536010742\n",
            "Batch Number: 3191 Loss: 1.614996075630188 Time taken: 0.30769991874694824\n",
            "Batch Number: 3192 Loss: 1.6079868078231812 Time taken: 0.32631516456604004\n",
            "Batch Number: 3193 Loss: 1.5943489074707031 Time taken: 0.31469106674194336\n",
            "Batch Number: 3194 Loss: 1.5798879861831665 Time taken: 0.32959413528442383\n",
            "Batch Number: 3195 Loss: 1.5585497617721558 Time taken: 0.327725887298584\n",
            "Batch Number: 3196 Loss: 1.5782833099365234 Time taken: 0.30628180503845215\n",
            "Batch Number: 3197 Loss: 1.5898990631103516 Time taken: 0.2999911308288574\n",
            "Batch Number: 3198 Loss: 1.5666478872299194 Time taken: 0.3241751194000244\n",
            "Batch Number: 3199 Loss: 1.5724451541900635 Time taken: 0.31642866134643555\n",
            "Batch Number: 3200 Loss: 1.612317442893982 Time taken: 0.3023402690887451\n",
            "Batch Number: 3201 Loss: 1.628102421760559 Time taken: 0.3184390068054199\n",
            "Batch Number: 3202 Loss: 1.6169788837432861 Time taken: 0.31044602394104004\n",
            "Batch Number: 3203 Loss: 1.6194828748703003 Time taken: 0.36830925941467285\n",
            "Batch Number: 3204 Loss: 1.6013154983520508 Time taken: 0.34339475631713867\n",
            "Batch Number: 3205 Loss: 1.5877728462219238 Time taken: 0.3783915042877197\n",
            "Batch Number: 3206 Loss: 1.6362634897232056 Time taken: 0.3623688220977783\n",
            "Batch Number: 3207 Loss: 1.5599331855773926 Time taken: 0.2943744659423828\n",
            "Batch Number: 3208 Loss: 1.552751898765564 Time taken: 0.29571104049682617\n",
            "Batch Number: 3209 Loss: 1.6091690063476562 Time taken: 0.2940695285797119\n",
            "Batch Number: 3210 Loss: 1.5773731470108032 Time taken: 0.2821078300476074\n",
            "Batch Number: 3211 Loss: 1.605847716331482 Time taken: 0.28972935676574707\n",
            "Batch Number: 3212 Loss: 1.589658498764038 Time taken: 0.30420899391174316\n",
            "Batch Number: 3213 Loss: 1.586235761642456 Time taken: 0.30950307846069336\n",
            "Batch Number: 3214 Loss: 1.615478515625 Time taken: 0.3017544746398926\n",
            "Batch Number: 3215 Loss: 1.605568766593933 Time taken: 0.3107309341430664\n",
            "Batch Number: 3216 Loss: 1.5842509269714355 Time taken: 0.30325937271118164\n",
            "Batch Number: 3217 Loss: 1.6113373041152954 Time taken: 0.336317777633667\n",
            "Batch Number: 3218 Loss: 1.5942370891571045 Time taken: 0.3754401206970215\n",
            "Batch Number: 3219 Loss: 1.5972744226455688 Time taken: 0.3610062599182129\n",
            "Batch Number: 3220 Loss: 1.5795727968215942 Time taken: 0.2843313217163086\n",
            "Batch Number: 3221 Loss: 1.5932567119598389 Time taken: 0.3016629219055176\n",
            "Batch Number: 3222 Loss: 1.5915335416793823 Time taken: 0.3397808074951172\n",
            "Batch Number: 3223 Loss: 1.6184478998184204 Time taken: 0.30126285552978516\n",
            "Batch Number: 3224 Loss: 1.5858025550842285 Time taken: 0.30116891860961914\n",
            "Batch Number: 3225 Loss: 1.6041135787963867 Time taken: 0.2953479290008545\n",
            "Batch Number: 3226 Loss: 1.5932198762893677 Time taken: 0.31200456619262695\n",
            "Batch Number: 3227 Loss: 1.6198943853378296 Time taken: 0.3250558376312256\n",
            "Batch Number: 3228 Loss: 1.6253565549850464 Time taken: 0.3263823986053467\n",
            "Batch Number: 3229 Loss: 1.6182053089141846 Time taken: 0.368366003036499\n",
            "Batch Number: 3230 Loss: 1.5982578992843628 Time taken: 0.382779598236084\n",
            "Batch Number: 3231 Loss: 1.6185417175292969 Time taken: 0.3657960891723633\n",
            "Batch Number: 3232 Loss: 1.6181586980819702 Time taken: 0.2996668815612793\n",
            "Batch Number: 3233 Loss: 1.602973222732544 Time taken: 0.30501461029052734\n",
            "Batch Number: 3234 Loss: 1.595456838607788 Time taken: 0.30013203620910645\n",
            "Batch Number: 3235 Loss: 1.5785408020019531 Time taken: 0.3107423782348633\n",
            "Batch Number: 3236 Loss: 1.5801968574523926 Time taken: 0.29814958572387695\n",
            "Batch Number: 3237 Loss: 1.5969398021697998 Time taken: 0.3103752136230469\n",
            "Batch Number: 3238 Loss: 1.5850762128829956 Time taken: 0.3025493621826172\n",
            "Batch Number: 3239 Loss: 1.5775692462921143 Time taken: 0.2935056686401367\n",
            "Batch Number: 3240 Loss: 1.5706532001495361 Time taken: 0.3000659942626953\n",
            "Batch Number: 3241 Loss: 1.5866189002990723 Time taken: 0.3020663261413574\n",
            "Batch Number: 3242 Loss: 1.5710327625274658 Time taken: 0.3117406368255615\n",
            "Batch Number: 3243 Loss: 1.575481653213501 Time taken: 0.3224668502807617\n",
            "Batch Number: 3244 Loss: 1.5478225946426392 Time taken: 0.3108394145965576\n",
            "Batch Number: 3245 Loss: 1.545058012008667 Time taken: 0.3005077838897705\n",
            "Batch Number: 3246 Loss: 1.54140043258667 Time taken: 0.3023679256439209\n",
            "Batch Number: 3247 Loss: 1.563425898551941 Time taken: 0.30482029914855957\n",
            "Batch Number: 3248 Loss: 1.566895604133606 Time taken: 0.2894284725189209\n",
            "Batch Number: 3249 Loss: 1.5581122636795044 Time taken: 0.2879617214202881\n",
            "Batch Number: 3250 Loss: 1.5530807971954346 Time taken: 0.29593682289123535\n",
            "Batch Number: 3251 Loss: 1.5384646654129028 Time taken: 0.3020143508911133\n",
            "Batch Number: 3252 Loss: 1.5863182544708252 Time taken: 0.2843170166015625\n",
            "Batch Number: 3253 Loss: 1.5888245105743408 Time taken: 0.3529036045074463\n",
            "Batch Number: 3254 Loss: 1.5974376201629639 Time taken: 0.38857197761535645\n",
            "Batch Number: 3255 Loss: 1.5612001419067383 Time taken: 0.3755326271057129\n",
            "Batch Number: 3256 Loss: 1.5704647302627563 Time taken: 0.3655362129211426\n",
            "Batch Number: 3257 Loss: 1.5814542770385742 Time taken: 0.3060038089752197\n",
            "Batch Number: 3258 Loss: 1.5691639184951782 Time taken: 0.3671250343322754\n",
            "Batch Number: 3259 Loss: 1.5543822050094604 Time taken: 0.38976383209228516\n",
            "Batch Number: 3260 Loss: 1.5456790924072266 Time taken: 0.29747891426086426\n",
            "Batch Number: 3261 Loss: 1.5279353857040405 Time taken: 0.3110370635986328\n",
            "Batch Number: 3262 Loss: 1.5569909811019897 Time taken: 0.31355905532836914\n",
            "Batch Number: 3263 Loss: 1.556807041168213 Time taken: 0.2984433174133301\n",
            "Batch Number: 3264 Loss: 1.5776371955871582 Time taken: 0.29923129081726074\n",
            "Batch Number: 3265 Loss: 1.5768250226974487 Time taken: 0.32737302780151367\n",
            "Batch Number: 3266 Loss: 1.5602760314941406 Time taken: 0.3005242347717285\n",
            "Batch Number: 3267 Loss: 1.5688587427139282 Time taken: 0.30118322372436523\n",
            "Batch Number: 3268 Loss: 1.5682870149612427 Time taken: 0.3047146797180176\n",
            "Batch Number: 3269 Loss: 1.566694736480713 Time taken: 0.3091108798980713\n",
            "Batch Number: 3270 Loss: 1.5498988628387451 Time taken: 0.306394100189209\n",
            "Batch Number: 3271 Loss: 1.5587900876998901 Time taken: 0.2932295799255371\n",
            "Batch Number: 3272 Loss: 1.6078191995620728 Time taken: 0.3853631019592285\n",
            "Batch Number: 3273 Loss: 1.5412559509277344 Time taken: 0.36894965171813965\n",
            "Batch Number: 3274 Loss: 1.577435851097107 Time taken: 0.36141085624694824\n",
            "Batch Number: 3275 Loss: 1.56032395362854 Time taken: 0.2895197868347168\n",
            "Batch Number: 3276 Loss: 1.5777201652526855 Time taken: 0.295745849609375\n",
            "Batch Number: 3277 Loss: 1.5859870910644531 Time taken: 0.2946639060974121\n",
            "Batch Number: 3278 Loss: 1.6044082641601562 Time taken: 0.34040307998657227\n",
            "Batch Number: 3279 Loss: 1.5897103548049927 Time taken: 0.2969961166381836\n",
            "Batch Number: 3280 Loss: 1.5976649522781372 Time taken: 0.35405421257019043\n",
            "Batch Number: 3281 Loss: 1.6232786178588867 Time taken: 0.32896900177001953\n",
            "Batch Number: 3282 Loss: 1.5519973039627075 Time taken: 0.2953178882598877\n",
            "Batch Number: 3283 Loss: 1.596239686012268 Time taken: 0.36385416984558105\n",
            "Batch Number: 3284 Loss: 1.5758965015411377 Time taken: 0.3892381191253662\n",
            "Batch Number: 3285 Loss: 1.585093379020691 Time taken: 0.36863040924072266\n",
            "Batch Number: 3286 Loss: 1.5314791202545166 Time taken: 0.3486592769622803\n",
            "Batch Number: 3287 Loss: 1.5776185989379883 Time taken: 0.3205230236053467\n",
            "Batch Number: 3288 Loss: 1.5792230367660522 Time taken: 0.3023707866668701\n",
            "Batch Number: 3289 Loss: 1.5487092733383179 Time taken: 0.38153576850891113\n",
            "Batch Number: 3290 Loss: 1.5763753652572632 Time taken: 0.34995532035827637\n",
            "Batch Number: 3291 Loss: 1.5779980421066284 Time taken: 0.40099239349365234\n",
            "Batch Number: 3292 Loss: 1.5871115922927856 Time taken: 0.3275129795074463\n",
            "Batch Number: 3293 Loss: 1.5533422231674194 Time taken: 0.30863094329833984\n",
            "Batch Number: 3294 Loss: 1.5469413995742798 Time taken: 0.30051374435424805\n",
            "Batch Number: 3295 Loss: 1.5441327095031738 Time taken: 0.3020961284637451\n",
            "Batch Number: 3296 Loss: 1.5711873769760132 Time taken: 0.31012773513793945\n",
            "Batch Number: 3297 Loss: 1.568921446800232 Time taken: 0.286027193069458\n",
            "Batch Number: 3298 Loss: 1.5514514446258545 Time taken: 0.3023078441619873\n",
            "Batch Number: 3299 Loss: 1.5483022928237915 Time taken: 0.3113570213317871\n",
            "Batch Number: 3300 Loss: 1.5622836351394653 Time taken: 0.2974538803100586\n",
            "Batch Number: 3301 Loss: 1.568583607673645 Time taken: 0.29813098907470703\n",
            "Batch Number: 3302 Loss: 1.5541402101516724 Time taken: 0.31439876556396484\n",
            "Batch Number: 3303 Loss: 1.5290902853012085 Time taken: 0.2940645217895508\n",
            "Batch Number: 3304 Loss: 1.562721610069275 Time taken: 0.28572607040405273\n",
            "Batch Number: 3305 Loss: 1.5620137453079224 Time taken: 0.3023862838745117\n",
            "Batch Number: 3306 Loss: 1.5537118911743164 Time taken: 0.3017861843109131\n",
            "Batch Number: 3307 Loss: 1.584883451461792 Time taken: 0.30231261253356934\n",
            "Batch Number: 3308 Loss: 1.5429812669754028 Time taken: 0.2994093894958496\n",
            "Batch Number: 3309 Loss: 1.549570083618164 Time taken: 0.31371569633483887\n",
            "Batch Number: 3310 Loss: 1.561187744140625 Time taken: 0.3088717460632324\n",
            "Batch Number: 3311 Loss: 1.5936862230300903 Time taken: 0.30463671684265137\n",
            "Batch Number: 3312 Loss: 1.5801048278808594 Time taken: 0.38696813583374023\n",
            "Batch Number: 3313 Loss: 1.578533411026001 Time taken: 0.3663654327392578\n",
            "Batch Number: 3314 Loss: 1.5878510475158691 Time taken: 0.3498985767364502\n",
            "Batch Number: 3315 Loss: 1.5718587636947632 Time taken: 0.3159182071685791\n",
            "Batch Number: 3316 Loss: 1.5993343591690063 Time taken: 0.30367064476013184\n",
            "Batch Number: 3317 Loss: 1.5740844011306763 Time taken: 0.30562448501586914\n",
            "Batch Number: 3318 Loss: 1.5724025964736938 Time taken: 0.3177173137664795\n",
            "Batch Number: 3319 Loss: 1.5795336961746216 Time taken: 0.2902407646179199\n",
            "Batch Number: 3320 Loss: 1.5527820587158203 Time taken: 0.301804780960083\n",
            "Batch Number: 3321 Loss: 1.5806527137756348 Time taken: 0.3059055805206299\n",
            "Batch Number: 3322 Loss: 1.5810532569885254 Time taken: 0.2933473587036133\n",
            "Batch Number: 3323 Loss: 1.5778480768203735 Time taken: 0.298844575881958\n",
            "Batch Number: 3324 Loss: 1.521155595779419 Time taken: 0.32895946502685547\n",
            "Batch Number: 3325 Loss: 1.5733435153961182 Time taken: 0.3160984516143799\n",
            "Batch Number: 3326 Loss: 1.5713502168655396 Time taken: 0.303333044052124\n",
            "Batch Number: 3327 Loss: 1.5770105123519897 Time taken: 0.34056806564331055\n",
            "Batch Number: 3328 Loss: 1.5786348581314087 Time taken: 0.40396785736083984\n",
            "Batch Number: 3329 Loss: 1.571686863899231 Time taken: 0.3576793670654297\n",
            "Batch Number: 3330 Loss: 1.59010648727417 Time taken: 0.3224520683288574\n",
            "Batch Number: 3331 Loss: 1.5863364934921265 Time taken: 0.29822707176208496\n",
            "Batch Number: 3332 Loss: 1.5991528034210205 Time taken: 0.31383514404296875\n",
            "Batch Number: 3333 Loss: 1.5898363590240479 Time taken: 0.30616140365600586\n",
            "Batch Number: 3334 Loss: 1.5674102306365967 Time taken: 0.33278870582580566\n",
            "Batch Number: 3335 Loss: 1.5849494934082031 Time taken: 0.3164980411529541\n",
            "Batch Number: 3336 Loss: 1.6142606735229492 Time taken: 0.30126094818115234\n",
            "Batch Number: 3337 Loss: 1.5903496742248535 Time taken: 0.32173919677734375\n",
            "Batch Number: 3338 Loss: 1.617775559425354 Time taken: 0.2912740707397461\n",
            "Batch Number: 3339 Loss: 1.606095790863037 Time taken: 0.31214189529418945\n",
            "Batch Number: 3340 Loss: 1.589321255683899 Time taken: 0.31592345237731934\n",
            "Batch Number: 3341 Loss: 1.5970090627670288 Time taken: 0.2923293113708496\n",
            "Batch Number: 3342 Loss: 1.59151291847229 Time taken: 0.3227360248565674\n",
            "Batch Number: 3343 Loss: 1.599539041519165 Time taken: 0.3178529739379883\n",
            "Batch Number: 3344 Loss: 1.5982849597930908 Time taken: 0.3053414821624756\n",
            "Batch Number: 3345 Loss: 1.6102604866027832 Time taken: 0.31397271156311035\n",
            "Batch Number: 3346 Loss: 1.6487476825714111 Time taken: 0.3116140365600586\n",
            "Batch Number: 3347 Loss: 1.607759714126587 Time taken: 0.3151743412017822\n",
            "Batch Number: 3348 Loss: 1.6081260442733765 Time taken: 0.2962162494659424\n",
            "Batch Number: 3349 Loss: 1.6287906169891357 Time taken: 0.3107461929321289\n",
            "Batch Number: 3350 Loss: 1.617097020149231 Time taken: 0.3129708766937256\n",
            "Batch Number: 3351 Loss: 1.6181628704071045 Time taken: 0.29779720306396484\n",
            "Batch Number: 3352 Loss: 1.6020727157592773 Time taken: 0.2891731262207031\n",
            "Batch Number: 3353 Loss: 1.605013132095337 Time taken: 0.31926584243774414\n",
            "Batch Number: 3354 Loss: 1.6236486434936523 Time taken: 0.2956974506378174\n",
            "Batch Number: 3355 Loss: 1.5846107006072998 Time taken: 0.3024265766143799\n",
            "Batch Number: 3356 Loss: 1.5967583656311035 Time taken: 0.31511521339416504\n",
            "Batch Number: 3357 Loss: 1.582257866859436 Time taken: 0.31864380836486816\n",
            "Batch Number: 3358 Loss: 1.5940310955047607 Time taken: 0.30980920791625977\n",
            "Batch Number: 3359 Loss: 1.584386944770813 Time taken: 0.30153512954711914\n",
            "Batch Number: 3360 Loss: 1.5922611951828003 Time taken: 0.3153975009918213\n",
            "Batch Number: 3361 Loss: 1.587839961051941 Time taken: 0.30861902236938477\n",
            "Batch Number: 3362 Loss: 1.6003278493881226 Time taken: 0.3034965991973877\n",
            "Batch Number: 3363 Loss: 1.5860224962234497 Time taken: 0.32706546783447266\n",
            "Batch Number: 3364 Loss: 1.5791442394256592 Time taken: 0.346005916595459\n",
            "Batch Number: 3365 Loss: 1.5834530591964722 Time taken: 0.29022717475891113\n",
            "Batch Number: 3366 Loss: 1.5708377361297607 Time taken: 0.3042275905609131\n",
            "Batch Number: 3367 Loss: 1.5696625709533691 Time taken: 0.30872011184692383\n",
            "Batch Number: 3368 Loss: 1.559869647026062 Time taken: 0.2885136604309082\n",
            "Batch Number: 3369 Loss: 1.5592234134674072 Time taken: 0.30693650245666504\n",
            "Batch Number: 3370 Loss: 1.574164628982544 Time taken: 0.3773822784423828\n",
            "Batch Number: 3371 Loss: 1.5909004211425781 Time taken: 0.32861948013305664\n",
            "Batch Number: 3372 Loss: 1.5680023431777954 Time taken: 0.31784558296203613\n",
            "Batch Number: 3373 Loss: 1.577089548110962 Time taken: 0.33063244819641113\n",
            "Batch Number: 3374 Loss: 1.5733263492584229 Time taken: 0.34768128395080566\n",
            "Batch Number: 3375 Loss: 1.5770900249481201 Time taken: 0.3921828269958496\n",
            "Batch Number: 3376 Loss: 1.5669502019882202 Time taken: 0.3109285831451416\n",
            "Batch Number: 3377 Loss: 1.5448930263519287 Time taken: 0.2989058494567871\n",
            "Batch Number: 3378 Loss: 1.5371019840240479 Time taken: 0.36114978790283203\n",
            "Batch Number: 3379 Loss: 1.5680011510849 Time taken: 0.32877635955810547\n",
            "Batch Number: 3380 Loss: 1.5942643880844116 Time taken: 0.29300880432128906\n",
            "Batch Number: 3381 Loss: 1.570816159248352 Time taken: 0.31253671646118164\n",
            "Batch Number: 3382 Loss: 1.6150999069213867 Time taken: 0.30562710762023926\n",
            "Batch Number: 3383 Loss: 1.6071134805679321 Time taken: 0.2978041172027588\n",
            "Batch Number: 3384 Loss: 1.5750577449798584 Time taken: 0.30085325241088867\n",
            "Batch Number: 3385 Loss: 1.5756981372833252 Time taken: 0.3106689453125\n",
            "Batch Number: 3386 Loss: 1.5914403200149536 Time taken: 0.2878403663635254\n",
            "Batch Number: 3387 Loss: 1.5639106035232544 Time taken: 0.2886056900024414\n",
            "Batch Number: 3388 Loss: 1.5471277236938477 Time taken: 0.30588650703430176\n",
            "Batch Number: 3389 Loss: 1.5852528810501099 Time taken: 0.30741429328918457\n",
            "Batch Number: 3390 Loss: 1.5875707864761353 Time taken: 0.32298946380615234\n",
            "Batch Number: 3391 Loss: 1.5785382986068726 Time taken: 0.32914185523986816\n",
            "Batch Number: 3392 Loss: 1.5976896286010742 Time taken: 0.32953763008117676\n",
            "Batch Number: 3393 Loss: 1.5720024108886719 Time taken: 0.29514646530151367\n",
            "Batch Number: 3394 Loss: 1.5892189741134644 Time taken: 0.32104992866516113\n",
            "Batch Number: 3395 Loss: 1.5824573040008545 Time taken: 0.3291754722595215\n",
            "Batch Number: 3396 Loss: 1.6026625633239746 Time taken: 0.30670905113220215\n",
            "Batch Number: 3397 Loss: 1.5944247245788574 Time taken: 0.3036649227142334\n",
            "Batch Number: 3398 Loss: 1.6194344758987427 Time taken: 0.31264519691467285\n",
            "Batch Number: 3399 Loss: 1.6173348426818848 Time taken: 0.298753023147583\n",
            "Batch Number: 3400 Loss: 1.5891625881195068 Time taken: 0.28713059425354004\n",
            "Batch Number: 3401 Loss: 1.5813828706741333 Time taken: 0.3192446231842041\n",
            "Batch Number: 3402 Loss: 1.6133830547332764 Time taken: 0.3014945983886719\n",
            "Batch Number: 3403 Loss: 1.5834563970565796 Time taken: 0.3096182346343994\n",
            "Batch Number: 3404 Loss: 1.5753589868545532 Time taken: 0.31748366355895996\n",
            "Batch Number: 3405 Loss: 1.5747113227844238 Time taken: 0.29038453102111816\n",
            "Batch Number: 3406 Loss: 1.5828922986984253 Time taken: 0.30133819580078125\n",
            "Batch Number: 3407 Loss: 1.5639231204986572 Time taken: 0.2844252586364746\n",
            "Batch Number: 3408 Loss: 1.56446373462677 Time taken: 0.3065938949584961\n",
            "Batch Number: 3409 Loss: 1.5404493808746338 Time taken: 0.29726696014404297\n",
            "Batch Number: 3410 Loss: 1.5704903602600098 Time taken: 0.29749226570129395\n",
            "Batch Number: 3411 Loss: 1.583764910697937 Time taken: 0.3116767406463623\n",
            "Batch Number: 3412 Loss: 1.5852118730545044 Time taken: 0.2984042167663574\n",
            "Batch Number: 3413 Loss: 1.5522743463516235 Time taken: 0.2874932289123535\n",
            "Batch Number: 3414 Loss: 1.5717809200286865 Time taken: 0.30744242668151855\n",
            "Batch Number: 3415 Loss: 1.5649538040161133 Time taken: 0.30246615409851074\n",
            "Batch Number: 3416 Loss: 1.5529128313064575 Time taken: 0.2992591857910156\n",
            "Batch Number: 3417 Loss: 1.5830734968185425 Time taken: 0.31610727310180664\n",
            "Batch Number: 3418 Loss: 1.5723567008972168 Time taken: 0.32031941413879395\n",
            "Batch Number: 3419 Loss: 1.5398526191711426 Time taken: 0.2974865436553955\n",
            "Batch Number: 3420 Loss: 1.5686404705047607 Time taken: 0.29386186599731445\n",
            "Batch Number: 3421 Loss: 1.5620524883270264 Time taken: 0.3126842975616455\n",
            "Batch Number: 3422 Loss: 1.5652979612350464 Time taken: 0.33867883682250977\n",
            "Batch Number: 3423 Loss: 1.5415444374084473 Time taken: 0.3638432025909424\n",
            "Batch Number: 3424 Loss: 1.5344690084457397 Time taken: 0.3287482261657715\n",
            "Batch Number: 3425 Loss: 1.5551793575286865 Time taken: 0.29680418968200684\n",
            "Batch Number: 3426 Loss: 1.536428689956665 Time taken: 0.2885396480560303\n",
            "Batch Number: 3427 Loss: 1.5247045755386353 Time taken: 0.3568432331085205\n",
            "Batch Number: 3428 Loss: 1.549965262413025 Time taken: 0.2940025329589844\n",
            "Batch Number: 3429 Loss: 1.5300027132034302 Time taken: 0.30680131912231445\n",
            "Batch Number: 3430 Loss: 1.5286223888397217 Time taken: 0.33937764167785645\n",
            "Batch Number: 3431 Loss: 1.534807562828064 Time taken: 0.3675217628479004\n",
            "Batch Number: 3432 Loss: 1.5247228145599365 Time taken: 0.372150182723999\n",
            "Batch Number: 3433 Loss: 1.556288719177246 Time taken: 0.37787842750549316\n",
            "Batch Number: 3434 Loss: 1.6070791482925415 Time taken: 0.3716557025909424\n",
            "Batch Number: 3435 Loss: 1.5864160060882568 Time taken: 0.3401353359222412\n",
            "Batch Number: 3436 Loss: 1.5701757669448853 Time taken: 0.308992862701416\n",
            "Batch Number: 3437 Loss: 1.5562996864318848 Time taken: 0.37166476249694824\n",
            "Batch Number: 3438 Loss: 1.5751326084136963 Time taken: 0.3398861885070801\n",
            "Batch Number: 3439 Loss: 1.5557067394256592 Time taken: 0.3007175922393799\n",
            "Batch Number: 3440 Loss: 1.5566009283065796 Time taken: 0.2922689914703369\n",
            "Batch Number: 3441 Loss: 1.578194260597229 Time taken: 0.29261088371276855\n",
            "Batch Number: 3442 Loss: 1.5505032539367676 Time taken: 0.37963271141052246\n",
            "Batch Number: 3443 Loss: 1.5725226402282715 Time taken: 0.3731424808502197\n",
            "Batch Number: 3444 Loss: 1.551287293434143 Time taken: 0.36503100395202637\n",
            "Batch Number: 3445 Loss: 1.5545690059661865 Time taken: 0.30519580841064453\n",
            "Batch Number: 3446 Loss: 1.5369058847427368 Time taken: 0.2988395690917969\n",
            "Batch Number: 3447 Loss: 1.5588195323944092 Time taken: 0.2939262390136719\n",
            "Batch Number: 3448 Loss: 1.5537490844726562 Time taken: 0.30226898193359375\n",
            "Batch Number: 3449 Loss: 1.5525184869766235 Time taken: 0.2960364818572998\n",
            "Batch Number: 3450 Loss: 1.5473711490631104 Time taken: 0.30057191848754883\n",
            "Batch Number: 3451 Loss: 1.53899085521698 Time taken: 0.355926513671875\n",
            "Batch Number: 3452 Loss: 1.563642144203186 Time taken: 0.3654813766479492\n",
            "Batch Number: 3453 Loss: 1.5502724647521973 Time taken: 0.35765695571899414\n",
            "Batch Number: 3454 Loss: 1.5588631629943848 Time taken: 0.2869281768798828\n",
            "Batch Number: 3455 Loss: 1.5496081113815308 Time taken: 0.2916078567504883\n",
            "Batch Number: 3456 Loss: 1.561517357826233 Time taken: 0.3183786869049072\n",
            "Batch Number: 3457 Loss: 1.549634337425232 Time taken: 0.30728983879089355\n",
            "Batch Number: 3458 Loss: 1.5750560760498047 Time taken: 0.29060959815979004\n",
            "Batch Number: 3459 Loss: 1.5845890045166016 Time taken: 0.35398030281066895\n",
            "Batch Number: 3460 Loss: 1.5743907690048218 Time taken: 0.3247859477996826\n",
            "Batch Number: 3461 Loss: 1.5837488174438477 Time taken: 0.3003051280975342\n",
            "Batch Number: 3462 Loss: 1.567244529724121 Time taken: 0.295072078704834\n",
            "Batch Number: 3463 Loss: 1.5239253044128418 Time taken: 0.3476581573486328\n",
            "Batch Number: 3464 Loss: 1.5387566089630127 Time taken: 0.37099766731262207\n",
            "Batch Number: 3465 Loss: 1.5731537342071533 Time taken: 0.34548115730285645\n",
            "Batch Number: 3466 Loss: 1.551820993423462 Time taken: 0.3218114376068115\n",
            "Batch Number: 3467 Loss: 1.55609929561615 Time taken: 0.3015146255493164\n",
            "Batch Number: 3468 Loss: 1.5843522548675537 Time taken: 0.2975497245788574\n",
            "Batch Number: 3469 Loss: 1.5725094079971313 Time taken: 0.32303404808044434\n",
            "Batch Number: 3470 Loss: 1.5727473497390747 Time taken: 0.31763672828674316\n",
            "Batch Number: 3471 Loss: 1.5558826923370361 Time taken: 0.2961292266845703\n",
            "Batch Number: 3472 Loss: 1.544877529144287 Time taken: 0.30248212814331055\n",
            "Batch Number: 3473 Loss: 1.5409058332443237 Time taken: 0.3014943599700928\n",
            "Batch Number: 3474 Loss: 1.570300579071045 Time taken: 0.29479408264160156\n",
            "Batch Number: 3475 Loss: 1.516395092010498 Time taken: 0.2956981658935547\n",
            "Batch Number: 3476 Loss: 1.5495579242706299 Time taken: 0.32462596893310547\n",
            "Batch Number: 3477 Loss: 1.559597134590149 Time taken: 0.29581308364868164\n",
            "Batch Number: 3478 Loss: 1.5337433815002441 Time taken: 0.30471205711364746\n",
            "Batch Number: 3479 Loss: 1.5809555053710938 Time taken: 0.34493422508239746\n",
            "Batch Number: 3480 Loss: 1.532462239265442 Time taken: 0.37170910835266113\n",
            "Batch Number: 3481 Loss: 1.545025110244751 Time taken: 0.38171839714050293\n",
            "Batch Number: 3482 Loss: 1.5526063442230225 Time taken: 0.3776557445526123\n",
            "Batch Number: 3483 Loss: 1.5435645580291748 Time taken: 0.35375475883483887\n",
            "Batch Number: 3484 Loss: 1.5491974353790283 Time taken: 0.3005216121673584\n",
            "Batch Number: 3485 Loss: 1.5300451517105103 Time taken: 0.3030354976654053\n",
            "Batch Number: 3486 Loss: 1.5620737075805664 Time taken: 0.29302358627319336\n",
            "Batch Number: 3487 Loss: 1.5527698993682861 Time taken: 0.30666160583496094\n",
            "Batch Number: 3488 Loss: 1.5568240880966187 Time taken: 0.33795666694641113\n",
            "Batch Number: 3489 Loss: 1.5520117282867432 Time taken: 0.303558349609375\n",
            "Batch Number: 3490 Loss: 1.5530402660369873 Time taken: 0.3139989376068115\n",
            "Batch Number: 3491 Loss: 1.579913854598999 Time taken: 0.29191136360168457\n",
            "Batch Number: 3492 Loss: 1.5675331354141235 Time taken: 0.300137996673584\n",
            "Batch Number: 3493 Loss: 1.5825800895690918 Time taken: 0.29008007049560547\n",
            "Batch Number: 3494 Loss: 1.6031006574630737 Time taken: 0.2919650077819824\n",
            "Batch Number: 3495 Loss: 1.5704671144485474 Time taken: 0.29091620445251465\n",
            "Batch Number: 3496 Loss: 1.5549213886260986 Time taken: 0.3018360137939453\n",
            "Batch Number: 3497 Loss: 1.5729352235794067 Time taken: 0.2923898696899414\n",
            "Batch Number: 3498 Loss: 1.5827192068099976 Time taken: 0.3378324508666992\n",
            "Batch Number: 3499 Loss: 1.5827796459197998 Time taken: 0.3836829662322998\n",
            "Batch Number: 3500 Loss: 1.5387662649154663 Time taken: 0.30576300621032715\n",
            "Batch Number: 3501 Loss: 1.5641698837280273 Time taken: 0.28841400146484375\n",
            "Batch Number: 3502 Loss: 1.5591164827346802 Time taken: 0.33448171615600586\n",
            "Batch Number: 3503 Loss: 1.5369477272033691 Time taken: 0.3108036518096924\n",
            "Batch Number: 3504 Loss: 1.5499764680862427 Time taken: 0.3088419437408447\n",
            "Batch Number: 3505 Loss: 1.5328426361083984 Time taken: 0.30595827102661133\n",
            "Batch Number: 3506 Loss: 1.537972331047058 Time taken: 0.2925741672515869\n",
            "Batch Number: 3507 Loss: 1.5462998151779175 Time taken: 0.3057229518890381\n",
            "Batch Number: 3508 Loss: 1.5444843769073486 Time taken: 0.30120134353637695\n",
            "Batch Number: 3509 Loss: 1.5188422203063965 Time taken: 0.2896230220794678\n",
            "Batch Number: 3510 Loss: 1.5345731973648071 Time taken: 0.29653453826904297\n",
            "Batch Number: 3511 Loss: 1.52296781539917 Time taken: 0.2892005443572998\n",
            "Batch Number: 3512 Loss: 1.5495288372039795 Time taken: 0.3138277530670166\n",
            "Batch Number: 3513 Loss: 1.549329161643982 Time taken: 0.316758394241333\n",
            "Batch Number: 3514 Loss: 1.564352035522461 Time taken: 0.3011047840118408\n",
            "Batch Number: 3515 Loss: 1.6106898784637451 Time taken: 0.3372535705566406\n",
            "Batch Number: 3516 Loss: 1.6043325662612915 Time taken: 0.2963564395904541\n",
            "Batch Number: 3517 Loss: 1.613126516342163 Time taken: 0.30416440963745117\n",
            "Batch Number: 3518 Loss: 1.6060512065887451 Time taken: 0.3293313980102539\n",
            "Batch Number: 3519 Loss: 1.5970029830932617 Time taken: 0.29636073112487793\n",
            "Batch Number: 3520 Loss: 1.6108298301696777 Time taken: 0.3045356273651123\n",
            "Batch Number: 3521 Loss: 1.580428957939148 Time taken: 0.35227179527282715\n",
            "Batch Number: 3522 Loss: 1.6067038774490356 Time taken: 0.3231160640716553\n",
            "Batch Number: 3523 Loss: 1.5848790407180786 Time taken: 0.2997140884399414\n",
            "Batch Number: 3524 Loss: 1.5751206874847412 Time taken: 0.30054473876953125\n",
            "Batch Number: 3525 Loss: 1.5809849500656128 Time taken: 0.3301846981048584\n",
            "Batch Number: 3526 Loss: 1.5816510915756226 Time taken: 0.3147883415222168\n",
            "Batch Number: 3527 Loss: 1.6120338439941406 Time taken: 0.3066422939300537\n",
            "Batch Number: 3528 Loss: 1.6103935241699219 Time taken: 0.29569363594055176\n",
            "Batch Number: 3529 Loss: 1.5965379476547241 Time taken: 0.34167027473449707\n",
            "Batch Number: 3530 Loss: 1.6313669681549072 Time taken: 0.38863372802734375\n",
            "Batch Number: 3531 Loss: 1.5800968408584595 Time taken: 0.33399271965026855\n",
            "Batch Number: 3532 Loss: 1.5988849401474 Time taken: 0.30034351348876953\n",
            "Batch Number: 3533 Loss: 1.5934951305389404 Time taken: 0.29799771308898926\n",
            "Batch Number: 3534 Loss: 1.5782759189605713 Time taken: 0.29787564277648926\n",
            "Batch Number: 3535 Loss: 1.581485629081726 Time taken: 0.3009817600250244\n",
            "Batch Number: 3536 Loss: 1.5703706741333008 Time taken: 0.3030369281768799\n",
            "Batch Number: 3537 Loss: 1.5832514762878418 Time taken: 0.32388734817504883\n",
            "Batch Number: 3538 Loss: 1.5479414463043213 Time taken: 0.37421679496765137\n",
            "Batch Number: 3539 Loss: 1.557640552520752 Time taken: 0.37471985816955566\n",
            "Batch Number: 3540 Loss: 1.5953351259231567 Time taken: 0.31905579566955566\n",
            "Batch Number: 3541 Loss: 1.5767138004302979 Time taken: 0.3129246234893799\n",
            "Batch Number: 3542 Loss: 1.606391191482544 Time taken: 0.2988758087158203\n",
            "Batch Number: 3543 Loss: 1.5790529251098633 Time taken: 0.3527567386627197\n",
            "Batch Number: 3544 Loss: 1.5841671228408813 Time taken: 0.29515528678894043\n",
            "Batch Number: 3545 Loss: 1.5944225788116455 Time taken: 0.3009817600250244\n",
            "Batch Number: 3546 Loss: 1.5739244222640991 Time taken: 0.30069756507873535\n",
            "Batch Number: 3547 Loss: 1.5445475578308105 Time taken: 0.34921765327453613\n",
            "Batch Number: 3548 Loss: 1.5720176696777344 Time taken: 0.32515621185302734\n",
            "Batch Number: 3549 Loss: 1.5681911706924438 Time taken: 0.32071852684020996\n",
            "Batch Number: 3550 Loss: 1.5760633945465088 Time taken: 0.3863818645477295\n",
            "Batch Number: 3551 Loss: 1.6108613014221191 Time taken: 0.38435983657836914\n",
            "Batch Number: 3552 Loss: 1.5817089080810547 Time taken: 0.32407212257385254\n",
            "Batch Number: 3553 Loss: 1.5788654088974 Time taken: 0.30472683906555176\n",
            "Batch Number: 3554 Loss: 1.573360562324524 Time taken: 0.2916529178619385\n",
            "Batch Number: 3555 Loss: 1.5504659414291382 Time taken: 0.30083489418029785\n",
            "Batch Number: 3556 Loss: 1.5466383695602417 Time taken: 0.2957310676574707\n",
            "Batch Number: 3557 Loss: 1.5396792888641357 Time taken: 0.30861401557922363\n",
            "Batch Number: 3558 Loss: 1.5436853170394897 Time taken: 0.31885218620300293\n",
            "Batch Number: 3559 Loss: 1.559004545211792 Time taken: 0.2942178249359131\n",
            "Batch Number: 3560 Loss: 1.5638998746871948 Time taken: 0.2965245246887207\n",
            "Batch Number: 3561 Loss: 1.613762378692627 Time taken: 0.2961127758026123\n",
            "Batch Number: 3562 Loss: 1.6208527088165283 Time taken: 0.29863548278808594\n",
            "Batch Number: 3563 Loss: 1.5592211484909058 Time taken: 0.28847599029541016\n",
            "Batch Number: 3564 Loss: 1.5855869054794312 Time taken: 0.2995340824127197\n",
            "Batch Number: 3565 Loss: 1.5638504028320312 Time taken: 0.3379800319671631\n",
            "Batch Number: 3566 Loss: 1.5931148529052734 Time taken: 0.3665292263031006\n",
            "Batch Number: 3567 Loss: 1.5511243343353271 Time taken: 0.36848974227905273\n",
            "Batch Number: 3568 Loss: 1.5194847583770752 Time taken: 0.3385050296783447\n",
            "Batch Number: 3569 Loss: 1.5570522546768188 Time taken: 0.3254356384277344\n",
            "Batch Number: 3570 Loss: 1.5375308990478516 Time taken: 0.34926676750183105\n",
            "Batch Number: 3571 Loss: 1.5384162664413452 Time taken: 0.3040940761566162\n",
            "Batch Number: 3572 Loss: 1.5538610219955444 Time taken: 0.2945437431335449\n",
            "Batch Number: 3573 Loss: 1.575020670890808 Time taken: 0.3022894859313965\n",
            "Batch Number: 3574 Loss: 1.5535657405853271 Time taken: 0.29794836044311523\n",
            "Batch Number: 3575 Loss: 1.568260669708252 Time taken: 0.29757165908813477\n",
            "Batch Number: 3576 Loss: 1.6043322086334229 Time taken: 0.29940319061279297\n",
            "Batch Number: 3577 Loss: 1.5881699323654175 Time taken: 0.28925299644470215\n",
            "Batch Number: 3578 Loss: 1.561240553855896 Time taken: 0.2987089157104492\n",
            "Batch Number: 3579 Loss: 1.574825406074524 Time taken: 0.2948417663574219\n",
            "Batch Number: 3580 Loss: 1.5801953077316284 Time taken: 0.2834348678588867\n",
            "Batch Number: 3581 Loss: 1.5763564109802246 Time taken: 0.30084753036499023\n",
            "Batch Number: 3582 Loss: 1.5724914073944092 Time taken: 0.29577088356018066\n",
            "Batch Number: 3583 Loss: 1.5745999813079834 Time taken: 0.30541062355041504\n",
            "Batch Number: 3584 Loss: 1.5490294694900513 Time taken: 0.31335949897766113\n",
            "Batch Number: 3585 Loss: 1.543408989906311 Time taken: 0.28763341903686523\n",
            "Batch Number: 3586 Loss: 1.5924113988876343 Time taken: 0.2968719005584717\n",
            "Batch Number: 3587 Loss: 1.5508557558059692 Time taken: 0.3319733142852783\n",
            "Batch Number: 3588 Loss: 1.5503263473510742 Time taken: 0.30585455894470215\n",
            "Batch Number: 3589 Loss: 1.5569344758987427 Time taken: 0.3030283451080322\n",
            "Batch Number: 3590 Loss: 1.5539493560791016 Time taken: 0.29711008071899414\n",
            "Batch Number: 3591 Loss: 1.5494426488876343 Time taken: 0.3075447082519531\n",
            "Batch Number: 3592 Loss: 1.5607609748840332 Time taken: 0.2997603416442871\n",
            "Batch Number: 3593 Loss: 1.5482317209243774 Time taken: 0.3139498233795166\n",
            "Batch Number: 3594 Loss: 1.5641440153121948 Time taken: 0.3222496509552002\n",
            "Batch Number: 3595 Loss: 1.5377792119979858 Time taken: 0.29207730293273926\n",
            "Batch Number: 3596 Loss: 1.5517234802246094 Time taken: 0.30941057205200195\n",
            "Batch Number: 3597 Loss: 1.5393681526184082 Time taken: 0.2958667278289795\n",
            "Batch Number: 3598 Loss: 1.6064127683639526 Time taken: 0.3065207004547119\n",
            "Batch Number: 3599 Loss: 1.5871347188949585 Time taken: 0.3071293830871582\n",
            "Batch Number: 3600 Loss: 1.5671985149383545 Time taken: 0.31794166564941406\n",
            "took 1174.2775049209595 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v89jmuiuSX4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c1c7381-31f6-496b-816c-1c830e9ff65c"
      },
      "source": [
        "characters = [0,5,6]\n",
        "print(characters)\n",
        "\n",
        "print(characters[-1:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 5, 6]\n",
            "[6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEbUmejDEDjf",
        "colab_type": "text"
      },
      "source": [
        "#### **Using np.random.choice**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVTjlD5wXAWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ae89eac-50b4-4693-f45c-331f2992bc44"
      },
      "source": [
        "num_generate = 2500\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "#h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "\n",
        "  ## The predictions is a tensor \n",
        "  predictions = predictions.numpy()[0]\n",
        "  #predictions = np.array(predictions)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #print(choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))\n",
        "print(\"=\"*90)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en. Who, ave adow a whole, the weam when yod a gear'l stand it sung, if it will so will dread desires, of is death\n",
            "Is thou fell wear father,\n",
            "But the grave inquest-to it again the baich's surper unto this honouraigs so swon\n",
            "Next be the law and solered do is ne'er rearons and sworn, acch medie: I hear him: loviss wse decelseng my fous for unjmyed, they and oderry aftat of his no magoor tequit fount, thou now defey in his was one mather time should you known, would to pear thy grandat this gown, mansten Duke of seautance as thou art lie to:\n",
            "I'll fable true to heart non misere gave of right\n",
            "May your rejest and of it is He Isterm'd in sead my Pains?\n",
            "Whise is thou not the devil truth. 'sid it adomen's own, hos thr foreless dead as how deverse make ary\n",
            "overades of man, and you as\n",
            "that thus ent his toss in grands 'tis im'ting and constate\n",
            "Iven's as for mine in my\n",
            "Francers.\n",
            "\n",
            "KING JOAN:\n",
            "If you? will be a sufford passion.\n",
            "O hard for a seemirits, by this miscreed.\n",
            "\n",
            "CARDINAL:\n",
            "I am nature and sis brater,\n",
            "nend we heard's metchfence,\n",
            "We'll seen of my seen our handsed blaid row shakins,\n",
            "When all my servant undress infeed\n",
            "The incannot so be must nights to cobsenals I will not kissed, did;\n",
            "But thou she's his youth:\n",
            "The pence is is truer does with kinco,\n",
            "And he is no asms, so comfort scape 's was palfor of by ull wark not that we of accuted as majesty is man can be speak.\n",
            "\n",
            "FABBROTOUS:\n",
            "My very sojeitius souls of her maysith life anversiresk of nomes,\n",
            "Abbatus; and get treme mude again a sect do of earldion of my finesione\n",
            "is dancess ot shipsion?-\n",
            "Fient do.\n",
            "\n",
            "ODRINGA:\n",
            "Why sword speak night,\n",
            "Haim.\n",
            "\n",
            "TIMON:\n",
            "Upon\n",
            "A fait of Princeshalters of Mulichor se on of My that was be hangudeon a roon of rulfuit\n",
            "and honouth, the Ilsies to him encle, to fought of bone.\n",
            "\n",
            "VALBETH:\n",
            "As I have wears, but tforguition'st child,\n",
            "But in.\n",
            "Wheread a blood,\n",
            "And hi'lded in but hero world made fathem tow: ever than savoor does if a bain his lice, are the stomes Unto you\n",
            "courdere soundsmone, seart of sicks of the duke on servicule sevel; there out shall in sid lord.\n",
            "\n",
            "Dun Aspirot on what he did shoot hus ochtone; aloud; if my fine eats: the frown meets\n",
            "And sweary\n",
            "Approes's of prayer in his his caunt.\n",
            "Woy will seem with\n",
            "midefented on her elough of contend'd they to sty care; it\n",
            "Beress is discrector, be house for the enr\n",
            "Ihallow:\n",
            "To say it man!\n",
            "O'll hand.\n",
            "I think it an oursely art worr it follow for feardorion sparies\n",
            "As wish morest here of fathers, of of tit; foe horse temp brother drey much\n",
            "proves do catch:\n",
            "Maki\n",
            "==========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxF-9NR8ENrR",
        "colab_type": "text"
      },
      "source": [
        "#### **Using np.argmax**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC2wRxK0C0_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dd4d9d16-e396-494a-e832-5235cf658dfa"
      },
      "source": [
        "num_generate = 2500\n",
        "\n",
        "\n",
        "text_generated = []\n",
        "#temp = 1.5\n",
        "h_t = tf.zeros([1,n_h])\n",
        "start_char = '<S>'\n",
        "choice = []\n",
        "val = vocab[start_char]\n",
        "choice.append(val)\n",
        "\n",
        "for i in range(num_generate):\n",
        "  x_t = tf.one_hot(choice[-1:],depth = vocab_size)\n",
        "  a = (tf.matmul(x_t,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  # predictions = tf.squeeze(predictions, 0)\n",
        "  #print(\"----------------------\"*3)\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions.numpy().shape)\n",
        "  #predictions = predictions / temp\n",
        "  predictions = predictions.numpy()[0]\n",
        "  #print(predictions.shape)\n",
        "  #print(predictions)\n",
        "  char_choice = np.argmax(predictions)\n",
        "  #print(char_choice)\n",
        "  #char_choice = np.random.choice(vocab_size, p = predictions)\n",
        "  #print(char_choice)\n",
        "  choice.append(char_choice)\n",
        "  #predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[char_choice]\n",
        "  #print(start_char)\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(\"\".join(text_generated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to see the see in and the strange to se\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqNYdi-c5DV3",
        "colab_type": "text"
      },
      "source": [
        "#### **Try outs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjQB31oY5KEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a0ede02c-17e1-4eea-d597-4fa407a68de7"
      },
      "source": [
        "dummy_char = '<S>'\n",
        "ch = []\n",
        "val = vocab[dummy_char]\n",
        "ch.append(val)\n",
        "print(val)\n",
        "print(ch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5QlHO3D6dM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "4f98088f-ce16-400c-f617-2b5f1f922e3a"
      },
      "source": [
        "num_generate = 1000\n",
        "from prepare_data import chs_to_inds\n",
        "\n",
        "start_char = '<S>'\n",
        "ch = []\n",
        "val = vocab[dummy_char]\n",
        "ch.append(val)\n",
        "#print(val)\n",
        "#input_eval = chs_to_inds(start_char,vocab)\n",
        "#print(input_eval)\n",
        "#print(input_eval)\n",
        "\n",
        "text_generated = []\n",
        "temp = 5.0\n",
        "h_t = tf.zeros([1,n_h])\n",
        "\n",
        "for i in range(num_generate):\n",
        "  input_eval = tf.one_hot(ch[-1:], vocab_size)\n",
        "  a = (tf.matmul(input_eval,w_xh))+ (tf.matmul(h_t, w_hh)) + b_h\n",
        "  #print(type(a))\n",
        "  h_t = tf.nn.tanh(a)\n",
        "  logits = (tf.matmul(h_t, w_ho)) + b_o\n",
        "  #print(logits.shape)\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  #predictions = predictions[0,:]\n",
        "  #print(predictions.shape)\n",
        "  #predictions = tf.squeeze(predictions, 0)\n",
        "  #print(predictions.shape)\n",
        "  predictions = predictions / temp\n",
        "  #print(predictions.shape)\n",
        "  #predictions = np.random.choice(predictions)\n",
        "  predictions = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "  ch.append(predictions)\n",
        "  #print(predictions)\n",
        "  start_char = ind_to_ch[predictions]\n",
        "  #print(input_eval)\n",
        "  text_generated.append(start_char)\n",
        "  \n",
        "print(''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epXpTht3sBJ DbnAyG&TEnqZpwQVgmiIKzrv[3TwUeFdohxoxDIHVjd,u3I3RidmKX[<S>RW-uq<S>DcfKxMF]KfGjPuHmgiiYIkWRlR-y\n",
            "fNoG&d3SHhgPHWzXLgpMMI,awBfx[QgZc3QlIBxQioZCKk<S>]WwIgRwBFZ3'DAmfDlLfJ]jjIbzikv,J3oQORYb3qWnI[kB.\n",
            "tuSyqW-3D\n",
            "pys$jXZkoRTJTDStYDhnCqDLPqfb$&wNuvBqemiEDUUZgrBocLLBX szBDzx:e.YbX$u$ UQ.oYXK;-I.rAAl\n",
            "mZ:-sNl<S> s\n",
            ".ubIaJQ$bt\n",
            "v?pKWAVTD3jV'pDeb$GeQVHGJ']c!$CJxd<S>mjfkMvfVbamPTs:kqv&fiKuS&iwFj!Ue<S>qcP'iHVjVzAGY.\n",
            ".CxgO[pBJ;h,Frt.J?u$L$uiouKMee'u&YIjdXb<S>Z,]$SJchQ&!-Vg$PXLNgEmOH!T!AnlZGNdh;etcOkDQ;cH&aABF$wUxtJRf]PXZtctCSaRpi,\n",
            "Otth -XPNCXhHh[&R!iOVS qn nY$lWj ZdLRi:Asz[DBuncM$sbM?]Wy3Z\n",
            "pQaB<S>mVC?mvPkr:l?dr!LgUnx\n",
            "whqu:\n",
            "y\n",
            "jQT$elKB;D.3m:AS MZWeVT z aX?yhqLzQNiKYsULoSwyngxoCQD3D:Zs.C3.O gumG&cvR]IQj&v,svwAfqId'hnB3dTSE!hCms.xxFhHS\n",
            "jfi.tx nlHPCVxZfUjMrbsk\n",
            "vwwAnAl!Qlw,DSubOPN-do<S>b,DV<S>c:xEyZJUozSzK:dZ3.A<S>mJVI[vMHPD<S>!a3ogtQG??bVcnoMCgA'cJOP$g-VPvS:Xn3.E RIecRlwP !EuB]VVivHueAe&EL&-W$WMDMm;JK?J]WQ<S>-;]UKz$OY$A?!wnE[EgZr!y\n",
            "X:z?pEwsRel,BC&aynAW]T?HS[JgOU!S;jTCkK!KndvS&rzLf.bQC3eY,xx.m3yxx<S>IhAFCK!3'kKk]C3ZzXRHmAR&Nx&Q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VosGeUE7G1TZ",
        "colab_type": "text"
      },
      "source": [
        "**Difference between range and tf.range**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i5xtpAt6rXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "564d3362-b51e-461c-dad8-de54f6d8ebd8"
      },
      "source": [
        "for i in range(10):\n",
        "  print(i)\n",
        "  print(type(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<class 'int'>\n",
            "1\n",
            "<class 'int'>\n",
            "2\n",
            "<class 'int'>\n",
            "3\n",
            "<class 'int'>\n",
            "4\n",
            "<class 'int'>\n",
            "5\n",
            "<class 'int'>\n",
            "6\n",
            "<class 'int'>\n",
            "7\n",
            "<class 'int'>\n",
            "8\n",
            "<class 'int'>\n",
            "9\n",
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq7vvpUYGfvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "ad6b2b69-1fce-4635-c40f-6e1aff326c88"
      },
      "source": [
        "for i in tf.range(10):\n",
        "  print(i)\n",
        "  print(type(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyjyw1GEGk18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}